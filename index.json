[{"categories":["documentation"],"content":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","date":"2022-03-10","objectID":"/paper03/","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/"},{"categories":["documentation"],"content":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling è®ºæ–‡è§£è¯»ã€‚ ","date":"2022-03-10","objectID":"/paper03/:0:0","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#"},{"categories":["documentation"],"content":"é¢˜ç›®Cross-Domain NER using Cross-Domain Language Modeling [ACL 2019] [Code] ","date":"2022-03-10","objectID":"/paper03/:0:1","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#é¢˜ç›®"},{"categories":["documentation"],"content":"æ‘˜è¦ç”±äºæ ‡ç­¾èµ„æºçš„é™åˆ¶ï¼Œè·¨åŸŸå‘½åå®ä½“è¯†åˆ«ï¼ˆCross-Domain NERï¼‰ä¸€ç›´æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å¤§å¤šæ•°ç°æœ‰çš„å·¥ä½œéƒ½æ˜¯åœ¨ç›‘ç£ä¸‹è¿›è¡Œçš„ï¼Œå³åˆ©ç”¨æºåŸŸå’Œç›®æ ‡åŸŸçš„æ ‡è®°æ•°æ®ã€‚è¿™ç±»æ–¹æ³•çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œå®ƒä»¬ä¸èƒ½å¯¹æ²¡æœ‰NERæ•°æ®çš„domainè¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è€ƒè™‘ä½¿ç”¨è·¨åŸŸçš„è¯­è¨€æ¨¡å‹(LMs)ä½œä¸ºNERé¢†åŸŸé€‚åº”çš„æ¡¥æ¢ï¼Œé€šè¿‡è®¾è®¡ä¸€ä¸ªæ–°çš„å‚æ•°ç”Ÿæˆç½‘ç»œè¿›è¡Œè·¨åŸŸå’Œè·¨ä»»åŠ¡çš„çŸ¥è¯†è½¬ç§»ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä»è·¨åŸŸLMså¯¹æ¯”ä¸­æå–åŸŸçš„å·®å¼‚ï¼Œå…è®¸æ— ç›‘ç£çš„åŸŸé€‚åº”ï¼ŒåŒæ—¶ä¹Ÿç»™å‡ºäº†æœ€å…ˆè¿›çš„ç»“æœã€‚ ","date":"2022-03-10","objectID":"/paper03/:0:2","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#æ‘˜è¦"},{"categories":["documentation"],"content":"æ¨¡å‹æ¨¡å‹çš„æ•´ä½“ç»“æ„å¦‚å›¾Fig-1æ‰€ç¤ºã€‚åº•éƒ¨å±•ç¤ºäº†ä¸¤ä¸ªé¢†åŸŸå’Œä¸¤ä¸ªä»»åŠ¡çš„ç»„åˆã€‚é¦–å…ˆç»™å®šä¸€ä¸ªè¾“å…¥å¥å­ï¼Œé€šè¿‡ä¸€ä¸ªå…±äº«çš„åµŒå…¥å±‚è®¡ç®—å•è¯è¡¨å¾ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ–°çš„å‚æ•°ç”Ÿæˆç½‘ç»œè®¡ç®—å‡ºä¸€ç»„ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„BiLSTMå‚æ•°ï¼Œç”¨äºç¼–ç è¾“å…¥åºåˆ—ï¼Œæœ€åä¸åŒçš„è¾“å‡ºå±‚è¢«ç”¨äºä¸åŒçš„ä»»åŠ¡å’Œé¢†åŸŸã€‚ Fig-1.Model architecture Input LayeræŒ‰ç…§Yangç­‰äººï¼ˆ2018ï¼‰çš„è¯´æ³•,ç»™å®šä¸€ä¸ªè¾“å…¥$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$ï¼Œæ¥è‡ªä»¥ä¸‹4ä¸ªæ•°æ®é›† æºåŸŸNERè®­ç»ƒé›†$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ ç›®æ ‡åŸŸNERè®­ç»ƒé›†$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ æºåŸŸåŸå§‹æ–‡æœ¬é›†$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ ç›®æ ‡åŸŸåŸå§‹æ–‡æœ¬é›†$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ æ¯ä¸ªè¯$x_i$è¢«è¡¨ç¤ºä¸ºå…¶è¯åµŒå…¥å’Œå­—ç¬¦çº§CNNè¾“å‡ºçš„è¿æ¥ï¼š $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ å…¶ä¸­$\\mathbf{e}^w$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„è¯åµŒå…¥æŸ¥è¯¢è¡¨ï¼Œ$\\mathbf{e}^c$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„å­—ç¬¦åµŒå…¥æŸ¥è¯¢è¡¨ã€‚$\\text{CNN}(\\cdot)$ä»£è¡¨ä¸€ä¸ªæ ‡å‡†çš„$\\text{CNN}$ï¼Œä½œç”¨äºä¸€ä¸ªè¯$x_i$çš„å­—ç¬¦åµŒå…¥åºåˆ—$\\mathbf{e}^c(x_i)$ï¼Œ$\\oplus$è¡¨ç¤ºçŸ¢é‡è¿æ¥ã€‚ Parameter Generation Networkå°†$\\mathbf{v}$é€å…¥ä¸€ä¸ªåŒå‘çš„LSTMå±‚ï¼Œä¸ºäº†å®ç°è·¨é¢†åŸŸå’Œè·¨ä»»åŠ¡çš„çŸ¥è¯†è½¬ç§»ï¼Œä½¿ç”¨ä¸€ä¸ªå‚æ•°ç”Ÿæˆç½‘ç»œ$f(\\cdot,\\cdot,\\cdot)$åŠ¨æ€åœ°ç”Ÿæˆ$\\text{BiLSTM}$çš„å‚æ•°ï¼Œç”±æ­¤äº§ç”Ÿçš„å‚æ•°è¢«è¡¨ç¤ºä¸º$\\theta_{\\text{LSTM}}^{d,t}$ï¼Œå…¶ä¸­$d \\in {src,tgt}$ï¼Œ$t\\in {ner,lm}$ åˆ†åˆ«ä»£è¡¨é¢†åŸŸæ ‡ç­¾å’Œä»»åŠ¡æ ‡ç­¾ $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$è¡¨ç¤ºè¾“å…¥è¯åµŒå…¥ $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$ä»£è¡¨ä¸€ç»„ä»¥ä¸‰é˜¶å¼ é‡å½¢å¼å­˜åœ¨çš„å…ƒå‚æ•° $\\mathbf{I}_d^D\\in \\mathbb{R}^U$ä»£è¡¨é¢†åŸŸè¯åµŒå…¥ $\\mathbf{I}_d^D\\in \\mathbb{R}^V$ä»£è¡¨ä»»åŠ¡è¯åµŒå…¥ $U$ã€ $V$åˆ†åˆ«ä»£è¡¨é¢†åŸŸå’Œä»»åŠ¡è¯åµŒå…¥çš„å¤§å° $P^{(LSTM)}$æ˜¯$\\text{BiLSTM}$å‚æ•°çš„æ•°é‡ $\\otimes$ æŒ‡å¼ é‡æ”¶ç¼© ç»™å®šè¾“å…¥$v$å’Œå‚æ•°$\\theta$ï¼Œä¸€ä¸ªä»»åŠ¡å’Œç‰¹å®šé¢†åŸŸ$\\text{BiLSTM}$å•å…ƒçš„éšè—è¾“å‡ºå¯ä»¥ç»Ÿä¸€å†™æˆ: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$ï¼Œ$\\overleftarrow{\\mathbf{h}}_i^{d,t}$åˆ†åˆ«ä¸ºå‰å‘å’Œåå‘ã€‚ Output Layersæ ‡å‡†CRFsè¢«ç”¨ä½œNERçš„è¾“å‡ºå±‚ï¼Œåœ¨è¾“å…¥å¥å­$\\mathbf{x}$ä¸Šäº§ç”Ÿçš„æ ‡ç­¾åºåˆ—$\\mathbf{y}=l_1,l_2,\\dots,l_i$çš„è¾“å‡ºæ¦‚ç‡$p(\\mathbf{y}\\vert \\mathbf{x})$æ˜¯ $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$ä»£è¡¨å‰å‘å’Œåå‘çš„ç»„åˆç‰¹å¾ $yâ€™$ä»£è¡¨ä¸€ä¸ªä»»æ„çš„æ ‡ç­¾åºåˆ— $\\mathbf{w}^{li}_{CRF}$æ˜¯$l_i$ç‰¹æœ‰çš„æ¨¡å‹å‚æ•° ${b_{CRF}^{(l_{i-1},l_i)}}$ æ˜¯ $l_{i-1}$ å’Œ $l_i$ç‰¹æœ‰çš„åç½® è€ƒè™‘åˆ°ä¸åŒé¢†åŸŸçš„NERæ ‡ç­¾é›†å¯èƒ½ä¸åŒï¼Œåœ¨Fig-1ä¸­åˆ†åˆ«ç”¨$\\text{CRF(S)}$å’Œ$\\text{CRF(T)}$æ¥è¡¨ç¤ºæºåŸŸå’Œç›®æ ‡åŸŸçš„$\\text{CRFs}$ï¼Œä½¿ç”¨ä¸€é˜¶Viterbiç®—æ³•æ¥å¯»æ‰¾é«˜åˆ†çš„æ ‡ç­¾åºåˆ—ã€‚ Language modelingå‰å‘$\\text{LM(LMf)}$ ä½¿ç”¨å‰å‘LSTMéšè—çŠ¶æ€$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{1:i}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸‹ä¸€ä¸ªè¯$x_{i+1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i+1}\\vert x_{1:i})$ åå‘$\\text{LM(LMb)}$ ä½¿ç”¨åå‘LSTMéšè—çŠ¶æ€$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{i:n}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸Šä¸€ä¸ªè¯$x_{i-1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i-1}\\vert x_{i:n})$ è€ƒè™‘åˆ°è®¡ç®—æ•ˆç‡ï¼Œé‡‡ç”¨è´Ÿé‡‡æ ·Softmaxï¼ˆNSSoftmaxï¼‰æ¥è®¡ç®—å‰å‘å’Œåå‘æ¦‚ç‡ï¼Œå…·ä½“å¦‚ä¸‹ï¼š $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#æ¨¡å‹"},{"categories":["documentation"],"content":"æ¨¡å‹æ¨¡å‹çš„æ•´ä½“ç»“æ„å¦‚å›¾Fig-1æ‰€ç¤ºã€‚åº•éƒ¨å±•ç¤ºäº†ä¸¤ä¸ªé¢†åŸŸå’Œä¸¤ä¸ªä»»åŠ¡çš„ç»„åˆã€‚é¦–å…ˆç»™å®šä¸€ä¸ªè¾“å…¥å¥å­ï¼Œé€šè¿‡ä¸€ä¸ªå…±äº«çš„åµŒå…¥å±‚è®¡ç®—å•è¯è¡¨å¾ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ–°çš„å‚æ•°ç”Ÿæˆç½‘ç»œè®¡ç®—å‡ºä¸€ç»„ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„BiLSTMå‚æ•°ï¼Œç”¨äºç¼–ç è¾“å…¥åºåˆ—ï¼Œæœ€åä¸åŒçš„è¾“å‡ºå±‚è¢«ç”¨äºä¸åŒçš„ä»»åŠ¡å’Œé¢†åŸŸã€‚ Fig-1.Model architecture Input LayeræŒ‰ç…§Yangç­‰äººï¼ˆ2018ï¼‰çš„è¯´æ³•,ç»™å®šä¸€ä¸ªè¾“å…¥$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$ï¼Œæ¥è‡ªä»¥ä¸‹4ä¸ªæ•°æ®é›† æºåŸŸNERè®­ç»ƒé›†$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ ç›®æ ‡åŸŸNERè®­ç»ƒé›†$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ æºåŸŸåŸå§‹æ–‡æœ¬é›†$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ ç›®æ ‡åŸŸåŸå§‹æ–‡æœ¬é›†$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ æ¯ä¸ªè¯$x_i$è¢«è¡¨ç¤ºä¸ºå…¶è¯åµŒå…¥å’Œå­—ç¬¦çº§CNNè¾“å‡ºçš„è¿æ¥ï¼š $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ å…¶ä¸­$\\mathbf{e}^w$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„è¯åµŒå…¥æŸ¥è¯¢è¡¨ï¼Œ$\\mathbf{e}^c$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„å­—ç¬¦åµŒå…¥æŸ¥è¯¢è¡¨ã€‚$\\text{CNN}(\\cdot)$ä»£è¡¨ä¸€ä¸ªæ ‡å‡†çš„$\\text{CNN}$ï¼Œä½œç”¨äºä¸€ä¸ªè¯$x_i$çš„å­—ç¬¦åµŒå…¥åºåˆ—$\\mathbf{e}^c(x_i)$ï¼Œ$\\oplus$è¡¨ç¤ºçŸ¢é‡è¿æ¥ã€‚ Parameter Generation Networkå°†$\\mathbf{v}$é€å…¥ä¸€ä¸ªåŒå‘çš„LSTMå±‚ï¼Œä¸ºäº†å®ç°è·¨é¢†åŸŸå’Œè·¨ä»»åŠ¡çš„çŸ¥è¯†è½¬ç§»ï¼Œä½¿ç”¨ä¸€ä¸ªå‚æ•°ç”Ÿæˆç½‘ç»œ$f(\\cdot,\\cdot,\\cdot)$åŠ¨æ€åœ°ç”Ÿæˆ$\\text{BiLSTM}$çš„å‚æ•°ï¼Œç”±æ­¤äº§ç”Ÿçš„å‚æ•°è¢«è¡¨ç¤ºä¸º$\\theta_{\\text{LSTM}}^{d,t}$ï¼Œå…¶ä¸­$d \\in {src,tgt}$ï¼Œ$t\\in {ner,lm}$ åˆ†åˆ«ä»£è¡¨é¢†åŸŸæ ‡ç­¾å’Œä»»åŠ¡æ ‡ç­¾ $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$è¡¨ç¤ºè¾“å…¥è¯åµŒå…¥ $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$ä»£è¡¨ä¸€ç»„ä»¥ä¸‰é˜¶å¼ é‡å½¢å¼å­˜åœ¨çš„å…ƒå‚æ•° $\\mathbf{I}_d^D\\in \\mathbb{R}^U$ä»£è¡¨é¢†åŸŸè¯åµŒå…¥ $\\mathbf{I}_d^D\\in \\mathbb{R}^V$ä»£è¡¨ä»»åŠ¡è¯åµŒå…¥ $U$ã€ $V$åˆ†åˆ«ä»£è¡¨é¢†åŸŸå’Œä»»åŠ¡è¯åµŒå…¥çš„å¤§å° $P^{(LSTM)}$æ˜¯$\\text{BiLSTM}$å‚æ•°çš„æ•°é‡ $\\otimes$ æŒ‡å¼ é‡æ”¶ç¼© ç»™å®šè¾“å…¥$v$å’Œå‚æ•°$\\theta$ï¼Œä¸€ä¸ªä»»åŠ¡å’Œç‰¹å®šé¢†åŸŸ$\\text{BiLSTM}$å•å…ƒçš„éšè—è¾“å‡ºå¯ä»¥ç»Ÿä¸€å†™æˆ: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$ï¼Œ$\\overleftarrow{\\mathbf{h}}_i^{d,t}$åˆ†åˆ«ä¸ºå‰å‘å’Œåå‘ã€‚ Output Layersæ ‡å‡†CRFsè¢«ç”¨ä½œNERçš„è¾“å‡ºå±‚ï¼Œåœ¨è¾“å…¥å¥å­$\\mathbf{x}$ä¸Šäº§ç”Ÿçš„æ ‡ç­¾åºåˆ—$\\mathbf{y}=l_1,l_2,\\dots,l_i$çš„è¾“å‡ºæ¦‚ç‡$p(\\mathbf{y}\\vert \\mathbf{x})$æ˜¯ $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$ä»£è¡¨å‰å‘å’Œåå‘çš„ç»„åˆç‰¹å¾ $yâ€™$ä»£è¡¨ä¸€ä¸ªä»»æ„çš„æ ‡ç­¾åºåˆ— $\\mathbf{w}^{li}_{CRF}$æ˜¯$l_i$ç‰¹æœ‰çš„æ¨¡å‹å‚æ•° ${b_{CRF}^{(l_{i-1},l_i)}}$ æ˜¯ $l_{i-1}$ å’Œ $l_i$ç‰¹æœ‰çš„åç½® è€ƒè™‘åˆ°ä¸åŒé¢†åŸŸçš„NERæ ‡ç­¾é›†å¯èƒ½ä¸åŒï¼Œåœ¨Fig-1ä¸­åˆ†åˆ«ç”¨$\\text{CRF(S)}$å’Œ$\\text{CRF(T)}$æ¥è¡¨ç¤ºæºåŸŸå’Œç›®æ ‡åŸŸçš„$\\text{CRFs}$ï¼Œä½¿ç”¨ä¸€é˜¶Viterbiç®—æ³•æ¥å¯»æ‰¾é«˜åˆ†çš„æ ‡ç­¾åºåˆ—ã€‚ Language modelingå‰å‘$\\text{LM(LMf)}$ ä½¿ç”¨å‰å‘LSTMéšè—çŠ¶æ€$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{1:i}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸‹ä¸€ä¸ªè¯$x_{i+1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i+1}\\vert x_{1:i})$ åå‘$\\text{LM(LMb)}$ ä½¿ç”¨åå‘LSTMéšè—çŠ¶æ€$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{i:n}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸Šä¸€ä¸ªè¯$x_{i-1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i-1}\\vert x_{i:n})$ è€ƒè™‘åˆ°è®¡ç®—æ•ˆç‡ï¼Œé‡‡ç”¨è´Ÿé‡‡æ ·Softmaxï¼ˆNSSoftmaxï¼‰æ¥è®¡ç®—å‰å‘å’Œåå‘æ¦‚ç‡ï¼Œå…·ä½“å¦‚ä¸‹ï¼š $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#input-layer"},{"categories":["documentation"],"content":"æ¨¡å‹æ¨¡å‹çš„æ•´ä½“ç»“æ„å¦‚å›¾Fig-1æ‰€ç¤ºã€‚åº•éƒ¨å±•ç¤ºäº†ä¸¤ä¸ªé¢†åŸŸå’Œä¸¤ä¸ªä»»åŠ¡çš„ç»„åˆã€‚é¦–å…ˆç»™å®šä¸€ä¸ªè¾“å…¥å¥å­ï¼Œé€šè¿‡ä¸€ä¸ªå…±äº«çš„åµŒå…¥å±‚è®¡ç®—å•è¯è¡¨å¾ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ–°çš„å‚æ•°ç”Ÿæˆç½‘ç»œè®¡ç®—å‡ºä¸€ç»„ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„BiLSTMå‚æ•°ï¼Œç”¨äºç¼–ç è¾“å…¥åºåˆ—ï¼Œæœ€åä¸åŒçš„è¾“å‡ºå±‚è¢«ç”¨äºä¸åŒçš„ä»»åŠ¡å’Œé¢†åŸŸã€‚ Fig-1.Model architecture Input LayeræŒ‰ç…§Yangç­‰äººï¼ˆ2018ï¼‰çš„è¯´æ³•,ç»™å®šä¸€ä¸ªè¾“å…¥$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$ï¼Œæ¥è‡ªä»¥ä¸‹4ä¸ªæ•°æ®é›† æºåŸŸNERè®­ç»ƒé›†$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ ç›®æ ‡åŸŸNERè®­ç»ƒé›†$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ æºåŸŸåŸå§‹æ–‡æœ¬é›†$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ ç›®æ ‡åŸŸåŸå§‹æ–‡æœ¬é›†$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ æ¯ä¸ªè¯$x_i$è¢«è¡¨ç¤ºä¸ºå…¶è¯åµŒå…¥å’Œå­—ç¬¦çº§CNNè¾“å‡ºçš„è¿æ¥ï¼š $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ å…¶ä¸­$\\mathbf{e}^w$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„è¯åµŒå…¥æŸ¥è¯¢è¡¨ï¼Œ$\\mathbf{e}^c$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„å­—ç¬¦åµŒå…¥æŸ¥è¯¢è¡¨ã€‚$\\text{CNN}(\\cdot)$ä»£è¡¨ä¸€ä¸ªæ ‡å‡†çš„$\\text{CNN}$ï¼Œä½œç”¨äºä¸€ä¸ªè¯$x_i$çš„å­—ç¬¦åµŒå…¥åºåˆ—$\\mathbf{e}^c(x_i)$ï¼Œ$\\oplus$è¡¨ç¤ºçŸ¢é‡è¿æ¥ã€‚ Parameter Generation Networkå°†$\\mathbf{v}$é€å…¥ä¸€ä¸ªåŒå‘çš„LSTMå±‚ï¼Œä¸ºäº†å®ç°è·¨é¢†åŸŸå’Œè·¨ä»»åŠ¡çš„çŸ¥è¯†è½¬ç§»ï¼Œä½¿ç”¨ä¸€ä¸ªå‚æ•°ç”Ÿæˆç½‘ç»œ$f(\\cdot,\\cdot,\\cdot)$åŠ¨æ€åœ°ç”Ÿæˆ$\\text{BiLSTM}$çš„å‚æ•°ï¼Œç”±æ­¤äº§ç”Ÿçš„å‚æ•°è¢«è¡¨ç¤ºä¸º$\\theta_{\\text{LSTM}}^{d,t}$ï¼Œå…¶ä¸­$d \\in {src,tgt}$ï¼Œ$t\\in {ner,lm}$ åˆ†åˆ«ä»£è¡¨é¢†åŸŸæ ‡ç­¾å’Œä»»åŠ¡æ ‡ç­¾ $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$è¡¨ç¤ºè¾“å…¥è¯åµŒå…¥ $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$ä»£è¡¨ä¸€ç»„ä»¥ä¸‰é˜¶å¼ é‡å½¢å¼å­˜åœ¨çš„å…ƒå‚æ•° $\\mathbf{I}_d^D\\in \\mathbb{R}^U$ä»£è¡¨é¢†åŸŸè¯åµŒå…¥ $\\mathbf{I}_d^D\\in \\mathbb{R}^V$ä»£è¡¨ä»»åŠ¡è¯åµŒå…¥ $U$ã€ $V$åˆ†åˆ«ä»£è¡¨é¢†åŸŸå’Œä»»åŠ¡è¯åµŒå…¥çš„å¤§å° $P^{(LSTM)}$æ˜¯$\\text{BiLSTM}$å‚æ•°çš„æ•°é‡ $\\otimes$ æŒ‡å¼ é‡æ”¶ç¼© ç»™å®šè¾“å…¥$v$å’Œå‚æ•°$\\theta$ï¼Œä¸€ä¸ªä»»åŠ¡å’Œç‰¹å®šé¢†åŸŸ$\\text{BiLSTM}$å•å…ƒçš„éšè—è¾“å‡ºå¯ä»¥ç»Ÿä¸€å†™æˆ: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$ï¼Œ$\\overleftarrow{\\mathbf{h}}_i^{d,t}$åˆ†åˆ«ä¸ºå‰å‘å’Œåå‘ã€‚ Output Layersæ ‡å‡†CRFsè¢«ç”¨ä½œNERçš„è¾“å‡ºå±‚ï¼Œåœ¨è¾“å…¥å¥å­$\\mathbf{x}$ä¸Šäº§ç”Ÿçš„æ ‡ç­¾åºåˆ—$\\mathbf{y}=l_1,l_2,\\dots,l_i$çš„è¾“å‡ºæ¦‚ç‡$p(\\mathbf{y}\\vert \\mathbf{x})$æ˜¯ $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$ä»£è¡¨å‰å‘å’Œåå‘çš„ç»„åˆç‰¹å¾ $yâ€™$ä»£è¡¨ä¸€ä¸ªä»»æ„çš„æ ‡ç­¾åºåˆ— $\\mathbf{w}^{li}_{CRF}$æ˜¯$l_i$ç‰¹æœ‰çš„æ¨¡å‹å‚æ•° ${b_{CRF}^{(l_{i-1},l_i)}}$ æ˜¯ $l_{i-1}$ å’Œ $l_i$ç‰¹æœ‰çš„åç½® è€ƒè™‘åˆ°ä¸åŒé¢†åŸŸçš„NERæ ‡ç­¾é›†å¯èƒ½ä¸åŒï¼Œåœ¨Fig-1ä¸­åˆ†åˆ«ç”¨$\\text{CRF(S)}$å’Œ$\\text{CRF(T)}$æ¥è¡¨ç¤ºæºåŸŸå’Œç›®æ ‡åŸŸçš„$\\text{CRFs}$ï¼Œä½¿ç”¨ä¸€é˜¶Viterbiç®—æ³•æ¥å¯»æ‰¾é«˜åˆ†çš„æ ‡ç­¾åºåˆ—ã€‚ Language modelingå‰å‘$\\text{LM(LMf)}$ ä½¿ç”¨å‰å‘LSTMéšè—çŠ¶æ€$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{1:i}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸‹ä¸€ä¸ªè¯$x_{i+1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i+1}\\vert x_{1:i})$ åå‘$\\text{LM(LMb)}$ ä½¿ç”¨åå‘LSTMéšè—çŠ¶æ€$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{i:n}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸Šä¸€ä¸ªè¯$x_{i-1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i-1}\\vert x_{i:n})$ è€ƒè™‘åˆ°è®¡ç®—æ•ˆç‡ï¼Œé‡‡ç”¨è´Ÿé‡‡æ ·Softmaxï¼ˆNSSoftmaxï¼‰æ¥è®¡ç®—å‰å‘å’Œåå‘æ¦‚ç‡ï¼Œå…·ä½“å¦‚ä¸‹ï¼š $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#parameter-generation-network"},{"categories":["documentation"],"content":"æ¨¡å‹æ¨¡å‹çš„æ•´ä½“ç»“æ„å¦‚å›¾Fig-1æ‰€ç¤ºã€‚åº•éƒ¨å±•ç¤ºäº†ä¸¤ä¸ªé¢†åŸŸå’Œä¸¤ä¸ªä»»åŠ¡çš„ç»„åˆã€‚é¦–å…ˆç»™å®šä¸€ä¸ªè¾“å…¥å¥å­ï¼Œé€šè¿‡ä¸€ä¸ªå…±äº«çš„åµŒå…¥å±‚è®¡ç®—å•è¯è¡¨å¾ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ–°çš„å‚æ•°ç”Ÿæˆç½‘ç»œè®¡ç®—å‡ºä¸€ç»„ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„BiLSTMå‚æ•°ï¼Œç”¨äºç¼–ç è¾“å…¥åºåˆ—ï¼Œæœ€åä¸åŒçš„è¾“å‡ºå±‚è¢«ç”¨äºä¸åŒçš„ä»»åŠ¡å’Œé¢†åŸŸã€‚ Fig-1.Model architecture Input LayeræŒ‰ç…§Yangç­‰äººï¼ˆ2018ï¼‰çš„è¯´æ³•,ç»™å®šä¸€ä¸ªè¾“å…¥$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$ï¼Œæ¥è‡ªä»¥ä¸‹4ä¸ªæ•°æ®é›† æºåŸŸNERè®­ç»ƒé›†$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ ç›®æ ‡åŸŸNERè®­ç»ƒé›†$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ æºåŸŸåŸå§‹æ–‡æœ¬é›†$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ ç›®æ ‡åŸŸåŸå§‹æ–‡æœ¬é›†$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ æ¯ä¸ªè¯$x_i$è¢«è¡¨ç¤ºä¸ºå…¶è¯åµŒå…¥å’Œå­—ç¬¦çº§CNNè¾“å‡ºçš„è¿æ¥ï¼š $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ å…¶ä¸­$\\mathbf{e}^w$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„è¯åµŒå…¥æŸ¥è¯¢è¡¨ï¼Œ$\\mathbf{e}^c$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„å­—ç¬¦åµŒå…¥æŸ¥è¯¢è¡¨ã€‚$\\text{CNN}(\\cdot)$ä»£è¡¨ä¸€ä¸ªæ ‡å‡†çš„$\\text{CNN}$ï¼Œä½œç”¨äºä¸€ä¸ªè¯$x_i$çš„å­—ç¬¦åµŒå…¥åºåˆ—$\\mathbf{e}^c(x_i)$ï¼Œ$\\oplus$è¡¨ç¤ºçŸ¢é‡è¿æ¥ã€‚ Parameter Generation Networkå°†$\\mathbf{v}$é€å…¥ä¸€ä¸ªåŒå‘çš„LSTMå±‚ï¼Œä¸ºäº†å®ç°è·¨é¢†åŸŸå’Œè·¨ä»»åŠ¡çš„çŸ¥è¯†è½¬ç§»ï¼Œä½¿ç”¨ä¸€ä¸ªå‚æ•°ç”Ÿæˆç½‘ç»œ$f(\\cdot,\\cdot,\\cdot)$åŠ¨æ€åœ°ç”Ÿæˆ$\\text{BiLSTM}$çš„å‚æ•°ï¼Œç”±æ­¤äº§ç”Ÿçš„å‚æ•°è¢«è¡¨ç¤ºä¸º$\\theta_{\\text{LSTM}}^{d,t}$ï¼Œå…¶ä¸­$d \\in {src,tgt}$ï¼Œ$t\\in {ner,lm}$ åˆ†åˆ«ä»£è¡¨é¢†åŸŸæ ‡ç­¾å’Œä»»åŠ¡æ ‡ç­¾ $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$è¡¨ç¤ºè¾“å…¥è¯åµŒå…¥ $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$ä»£è¡¨ä¸€ç»„ä»¥ä¸‰é˜¶å¼ é‡å½¢å¼å­˜åœ¨çš„å…ƒå‚æ•° $\\mathbf{I}_d^D\\in \\mathbb{R}^U$ä»£è¡¨é¢†åŸŸè¯åµŒå…¥ $\\mathbf{I}_d^D\\in \\mathbb{R}^V$ä»£è¡¨ä»»åŠ¡è¯åµŒå…¥ $U$ã€ $V$åˆ†åˆ«ä»£è¡¨é¢†åŸŸå’Œä»»åŠ¡è¯åµŒå…¥çš„å¤§å° $P^{(LSTM)}$æ˜¯$\\text{BiLSTM}$å‚æ•°çš„æ•°é‡ $\\otimes$ æŒ‡å¼ é‡æ”¶ç¼© ç»™å®šè¾“å…¥$v$å’Œå‚æ•°$\\theta$ï¼Œä¸€ä¸ªä»»åŠ¡å’Œç‰¹å®šé¢†åŸŸ$\\text{BiLSTM}$å•å…ƒçš„éšè—è¾“å‡ºå¯ä»¥ç»Ÿä¸€å†™æˆ: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$ï¼Œ$\\overleftarrow{\\mathbf{h}}_i^{d,t}$åˆ†åˆ«ä¸ºå‰å‘å’Œåå‘ã€‚ Output Layersæ ‡å‡†CRFsè¢«ç”¨ä½œNERçš„è¾“å‡ºå±‚ï¼Œåœ¨è¾“å…¥å¥å­$\\mathbf{x}$ä¸Šäº§ç”Ÿçš„æ ‡ç­¾åºåˆ—$\\mathbf{y}=l_1,l_2,\\dots,l_i$çš„è¾“å‡ºæ¦‚ç‡$p(\\mathbf{y}\\vert \\mathbf{x})$æ˜¯ $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$ä»£è¡¨å‰å‘å’Œåå‘çš„ç»„åˆç‰¹å¾ $yâ€™$ä»£è¡¨ä¸€ä¸ªä»»æ„çš„æ ‡ç­¾åºåˆ— $\\mathbf{w}^{li}_{CRF}$æ˜¯$l_i$ç‰¹æœ‰çš„æ¨¡å‹å‚æ•° ${b_{CRF}^{(l_{i-1},l_i)}}$ æ˜¯ $l_{i-1}$ å’Œ $l_i$ç‰¹æœ‰çš„åç½® è€ƒè™‘åˆ°ä¸åŒé¢†åŸŸçš„NERæ ‡ç­¾é›†å¯èƒ½ä¸åŒï¼Œåœ¨Fig-1ä¸­åˆ†åˆ«ç”¨$\\text{CRF(S)}$å’Œ$\\text{CRF(T)}$æ¥è¡¨ç¤ºæºåŸŸå’Œç›®æ ‡åŸŸçš„$\\text{CRFs}$ï¼Œä½¿ç”¨ä¸€é˜¶Viterbiç®—æ³•æ¥å¯»æ‰¾é«˜åˆ†çš„æ ‡ç­¾åºåˆ—ã€‚ Language modelingå‰å‘$\\text{LM(LMf)}$ ä½¿ç”¨å‰å‘LSTMéšè—çŠ¶æ€$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{1:i}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸‹ä¸€ä¸ªè¯$x_{i+1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i+1}\\vert x_{1:i})$ åå‘$\\text{LM(LMb)}$ ä½¿ç”¨åå‘LSTMéšè—çŠ¶æ€$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{i:n}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸Šä¸€ä¸ªè¯$x_{i-1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i-1}\\vert x_{i:n})$ è€ƒè™‘åˆ°è®¡ç®—æ•ˆç‡ï¼Œé‡‡ç”¨è´Ÿé‡‡æ ·Softmaxï¼ˆNSSoftmaxï¼‰æ¥è®¡ç®—å‰å‘å’Œåå‘æ¦‚ç‡ï¼Œå…·ä½“å¦‚ä¸‹ï¼š $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#output-layers"},{"categories":["documentation"],"content":"æ¨¡å‹æ¨¡å‹çš„æ•´ä½“ç»“æ„å¦‚å›¾Fig-1æ‰€ç¤ºã€‚åº•éƒ¨å±•ç¤ºäº†ä¸¤ä¸ªé¢†åŸŸå’Œä¸¤ä¸ªä»»åŠ¡çš„ç»„åˆã€‚é¦–å…ˆç»™å®šä¸€ä¸ªè¾“å…¥å¥å­ï¼Œé€šè¿‡ä¸€ä¸ªå…±äº«çš„åµŒå…¥å±‚è®¡ç®—å•è¯è¡¨å¾ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ–°çš„å‚æ•°ç”Ÿæˆç½‘ç»œè®¡ç®—å‡ºä¸€ç»„ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„BiLSTMå‚æ•°ï¼Œç”¨äºç¼–ç è¾“å…¥åºåˆ—ï¼Œæœ€åä¸åŒçš„è¾“å‡ºå±‚è¢«ç”¨äºä¸åŒçš„ä»»åŠ¡å’Œé¢†åŸŸã€‚ Fig-1.Model architecture Input LayeræŒ‰ç…§Yangç­‰äººï¼ˆ2018ï¼‰çš„è¯´æ³•,ç»™å®šä¸€ä¸ªè¾“å…¥$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$ï¼Œæ¥è‡ªä»¥ä¸‹4ä¸ªæ•°æ®é›† æºåŸŸNERè®­ç»ƒé›†$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ ç›®æ ‡åŸŸNERè®­ç»ƒé›†$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ æºåŸŸåŸå§‹æ–‡æœ¬é›†$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ ç›®æ ‡åŸŸåŸå§‹æ–‡æœ¬é›†$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ æ¯ä¸ªè¯$x_i$è¢«è¡¨ç¤ºä¸ºå…¶è¯åµŒå…¥å’Œå­—ç¬¦çº§CNNè¾“å‡ºçš„è¿æ¥ï¼š $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ å…¶ä¸­$\\mathbf{e}^w$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„è¯åµŒå…¥æŸ¥è¯¢è¡¨ï¼Œ$\\mathbf{e}^c$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„å­—ç¬¦åµŒå…¥æŸ¥è¯¢è¡¨ã€‚$\\text{CNN}(\\cdot)$ä»£è¡¨ä¸€ä¸ªæ ‡å‡†çš„$\\text{CNN}$ï¼Œä½œç”¨äºä¸€ä¸ªè¯$x_i$çš„å­—ç¬¦åµŒå…¥åºåˆ—$\\mathbf{e}^c(x_i)$ï¼Œ$\\oplus$è¡¨ç¤ºçŸ¢é‡è¿æ¥ã€‚ Parameter Generation Networkå°†$\\mathbf{v}$é€å…¥ä¸€ä¸ªåŒå‘çš„LSTMå±‚ï¼Œä¸ºäº†å®ç°è·¨é¢†åŸŸå’Œè·¨ä»»åŠ¡çš„çŸ¥è¯†è½¬ç§»ï¼Œä½¿ç”¨ä¸€ä¸ªå‚æ•°ç”Ÿæˆç½‘ç»œ$f(\\cdot,\\cdot,\\cdot)$åŠ¨æ€åœ°ç”Ÿæˆ$\\text{BiLSTM}$çš„å‚æ•°ï¼Œç”±æ­¤äº§ç”Ÿçš„å‚æ•°è¢«è¡¨ç¤ºä¸º$\\theta_{\\text{LSTM}}^{d,t}$ï¼Œå…¶ä¸­$d \\in {src,tgt}$ï¼Œ$t\\in {ner,lm}$ åˆ†åˆ«ä»£è¡¨é¢†åŸŸæ ‡ç­¾å’Œä»»åŠ¡æ ‡ç­¾ $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$è¡¨ç¤ºè¾“å…¥è¯åµŒå…¥ $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$ä»£è¡¨ä¸€ç»„ä»¥ä¸‰é˜¶å¼ é‡å½¢å¼å­˜åœ¨çš„å…ƒå‚æ•° $\\mathbf{I}_d^D\\in \\mathbb{R}^U$ä»£è¡¨é¢†åŸŸè¯åµŒå…¥ $\\mathbf{I}_d^D\\in \\mathbb{R}^V$ä»£è¡¨ä»»åŠ¡è¯åµŒå…¥ $U$ã€ $V$åˆ†åˆ«ä»£è¡¨é¢†åŸŸå’Œä»»åŠ¡è¯åµŒå…¥çš„å¤§å° $P^{(LSTM)}$æ˜¯$\\text{BiLSTM}$å‚æ•°çš„æ•°é‡ $\\otimes$ æŒ‡å¼ é‡æ”¶ç¼© ç»™å®šè¾“å…¥$v$å’Œå‚æ•°$\\theta$ï¼Œä¸€ä¸ªä»»åŠ¡å’Œç‰¹å®šé¢†åŸŸ$\\text{BiLSTM}$å•å…ƒçš„éšè—è¾“å‡ºå¯ä»¥ç»Ÿä¸€å†™æˆ: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$ï¼Œ$\\overleftarrow{\\mathbf{h}}_i^{d,t}$åˆ†åˆ«ä¸ºå‰å‘å’Œåå‘ã€‚ Output Layersæ ‡å‡†CRFsè¢«ç”¨ä½œNERçš„è¾“å‡ºå±‚ï¼Œåœ¨è¾“å…¥å¥å­$\\mathbf{x}$ä¸Šäº§ç”Ÿçš„æ ‡ç­¾åºåˆ—$\\mathbf{y}=l_1,l_2,\\dots,l_i$çš„è¾“å‡ºæ¦‚ç‡$p(\\mathbf{y}\\vert \\mathbf{x})$æ˜¯ $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$ä»£è¡¨å‰å‘å’Œåå‘çš„ç»„åˆç‰¹å¾ $yâ€™$ä»£è¡¨ä¸€ä¸ªä»»æ„çš„æ ‡ç­¾åºåˆ— $\\mathbf{w}^{li}_{CRF}$æ˜¯$l_i$ç‰¹æœ‰çš„æ¨¡å‹å‚æ•° ${b_{CRF}^{(l_{i-1},l_i)}}$ æ˜¯ $l_{i-1}$ å’Œ $l_i$ç‰¹æœ‰çš„åç½® è€ƒè™‘åˆ°ä¸åŒé¢†åŸŸçš„NERæ ‡ç­¾é›†å¯èƒ½ä¸åŒï¼Œåœ¨Fig-1ä¸­åˆ†åˆ«ç”¨$\\text{CRF(S)}$å’Œ$\\text{CRF(T)}$æ¥è¡¨ç¤ºæºåŸŸå’Œç›®æ ‡åŸŸçš„$\\text{CRFs}$ï¼Œä½¿ç”¨ä¸€é˜¶Viterbiç®—æ³•æ¥å¯»æ‰¾é«˜åˆ†çš„æ ‡ç­¾åºåˆ—ã€‚ Language modelingå‰å‘$\\text{LM(LMf)}$ ä½¿ç”¨å‰å‘LSTMéšè—çŠ¶æ€$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{1:i}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸‹ä¸€ä¸ªè¯$x_{i+1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i+1}\\vert x_{1:i})$ åå‘$\\text{LM(LMb)}$ ä½¿ç”¨åå‘LSTMéšè—çŠ¶æ€$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{i:n}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸Šä¸€ä¸ªè¯$x_{i-1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i-1}\\vert x_{i:n})$ è€ƒè™‘åˆ°è®¡ç®—æ•ˆç‡ï¼Œé‡‡ç”¨è´Ÿé‡‡æ ·Softmaxï¼ˆNSSoftmaxï¼‰æ¥è®¡ç®—å‰å‘å’Œåå‘æ¦‚ç‡ï¼Œå…·ä½“å¦‚ä¸‹ï¼š $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#language-modeling"},{"categories":["documentation"],"content":"æ¨¡å‹æ¨¡å‹çš„æ•´ä½“ç»“æ„å¦‚å›¾Fig-1æ‰€ç¤ºã€‚åº•éƒ¨å±•ç¤ºäº†ä¸¤ä¸ªé¢†åŸŸå’Œä¸¤ä¸ªä»»åŠ¡çš„ç»„åˆã€‚é¦–å…ˆç»™å®šä¸€ä¸ªè¾“å…¥å¥å­ï¼Œé€šè¿‡ä¸€ä¸ªå…±äº«çš„åµŒå…¥å±‚è®¡ç®—å•è¯è¡¨å¾ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ–°çš„å‚æ•°ç”Ÿæˆç½‘ç»œè®¡ç®—å‡ºä¸€ç»„ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„BiLSTMå‚æ•°ï¼Œç”¨äºç¼–ç è¾“å…¥åºåˆ—ï¼Œæœ€åä¸åŒçš„è¾“å‡ºå±‚è¢«ç”¨äºä¸åŒçš„ä»»åŠ¡å’Œé¢†åŸŸã€‚ Fig-1.Model architecture Input LayeræŒ‰ç…§Yangç­‰äººï¼ˆ2018ï¼‰çš„è¯´æ³•,ç»™å®šä¸€ä¸ªè¾“å…¥$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$ï¼Œæ¥è‡ªä»¥ä¸‹4ä¸ªæ•°æ®é›† æºåŸŸNERè®­ç»ƒé›†$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ ç›®æ ‡åŸŸNERè®­ç»ƒé›†$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ æºåŸŸåŸå§‹æ–‡æœ¬é›†$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ ç›®æ ‡åŸŸåŸå§‹æ–‡æœ¬é›†$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ æ¯ä¸ªè¯$x_i$è¢«è¡¨ç¤ºä¸ºå…¶è¯åµŒå…¥å’Œå­—ç¬¦çº§CNNè¾“å‡ºçš„è¿æ¥ï¼š $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ å…¶ä¸­$\\mathbf{e}^w$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„è¯åµŒå…¥æŸ¥è¯¢è¡¨ï¼Œ$\\mathbf{e}^c$ä»£è¡¨ä¸€ä¸ªå…±äº«çš„å­—ç¬¦åµŒå…¥æŸ¥è¯¢è¡¨ã€‚$\\text{CNN}(\\cdot)$ä»£è¡¨ä¸€ä¸ªæ ‡å‡†çš„$\\text{CNN}$ï¼Œä½œç”¨äºä¸€ä¸ªè¯$x_i$çš„å­—ç¬¦åµŒå…¥åºåˆ—$\\mathbf{e}^c(x_i)$ï¼Œ$\\oplus$è¡¨ç¤ºçŸ¢é‡è¿æ¥ã€‚ Parameter Generation Networkå°†$\\mathbf{v}$é€å…¥ä¸€ä¸ªåŒå‘çš„LSTMå±‚ï¼Œä¸ºäº†å®ç°è·¨é¢†åŸŸå’Œè·¨ä»»åŠ¡çš„çŸ¥è¯†è½¬ç§»ï¼Œä½¿ç”¨ä¸€ä¸ªå‚æ•°ç”Ÿæˆç½‘ç»œ$f(\\cdot,\\cdot,\\cdot)$åŠ¨æ€åœ°ç”Ÿæˆ$\\text{BiLSTM}$çš„å‚æ•°ï¼Œç”±æ­¤äº§ç”Ÿçš„å‚æ•°è¢«è¡¨ç¤ºä¸º$\\theta_{\\text{LSTM}}^{d,t}$ï¼Œå…¶ä¸­$d \\in {src,tgt}$ï¼Œ$t\\in {ner,lm}$ åˆ†åˆ«ä»£è¡¨é¢†åŸŸæ ‡ç­¾å’Œä»»åŠ¡æ ‡ç­¾ $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$è¡¨ç¤ºè¾“å…¥è¯åµŒå…¥ $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$ä»£è¡¨ä¸€ç»„ä»¥ä¸‰é˜¶å¼ é‡å½¢å¼å­˜åœ¨çš„å…ƒå‚æ•° $\\mathbf{I}_d^D\\in \\mathbb{R}^U$ä»£è¡¨é¢†åŸŸè¯åµŒå…¥ $\\mathbf{I}_d^D\\in \\mathbb{R}^V$ä»£è¡¨ä»»åŠ¡è¯åµŒå…¥ $U$ã€ $V$åˆ†åˆ«ä»£è¡¨é¢†åŸŸå’Œä»»åŠ¡è¯åµŒå…¥çš„å¤§å° $P^{(LSTM)}$æ˜¯$\\text{BiLSTM}$å‚æ•°çš„æ•°é‡ $\\otimes$ æŒ‡å¼ é‡æ”¶ç¼© ç»™å®šè¾“å…¥$v$å’Œå‚æ•°$\\theta$ï¼Œä¸€ä¸ªä»»åŠ¡å’Œç‰¹å®šé¢†åŸŸ$\\text{BiLSTM}$å•å…ƒçš„éšè—è¾“å‡ºå¯ä»¥ç»Ÿä¸€å†™æˆ: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$ï¼Œ$\\overleftarrow{\\mathbf{h}}_i^{d,t}$åˆ†åˆ«ä¸ºå‰å‘å’Œåå‘ã€‚ Output Layersæ ‡å‡†CRFsè¢«ç”¨ä½œNERçš„è¾“å‡ºå±‚ï¼Œåœ¨è¾“å…¥å¥å­$\\mathbf{x}$ä¸Šäº§ç”Ÿçš„æ ‡ç­¾åºåˆ—$\\mathbf{y}=l_1,l_2,\\dots,l_i$çš„è¾“å‡ºæ¦‚ç‡$p(\\mathbf{y}\\vert \\mathbf{x})$æ˜¯ $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ å‚æ•°è§£é‡Šï¼š $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$ä»£è¡¨å‰å‘å’Œåå‘çš„ç»„åˆç‰¹å¾ $yâ€™$ä»£è¡¨ä¸€ä¸ªä»»æ„çš„æ ‡ç­¾åºåˆ— $\\mathbf{w}^{li}_{CRF}$æ˜¯$l_i$ç‰¹æœ‰çš„æ¨¡å‹å‚æ•° ${b_{CRF}^{(l_{i-1},l_i)}}$ æ˜¯ $l_{i-1}$ å’Œ $l_i$ç‰¹æœ‰çš„åç½® è€ƒè™‘åˆ°ä¸åŒé¢†åŸŸçš„NERæ ‡ç­¾é›†å¯èƒ½ä¸åŒï¼Œåœ¨Fig-1ä¸­åˆ†åˆ«ç”¨$\\text{CRF(S)}$å’Œ$\\text{CRF(T)}$æ¥è¡¨ç¤ºæºåŸŸå’Œç›®æ ‡åŸŸçš„$\\text{CRFs}$ï¼Œä½¿ç”¨ä¸€é˜¶Viterbiç®—æ³•æ¥å¯»æ‰¾é«˜åˆ†çš„æ ‡ç­¾åºåˆ—ã€‚ Language modelingå‰å‘$\\text{LM(LMf)}$ ä½¿ç”¨å‰å‘LSTMéšè—çŠ¶æ€$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{1:i}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸‹ä¸€ä¸ªè¯$x_{i+1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i+1}\\vert x_{1:i})$ åå‘$\\text{LM(LMb)}$ ä½¿ç”¨åå‘LSTMéšè—çŠ¶æ€$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$ï¼š åœ¨ç»™å®š$x_{i:n}$æƒ…å†µä¸‹æ¥è®¡ç®—ä¸Šä¸€ä¸ªè¯$x_{i-1}$çš„æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸º$p^f (x_{i-1}\\vert x_{i:n})$ è€ƒè™‘åˆ°è®¡ç®—æ•ˆç‡ï¼Œé‡‡ç”¨è´Ÿé‡‡æ ·Softmaxï¼ˆNSSoftmaxï¼‰æ¥è®¡ç®—å‰å‘å’Œåå‘æ¦‚ç‡ï¼Œå…·ä½“å¦‚ä¸‹ï¼š $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#multi-task-learning-algorithm"},{"categories":["documentation"],"content":"å®éªŒç»“æœä¸è®¨è®ºä½œè€…åœ¨ä¸‰ä¸ªè·¨é¢†åŸŸæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåœ¨æœ‰ç›‘ç£çš„é¢†åŸŸé€‚åº”å’Œæ— ç›‘ç£çš„é¢†åŸŸé€‚åº”è®¾ç½®ä¸‹ï¼Œå°†æå‡ºçš„æ–¹æ³•ä¸ä¸€ç³»åˆ—è½¬ç§»å­¦ä¹ åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚ ","date":"2022-03-10","objectID":"/paper03/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#å®éªŒç»“æœä¸è®¨è®º"},{"categories":["documentation"],"content":"ç»“è®ºé€šè¿‡ä»åŸå§‹æ–‡æœ¬ä¸­æå–é¢†åŸŸå·®å¼‚çš„çŸ¥è¯†æ¥è¿›è¡ŒNERé¢†åŸŸé€‚åº”ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œä½œè€…é€šè¿‡ä¸€ä¸ªæ–°çš„å‚æ•°ç”Ÿæˆç½‘ç»œè¿›è¡Œè·¨é¢†åŸŸè¯­è¨€å»ºæ¨¡ï¼Œè¯¥ç½‘ç»œå°†é¢†åŸŸå’Œä»»åŠ¡çŸ¥è¯†åˆ†è§£ä¸ºä¸¤ç»„åµŒå…¥å‘é‡ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–¹æ³•åœ¨æœ‰ç›‘ç£çš„é¢†åŸŸé€‚åº”æ–¹æ³•ä¸­æ˜¯éå¸¸æœ‰æ•ˆçš„ï¼ŒåŒæ—¶å…è®¸åœ¨æ— ç›‘ç£çš„é¢†åŸŸé€‚åº”ä¸­è¿›è¡Œzero-shotå­¦ä¹ ã€‚ ","date":"2022-03-10","objectID":"/paper03/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#ç»“è®º"},{"categories":["documentation"],"content":"ä»£ç https://github.com/jiachenwestlake/Cross-Domain_NER ","date":"2022-03-10","objectID":"/paper03/:0:6","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#ä»£ç "},{"categories":["documentation"],"content":"Transformers Domain Adaptation","date":"2022-03-04","objectID":"/tools01/","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/"},{"categories":["documentation"],"content":"Transformers Domain Adaptationæœ¬æŒ‡å—è¯´æ˜äº†ç«¯åˆ°ç«¯Domain Adaptationå·¥ä½œæµç¨‹ï¼Œå…¶ä¸­æˆ‘ä»¬ä¸ºç”Ÿç‰©åŒ»å­¦NLPåº”ç”¨ç¨‹åºé€‚åº”é¢†åŸŸè½¬æ¢æ¨¡å‹ã€‚ å®ƒå±•ç¤ºäº†æˆ‘ä»¬åœ¨ç ”ç©¶ä¸­ç ”ç©¶çš„ä¸¤ç§é¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯: æ•°æ®é€‰æ‹© (Data Selection) è¯æ±‡é‡å¢åŠ  (Vocabulary Augmentation) æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºè¿™æ ·ä¸€ä¸ªDomain Adaptationçš„Transformeræ¨¡å‹æ˜¯å¦‚ä½•ä¸ğŸ¤—transformerçš„è®­ç»ƒæµç¨‹å…¼å®¹çš„ï¼Œä»¥åŠå®ƒå¦‚ä½•ä¼˜äºå¼€ç®±å³ç”¨çš„(æ— Domain Adaptationçš„)æ¨¡å‹,è¿™äº›æŠ€æœ¯åº”ç”¨äºBERT-smallï¼Œä½†æ˜¯ä»£ç åº“è¢«ç¼–å†™æˆå¯æ¨å¹¿åˆ°HuggingFaceæ”¯æŒçš„å…¶ä»–Transformerç±»ã€‚ è­¦å‘Šå¯¹äºæœ¬æŒ‡å—ï¼Œç”±äºå†…å­˜å’Œæ—¶é—´çš„é™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸŸå†…è¯­æ–™åº“çš„ä¸€ä¸ªå°å¾—å¤šçš„å­é›†(\u003c0.05%)ã€‚ ","date":"2022-03-04","objectID":"/tools01/:1:0","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#transformers-domain-adaptation"},{"categories":["documentation"],"content":"Transformers Domain Adaptationæœ¬æŒ‡å—è¯´æ˜äº†ç«¯åˆ°ç«¯Domain Adaptationå·¥ä½œæµç¨‹ï¼Œå…¶ä¸­æˆ‘ä»¬ä¸ºç”Ÿç‰©åŒ»å­¦NLPåº”ç”¨ç¨‹åºé€‚åº”é¢†åŸŸè½¬æ¢æ¨¡å‹ã€‚ å®ƒå±•ç¤ºäº†æˆ‘ä»¬åœ¨ç ”ç©¶ä¸­ç ”ç©¶çš„ä¸¤ç§é¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯: æ•°æ®é€‰æ‹© (Data Selection) è¯æ±‡é‡å¢åŠ  (Vocabulary Augmentation) æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºè¿™æ ·ä¸€ä¸ªDomain Adaptationçš„Transformeræ¨¡å‹æ˜¯å¦‚ä½•ä¸ğŸ¤—transformerçš„è®­ç»ƒæµç¨‹å…¼å®¹çš„ï¼Œä»¥åŠå®ƒå¦‚ä½•ä¼˜äºå¼€ç®±å³ç”¨çš„(æ— Domain Adaptationçš„)æ¨¡å‹,è¿™äº›æŠ€æœ¯åº”ç”¨äºBERT-smallï¼Œä½†æ˜¯ä»£ç åº“è¢«ç¼–å†™æˆå¯æ¨å¹¿åˆ°HuggingFaceæ”¯æŒçš„å…¶ä»–Transformerç±»ã€‚ è­¦å‘Šå¯¹äºæœ¬æŒ‡å—ï¼Œç”±äºå†…å­˜å’Œæ—¶é—´çš„é™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸŸå†…è¯­æ–™åº“çš„ä¸€ä¸ªå°å¾—å¤šçš„å­é›†(","date":"2022-03-04","objectID":"/tools01/:1:0","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#è­¦å‘Š"},{"categories":["documentation"],"content":"å‡†å¤‡å·¥ä½œå®‰è£…ä¾èµ–ç¨‹åºä½¿ç”¨pipå®‰è£…transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple ä¸‹è½½demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # è§£å‹æ–‡ä»¶ unzip ./files.zip å¸¸é‡æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ¨¡å‹å¡å’Œæ–‡æœ¬è¯­æ–™åº“çš„ç›¸å…³è·¯å¾„ã€‚ åœ¨domain adaptationçš„èƒŒæ™¯ä¸‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„è¯­æ–™åº“ã€‚ å¾®è°ƒè¯­æ–™åº“(Fine-Tuning Corpus) ç»™å®šä¸€ä¸ªNLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç­‰ï¼‰ï¼Œè¿™ä¸ªæ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†å°±æ˜¯å¾®è°ƒè¯­æ–™åº“ã€‚ åœ¨åŸŸè¯­æ–™åº“ (In-Domain Corpus) è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºé¢†åŸŸé¢„è®­ç»ƒã€‚æ–‡æœ¬é¢†åŸŸä¸å¾®è°ƒè¯­æ–™åº“çš„é¢†åŸŸç›¸åŒï¼Œç”šè‡³æ›´å¹¿æ³›ã€‚ # é¢„è®­ç»ƒæ¨¡å‹åç§° model_card = 'bert-base-uncased' # Domain-pre-training corpora é¢†åŸŸé¢„è®­ç»ƒè¯­æ–™ dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' åŠ è½½æ¨¡å‹å’Œtokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) æ•°æ®é€‰æ‹©åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é¢†åŸŸå†…è¯­æ–™åº“çš„æ•°æ®éƒ½å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„æˆ–ç›¸å…³çš„ã€‚å¯¹äºä¸ç›¸å…³çš„æ–‡ä»¶ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸ä¼šé™ä½é¢†åŸŸé€‚åº”æ¨¡å‹çš„æ€§èƒ½ï¼›åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šå€’é€€å¹¶å¤±å»å®è´µçš„é¢„è®­ç»ƒä¿¡æ¯å³ç¾éš¾æ€§çš„é—å¿˜ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Ruder \u0026 Plankè®¾è®¡çš„å„ç§ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»åŸŸå†…è¯­æ–™åº“ä¸­é€‰æ‹©å¯èƒ½ä¸ä¸‹æ¸¸å¾®è°ƒæ•°æ®é›†ç›¸å…³çš„æ–‡æ¡£ã€‚ from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # å°†æ–‡æœ¬æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # åœ¨å¾®è°ƒè¯­æ–™åº“è¿›è¡Œfit selector.fit(fine_tuning_texts) # ä»åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸­é€‰æ‹©ç›¸å…³æ–‡ä»¶ selected_corpus = selector.transform(training_texts) # åœ¨`dpt_corpus_train_data_selected`ä¸‹å°†é€‰å®šçš„è¯­æ–™åº“ä¿å­˜åˆ°ç£ç›˜ Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); ç”±äºæˆ‘ä»¬åœ¨DataSelectorä¸­æŒ‡å®šäº†keep=0.5ï¼Œæ‰€ä»¥é€‰æ‹©çš„è¯­æ–™åº“åº”è¯¥æ˜¯åŸŸå†…è¯­æ–™åº“çš„ä¸€åŠå¤§å°ï¼ŒåŒ…å«å‰50%æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P\u003c0. 05). The rate of Chla and Chlb decreased but the content of Chla,Chlb and Chl( a+b) increased gradually with continued shading. The maximum value of photosynthetically active radiation appeared at 10: 00-12: 00 am in a day. The maximum value of net photosynthetic rate appeared at 8: 30-9: 00 am and the minimum value appeared at 14: 00-14: 30 pm at each phenological stage if plants grew in full sunlight. However,when plants grew in shading,the maximum value of net photosynthetic rate appeared at about 10: 30 am and the minimum value appeared at 12: 20-12: 50 pm at each phenological stage. At expanding leaf stage and flowering stage,the average of net photosynthetic rate of leaves in full sunlight was remarkably higher than those in shading and it decreased greatly with decreasing of irradiance gradually( P \u003c 0. 05). However,at fruiting stage,the average of net photosynthetic rate of leaves in full sunlight was lower than those","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#å‡†å¤‡å·¥ä½œ"},{"categories":["documentation"],"content":"å‡†å¤‡å·¥ä½œå®‰è£…ä¾èµ–ç¨‹åºä½¿ç”¨pipå®‰è£…transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple ä¸‹è½½demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # è§£å‹æ–‡ä»¶ unzip ./files.zip å¸¸é‡æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ¨¡å‹å¡å’Œæ–‡æœ¬è¯­æ–™åº“çš„ç›¸å…³è·¯å¾„ã€‚ åœ¨domain adaptationçš„èƒŒæ™¯ä¸‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„è¯­æ–™åº“ã€‚ å¾®è°ƒè¯­æ–™åº“(Fine-Tuning Corpus) ç»™å®šä¸€ä¸ªNLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç­‰ï¼‰ï¼Œè¿™ä¸ªæ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†å°±æ˜¯å¾®è°ƒè¯­æ–™åº“ã€‚ åœ¨åŸŸè¯­æ–™åº“ (In-Domain Corpus) è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºé¢†åŸŸé¢„è®­ç»ƒã€‚æ–‡æœ¬é¢†åŸŸä¸å¾®è°ƒè¯­æ–™åº“çš„é¢†åŸŸç›¸åŒï¼Œç”šè‡³æ›´å¹¿æ³›ã€‚ # é¢„è®­ç»ƒæ¨¡å‹åç§° model_card = 'bert-base-uncased' # Domain-pre-training corpora é¢†åŸŸé¢„è®­ç»ƒè¯­æ–™ dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' åŠ è½½æ¨¡å‹å’Œtokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) æ•°æ®é€‰æ‹©åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é¢†åŸŸå†…è¯­æ–™åº“çš„æ•°æ®éƒ½å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„æˆ–ç›¸å…³çš„ã€‚å¯¹äºä¸ç›¸å…³çš„æ–‡ä»¶ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸ä¼šé™ä½é¢†åŸŸé€‚åº”æ¨¡å‹çš„æ€§èƒ½ï¼›åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šå€’é€€å¹¶å¤±å»å®è´µçš„é¢„è®­ç»ƒä¿¡æ¯å³ç¾éš¾æ€§çš„é—å¿˜ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Ruder \u0026 Plankè®¾è®¡çš„å„ç§ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»åŸŸå†…è¯­æ–™åº“ä¸­é€‰æ‹©å¯èƒ½ä¸ä¸‹æ¸¸å¾®è°ƒæ•°æ®é›†ç›¸å…³çš„æ–‡æ¡£ã€‚ from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # å°†æ–‡æœ¬æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # åœ¨å¾®è°ƒè¯­æ–™åº“è¿›è¡Œfit selector.fit(fine_tuning_texts) # ä»åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸­é€‰æ‹©ç›¸å…³æ–‡ä»¶ selected_corpus = selector.transform(training_texts) # åœ¨`dpt_corpus_train_data_selected`ä¸‹å°†é€‰å®šçš„è¯­æ–™åº“ä¿å­˜åˆ°ç£ç›˜ Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); ç”±äºæˆ‘ä»¬åœ¨DataSelectorä¸­æŒ‡å®šäº†keep=0.5ï¼Œæ‰€ä»¥é€‰æ‹©çš„è¯­æ–™åº“åº”è¯¥æ˜¯åŸŸå†…è¯­æ–™åº“çš„ä¸€åŠå¤§å°ï¼ŒåŒ…å«å‰50%æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#å®‰è£…ä¾èµ–ç¨‹åº"},{"categories":["documentation"],"content":"å‡†å¤‡å·¥ä½œå®‰è£…ä¾èµ–ç¨‹åºä½¿ç”¨pipå®‰è£…transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple ä¸‹è½½demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # è§£å‹æ–‡ä»¶ unzip ./files.zip å¸¸é‡æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ¨¡å‹å¡å’Œæ–‡æœ¬è¯­æ–™åº“çš„ç›¸å…³è·¯å¾„ã€‚ åœ¨domain adaptationçš„èƒŒæ™¯ä¸‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„è¯­æ–™åº“ã€‚ å¾®è°ƒè¯­æ–™åº“(Fine-Tuning Corpus) ç»™å®šä¸€ä¸ªNLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç­‰ï¼‰ï¼Œè¿™ä¸ªæ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†å°±æ˜¯å¾®è°ƒè¯­æ–™åº“ã€‚ åœ¨åŸŸè¯­æ–™åº“ (In-Domain Corpus) è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºé¢†åŸŸé¢„è®­ç»ƒã€‚æ–‡æœ¬é¢†åŸŸä¸å¾®è°ƒè¯­æ–™åº“çš„é¢†åŸŸç›¸åŒï¼Œç”šè‡³æ›´å¹¿æ³›ã€‚ # é¢„è®­ç»ƒæ¨¡å‹åç§° model_card = 'bert-base-uncased' # Domain-pre-training corpora é¢†åŸŸé¢„è®­ç»ƒè¯­æ–™ dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' åŠ è½½æ¨¡å‹å’Œtokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) æ•°æ®é€‰æ‹©åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é¢†åŸŸå†…è¯­æ–™åº“çš„æ•°æ®éƒ½å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„æˆ–ç›¸å…³çš„ã€‚å¯¹äºä¸ç›¸å…³çš„æ–‡ä»¶ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸ä¼šé™ä½é¢†åŸŸé€‚åº”æ¨¡å‹çš„æ€§èƒ½ï¼›åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šå€’é€€å¹¶å¤±å»å®è´µçš„é¢„è®­ç»ƒä¿¡æ¯å³ç¾éš¾æ€§çš„é—å¿˜ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Ruder \u0026 Plankè®¾è®¡çš„å„ç§ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»åŸŸå†…è¯­æ–™åº“ä¸­é€‰æ‹©å¯èƒ½ä¸ä¸‹æ¸¸å¾®è°ƒæ•°æ®é›†ç›¸å…³çš„æ–‡æ¡£ã€‚ from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # å°†æ–‡æœ¬æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # åœ¨å¾®è°ƒè¯­æ–™åº“è¿›è¡Œfit selector.fit(fine_tuning_texts) # ä»åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸­é€‰æ‹©ç›¸å…³æ–‡ä»¶ selected_corpus = selector.transform(training_texts) # åœ¨`dpt_corpus_train_data_selected`ä¸‹å°†é€‰å®šçš„è¯­æ–™åº“ä¿å­˜åˆ°ç£ç›˜ Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); ç”±äºæˆ‘ä»¬åœ¨DataSelectorä¸­æŒ‡å®šäº†keep=0.5ï¼Œæ‰€ä»¥é€‰æ‹©çš„è¯­æ–™åº“åº”è¯¥æ˜¯åŸŸå†…è¯­æ–™åº“çš„ä¸€åŠå¤§å°ï¼ŒåŒ…å«å‰50%æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#ä¸‹è½½demo-files"},{"categories":["documentation"],"content":"å‡†å¤‡å·¥ä½œå®‰è£…ä¾èµ–ç¨‹åºä½¿ç”¨pipå®‰è£…transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple ä¸‹è½½demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # è§£å‹æ–‡ä»¶ unzip ./files.zip å¸¸é‡æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ¨¡å‹å¡å’Œæ–‡æœ¬è¯­æ–™åº“çš„ç›¸å…³è·¯å¾„ã€‚ åœ¨domain adaptationçš„èƒŒæ™¯ä¸‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„è¯­æ–™åº“ã€‚ å¾®è°ƒè¯­æ–™åº“(Fine-Tuning Corpus) ç»™å®šä¸€ä¸ªNLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç­‰ï¼‰ï¼Œè¿™ä¸ªæ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†å°±æ˜¯å¾®è°ƒè¯­æ–™åº“ã€‚ åœ¨åŸŸè¯­æ–™åº“ (In-Domain Corpus) è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºé¢†åŸŸé¢„è®­ç»ƒã€‚æ–‡æœ¬é¢†åŸŸä¸å¾®è°ƒè¯­æ–™åº“çš„é¢†åŸŸç›¸åŒï¼Œç”šè‡³æ›´å¹¿æ³›ã€‚ # é¢„è®­ç»ƒæ¨¡å‹åç§° model_card = 'bert-base-uncased' # Domain-pre-training corpora é¢†åŸŸé¢„è®­ç»ƒè¯­æ–™ dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' åŠ è½½æ¨¡å‹å’Œtokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) æ•°æ®é€‰æ‹©åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é¢†åŸŸå†…è¯­æ–™åº“çš„æ•°æ®éƒ½å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„æˆ–ç›¸å…³çš„ã€‚å¯¹äºä¸ç›¸å…³çš„æ–‡ä»¶ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸ä¼šé™ä½é¢†åŸŸé€‚åº”æ¨¡å‹çš„æ€§èƒ½ï¼›åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šå€’é€€å¹¶å¤±å»å®è´µçš„é¢„è®­ç»ƒä¿¡æ¯å³ç¾éš¾æ€§çš„é—å¿˜ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Ruder \u0026 Plankè®¾è®¡çš„å„ç§ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»åŸŸå†…è¯­æ–™åº“ä¸­é€‰æ‹©å¯èƒ½ä¸ä¸‹æ¸¸å¾®è°ƒæ•°æ®é›†ç›¸å…³çš„æ–‡æ¡£ã€‚ from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # å°†æ–‡æœ¬æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # åœ¨å¾®è°ƒè¯­æ–™åº“è¿›è¡Œfit selector.fit(fine_tuning_texts) # ä»åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸­é€‰æ‹©ç›¸å…³æ–‡ä»¶ selected_corpus = selector.transform(training_texts) # åœ¨`dpt_corpus_train_data_selected`ä¸‹å°†é€‰å®šçš„è¯­æ–™åº“ä¿å­˜åˆ°ç£ç›˜ Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); ç”±äºæˆ‘ä»¬åœ¨DataSelectorä¸­æŒ‡å®šäº†keep=0.5ï¼Œæ‰€ä»¥é€‰æ‹©çš„è¯­æ–™åº“åº”è¯¥æ˜¯åŸŸå†…è¯­æ–™åº“çš„ä¸€åŠå¤§å°ï¼ŒåŒ…å«å‰50%æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#å¸¸é‡"},{"categories":["documentation"],"content":"å‡†å¤‡å·¥ä½œå®‰è£…ä¾èµ–ç¨‹åºä½¿ç”¨pipå®‰è£…transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple ä¸‹è½½demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # è§£å‹æ–‡ä»¶ unzip ./files.zip å¸¸é‡æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ¨¡å‹å¡å’Œæ–‡æœ¬è¯­æ–™åº“çš„ç›¸å…³è·¯å¾„ã€‚ åœ¨domain adaptationçš„èƒŒæ™¯ä¸‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„è¯­æ–™åº“ã€‚ å¾®è°ƒè¯­æ–™åº“(Fine-Tuning Corpus) ç»™å®šä¸€ä¸ªNLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç­‰ï¼‰ï¼Œè¿™ä¸ªæ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†å°±æ˜¯å¾®è°ƒè¯­æ–™åº“ã€‚ åœ¨åŸŸè¯­æ–™åº“ (In-Domain Corpus) è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºé¢†åŸŸé¢„è®­ç»ƒã€‚æ–‡æœ¬é¢†åŸŸä¸å¾®è°ƒè¯­æ–™åº“çš„é¢†åŸŸç›¸åŒï¼Œç”šè‡³æ›´å¹¿æ³›ã€‚ # é¢„è®­ç»ƒæ¨¡å‹åç§° model_card = 'bert-base-uncased' # Domain-pre-training corpora é¢†åŸŸé¢„è®­ç»ƒè¯­æ–™ dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' åŠ è½½æ¨¡å‹å’Œtokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) æ•°æ®é€‰æ‹©åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é¢†åŸŸå†…è¯­æ–™åº“çš„æ•°æ®éƒ½å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„æˆ–ç›¸å…³çš„ã€‚å¯¹äºä¸ç›¸å…³çš„æ–‡ä»¶ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸ä¼šé™ä½é¢†åŸŸé€‚åº”æ¨¡å‹çš„æ€§èƒ½ï¼›åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šå€’é€€å¹¶å¤±å»å®è´µçš„é¢„è®­ç»ƒä¿¡æ¯å³ç¾éš¾æ€§çš„é—å¿˜ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Ruder \u0026 Plankè®¾è®¡çš„å„ç§ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»åŸŸå†…è¯­æ–™åº“ä¸­é€‰æ‹©å¯èƒ½ä¸ä¸‹æ¸¸å¾®è°ƒæ•°æ®é›†ç›¸å…³çš„æ–‡æ¡£ã€‚ from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # å°†æ–‡æœ¬æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # åœ¨å¾®è°ƒè¯­æ–™åº“è¿›è¡Œfit selector.fit(fine_tuning_texts) # ä»åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸­é€‰æ‹©ç›¸å…³æ–‡ä»¶ selected_corpus = selector.transform(training_texts) # åœ¨`dpt_corpus_train_data_selected`ä¸‹å°†é€‰å®šçš„è¯­æ–™åº“ä¿å­˜åˆ°ç£ç›˜ Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); ç”±äºæˆ‘ä»¬åœ¨DataSelectorä¸­æŒ‡å®šäº†keep=0.5ï¼Œæ‰€ä»¥é€‰æ‹©çš„è¯­æ–™åº“åº”è¯¥æ˜¯åŸŸå†…è¯­æ–™åº“çš„ä¸€åŠå¤§å°ï¼ŒåŒ…å«å‰50%æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#åŠ è½½æ¨¡å‹å’Œtokenizer"},{"categories":["documentation"],"content":"å‡†å¤‡å·¥ä½œå®‰è£…ä¾èµ–ç¨‹åºä½¿ç”¨pipå®‰è£…transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple ä¸‹è½½demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # è§£å‹æ–‡ä»¶ unzip ./files.zip å¸¸é‡æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ¨¡å‹å¡å’Œæ–‡æœ¬è¯­æ–™åº“çš„ç›¸å…³è·¯å¾„ã€‚ åœ¨domain adaptationçš„èƒŒæ™¯ä¸‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„è¯­æ–™åº“ã€‚ å¾®è°ƒè¯­æ–™åº“(Fine-Tuning Corpus) ç»™å®šä¸€ä¸ªNLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç­‰ï¼‰ï¼Œè¿™ä¸ªæ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†å°±æ˜¯å¾®è°ƒè¯­æ–™åº“ã€‚ åœ¨åŸŸè¯­æ–™åº“ (In-Domain Corpus) è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºé¢†åŸŸé¢„è®­ç»ƒã€‚æ–‡æœ¬é¢†åŸŸä¸å¾®è°ƒè¯­æ–™åº“çš„é¢†åŸŸç›¸åŒï¼Œç”šè‡³æ›´å¹¿æ³›ã€‚ # é¢„è®­ç»ƒæ¨¡å‹åç§° model_card = 'bert-base-uncased' # Domain-pre-training corpora é¢†åŸŸé¢„è®­ç»ƒè¯­æ–™ dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' åŠ è½½æ¨¡å‹å’Œtokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) æ•°æ®é€‰æ‹©åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é¢†åŸŸå†…è¯­æ–™åº“çš„æ•°æ®éƒ½å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„æˆ–ç›¸å…³çš„ã€‚å¯¹äºä¸ç›¸å…³çš„æ–‡ä»¶ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸ä¼šé™ä½é¢†åŸŸé€‚åº”æ¨¡å‹çš„æ€§èƒ½ï¼›åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šå€’é€€å¹¶å¤±å»å®è´µçš„é¢„è®­ç»ƒä¿¡æ¯å³ç¾éš¾æ€§çš„é—å¿˜ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Ruder \u0026 Plankè®¾è®¡çš„å„ç§ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»åŸŸå†…è¯­æ–™åº“ä¸­é€‰æ‹©å¯èƒ½ä¸ä¸‹æ¸¸å¾®è°ƒæ•°æ®é›†ç›¸å…³çš„æ–‡æ¡£ã€‚ from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # å°†æ–‡æœ¬æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # åœ¨å¾®è°ƒè¯­æ–™åº“è¿›è¡Œfit selector.fit(fine_tuning_texts) # ä»åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸­é€‰æ‹©ç›¸å…³æ–‡ä»¶ selected_corpus = selector.transform(training_texts) # åœ¨`dpt_corpus_train_data_selected`ä¸‹å°†é€‰å®šçš„è¯­æ–™åº“ä¿å­˜åˆ°ç£ç›˜ Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); ç”±äºæˆ‘ä»¬åœ¨DataSelectorä¸­æŒ‡å®šäº†keep=0.5ï¼Œæ‰€ä»¥é€‰æ‹©çš„è¯­æ–™åº“åº”è¯¥æ˜¯åŸŸå†…è¯­æ–™åº“çš„ä¸€åŠå¤§å°ï¼ŒåŒ…å«å‰50%æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#æ•°æ®é€‰æ‹©"},{"categories":["documentation"],"content":"å‡†å¤‡å·¥ä½œå®‰è£…ä¾èµ–ç¨‹åºä½¿ç”¨pipå®‰è£…transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple ä¸‹è½½demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # è§£å‹æ–‡ä»¶ unzip ./files.zip å¸¸é‡æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ¨¡å‹å¡å’Œæ–‡æœ¬è¯­æ–™åº“çš„ç›¸å…³è·¯å¾„ã€‚ åœ¨domain adaptationçš„èƒŒæ™¯ä¸‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„è¯­æ–™åº“ã€‚ å¾®è°ƒè¯­æ–™åº“(Fine-Tuning Corpus) ç»™å®šä¸€ä¸ªNLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç­‰ï¼‰ï¼Œè¿™ä¸ªæ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†å°±æ˜¯å¾®è°ƒè¯­æ–™åº“ã€‚ åœ¨åŸŸè¯­æ–™åº“ (In-Domain Corpus) è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºé¢†åŸŸé¢„è®­ç»ƒã€‚æ–‡æœ¬é¢†åŸŸä¸å¾®è°ƒè¯­æ–™åº“çš„é¢†åŸŸç›¸åŒï¼Œç”šè‡³æ›´å¹¿æ³›ã€‚ # é¢„è®­ç»ƒæ¨¡å‹åç§° model_card = 'bert-base-uncased' # Domain-pre-training corpora é¢†åŸŸé¢„è®­ç»ƒè¯­æ–™ dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' åŠ è½½æ¨¡å‹å’Œtokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) æ•°æ®é€‰æ‹©åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é¢†åŸŸå†…è¯­æ–™åº“çš„æ•°æ®éƒ½å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„æˆ–ç›¸å…³çš„ã€‚å¯¹äºä¸ç›¸å…³çš„æ–‡ä»¶ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸ä¼šé™ä½é¢†åŸŸé€‚åº”æ¨¡å‹çš„æ€§èƒ½ï¼›åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šå€’é€€å¹¶å¤±å»å®è´µçš„é¢„è®­ç»ƒä¿¡æ¯å³ç¾éš¾æ€§çš„é—å¿˜ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Ruder \u0026 Plankè®¾è®¡çš„å„ç§ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»åŸŸå†…è¯­æ–™åº“ä¸­é€‰æ‹©å¯èƒ½ä¸ä¸‹æ¸¸å¾®è°ƒæ•°æ®é›†ç›¸å…³çš„æ–‡æ¡£ã€‚ from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # å°†æ–‡æœ¬æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # åœ¨å¾®è°ƒè¯­æ–™åº“è¿›è¡Œfit selector.fit(fine_tuning_texts) # ä»åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸­é€‰æ‹©ç›¸å…³æ–‡ä»¶ selected_corpus = selector.transform(training_texts) # åœ¨`dpt_corpus_train_data_selected`ä¸‹å°†é€‰å®šçš„è¯­æ–™åº“ä¿å­˜åˆ°ç£ç›˜ Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); ç”±äºæˆ‘ä»¬åœ¨DataSelectorä¸­æŒ‡å®šäº†keep=0.5ï¼Œæ‰€ä»¥é€‰æ‹©çš„è¯­æ–™åº“åº”è¯¥æ˜¯åŸŸå†…è¯­æ–™åº“çš„ä¸€åŠå¤§å°ï¼ŒåŒ…å«å‰50%æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#è¯æ±‡æ‰©å……"},{"categories":["documentation"],"content":"å‡†å¤‡å·¥ä½œå®‰è£…ä¾èµ–ç¨‹åºä½¿ç”¨pipå®‰è£…transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple ä¸‹è½½demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # è§£å‹æ–‡ä»¶ unzip ./files.zip å¸¸é‡æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ¨¡å‹å¡å’Œæ–‡æœ¬è¯­æ–™åº“çš„ç›¸å…³è·¯å¾„ã€‚ åœ¨domain adaptationçš„èƒŒæ™¯ä¸‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„è¯­æ–™åº“ã€‚ å¾®è°ƒè¯­æ–™åº“(Fine-Tuning Corpus) ç»™å®šä¸€ä¸ªNLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç­‰ï¼‰ï¼Œè¿™ä¸ªæ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†å°±æ˜¯å¾®è°ƒè¯­æ–™åº“ã€‚ åœ¨åŸŸè¯­æ–™åº“ (In-Domain Corpus) è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºé¢†åŸŸé¢„è®­ç»ƒã€‚æ–‡æœ¬é¢†åŸŸä¸å¾®è°ƒè¯­æ–™åº“çš„é¢†åŸŸç›¸åŒï¼Œç”šè‡³æ›´å¹¿æ³›ã€‚ # é¢„è®­ç»ƒæ¨¡å‹åç§° model_card = 'bert-base-uncased' # Domain-pre-training corpora é¢†åŸŸé¢„è®­ç»ƒè¯­æ–™ dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' åŠ è½½æ¨¡å‹å’Œtokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) æ•°æ®é€‰æ‹©åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é¢†åŸŸå†…è¯­æ–™åº“çš„æ•°æ®éƒ½å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„æˆ–ç›¸å…³çš„ã€‚å¯¹äºä¸ç›¸å…³çš„æ–‡ä»¶ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸ä¼šé™ä½é¢†åŸŸé€‚åº”æ¨¡å‹çš„æ€§èƒ½ï¼›åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šå€’é€€å¹¶å¤±å»å®è´µçš„é¢„è®­ç»ƒä¿¡æ¯å³ç¾éš¾æ€§çš„é—å¿˜ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Ruder \u0026 Plankè®¾è®¡çš„å„ç§ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»åŸŸå†…è¯­æ–™åº“ä¸­é€‰æ‹©å¯èƒ½ä¸ä¸‹æ¸¸å¾®è°ƒæ•°æ®é›†ç›¸å…³çš„æ–‡æ¡£ã€‚ from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # å°†æ–‡æœ¬æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # åœ¨å¾®è°ƒè¯­æ–™åº“è¿›è¡Œfit selector.fit(fine_tuning_texts) # ä»åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸­é€‰æ‹©ç›¸å…³æ–‡ä»¶ selected_corpus = selector.transform(training_texts) # åœ¨`dpt_corpus_train_data_selected`ä¸‹å°†é€‰å®šçš„è¯­æ–™åº“ä¿å­˜åˆ°ç£ç›˜ Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); ç”±äºæˆ‘ä»¬åœ¨DataSelectorä¸­æŒ‡å®šäº†keep=0.5ï¼Œæ‰€ä»¥é€‰æ‹©çš„è¯­æ–™åº“åº”è¯¥æ˜¯åŸŸå†…è¯­æ–™åº“çš„ä¸€åŠå¤§å°ï¼ŒåŒ…å«å‰50%æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#ç”¨æ–°çš„è¯æ±‡æœ¯è¯­æ›´æ–°æ¨¡å‹å’Œtokenizer"},{"categories":["documentation"],"content":"Domain Pre-TrainingDomain PreTrainingæ˜¯Domain Adaptationçš„ç¬¬ä¸‰æ­¥ï¼Œæˆ‘ä»¬åœ¨é¢†åŸŸå†…è¯­æ–™åº“ä¸Šç”¨åŒæ ·çš„é¢„è®­ç»ƒç¨‹åºç»§ç»­è®­ç»ƒTransformeræ¨¡å‹ã€‚ åˆ›å»ºæ•°æ®é›† import itertools as it from pathlib import Path from typing import Sequence, Union, Generator from datasets import load_dataset from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments datasets = load_dataset( 'text', data_files={ \"train\": dpt_corpus_train_data_selected, \"val\": dpt_corpus_val } ) tokenized_datasets = datasets.map( lambda examples: tokenizer(examples['text'], truncation=True, max_length=model.config.max_position_embeddings), batched=True ) data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm=True, mlm_probability=0.15 ) å®ä¾‹åŒ–TrainingArgumentså’ŒTrainer training_args = TrainingArguments( output_dir=\"./results/domain_pre_training\", overwrite_output_dir=True, max_steps=100, per_device_train_batch_size=8, per_device_eval_batch_size=16, evaluation_strategy=\"steps\", save_steps=50, save_total_limit=2, logging_steps=50, seed=42, # fp16=True, dataloader_num_workers=2, disable_tqdm=False ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'], data_collator=data_collator, tokenizer=tokenizer, # è¿™ä¸ªæ ‡è®°å™¨æœ‰æ–°çš„tokens ) # è¿›è¡Œè®­ç»ƒ trainer.train() è®­ç»ƒç»“æœ Step Training Loss Validation Loss Runtime Samples Per Second 50 2.813800 2.409768 75.058500 13.323000 100 2.520700 2.342451 74.257200 13.467000 ","date":"2022-03-04","objectID":"/tools01/:1:2","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#domain-pre-training"},{"categories":["documentation"],"content":"Domain Pre-TrainingDomain PreTrainingæ˜¯Domain Adaptationçš„ç¬¬ä¸‰æ­¥ï¼Œæˆ‘ä»¬åœ¨é¢†åŸŸå†…è¯­æ–™åº“ä¸Šç”¨åŒæ ·çš„é¢„è®­ç»ƒç¨‹åºç»§ç»­è®­ç»ƒTransformeræ¨¡å‹ã€‚ åˆ›å»ºæ•°æ®é›† import itertools as it from pathlib import Path from typing import Sequence, Union, Generator from datasets import load_dataset from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments datasets = load_dataset( 'text', data_files={ \"train\": dpt_corpus_train_data_selected, \"val\": dpt_corpus_val } ) tokenized_datasets = datasets.map( lambda examples: tokenizer(examples['text'], truncation=True, max_length=model.config.max_position_embeddings), batched=True ) data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm=True, mlm_probability=0.15 ) å®ä¾‹åŒ–TrainingArgumentså’ŒTrainer training_args = TrainingArguments( output_dir=\"./results/domain_pre_training\", overwrite_output_dir=True, max_steps=100, per_device_train_batch_size=8, per_device_eval_batch_size=16, evaluation_strategy=\"steps\", save_steps=50, save_total_limit=2, logging_steps=50, seed=42, # fp16=True, dataloader_num_workers=2, disable_tqdm=False ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'], data_collator=data_collator, tokenizer=tokenizer, # è¿™ä¸ªæ ‡è®°å™¨æœ‰æ–°çš„tokens ) # è¿›è¡Œè®­ç»ƒ trainer.train() è®­ç»ƒç»“æœ Step Training Loss Validation Loss Runtime Samples Per Second 50 2.813800 2.409768 75.058500 13.323000 100 2.520700 2.342451 74.257200 13.467000 ","date":"2022-03-04","objectID":"/tools01/:1:2","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#åˆ›å»ºæ•°æ®é›†"},{"categories":["documentation"],"content":"Domain Pre-TrainingDomain PreTrainingæ˜¯Domain Adaptationçš„ç¬¬ä¸‰æ­¥ï¼Œæˆ‘ä»¬åœ¨é¢†åŸŸå†…è¯­æ–™åº“ä¸Šç”¨åŒæ ·çš„é¢„è®­ç»ƒç¨‹åºç»§ç»­è®­ç»ƒTransformeræ¨¡å‹ã€‚ åˆ›å»ºæ•°æ®é›† import itertools as it from pathlib import Path from typing import Sequence, Union, Generator from datasets import load_dataset from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments datasets = load_dataset( 'text', data_files={ \"train\": dpt_corpus_train_data_selected, \"val\": dpt_corpus_val } ) tokenized_datasets = datasets.map( lambda examples: tokenizer(examples['text'], truncation=True, max_length=model.config.max_position_embeddings), batched=True ) data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm=True, mlm_probability=0.15 ) å®ä¾‹åŒ–TrainingArgumentså’ŒTrainer training_args = TrainingArguments( output_dir=\"./results/domain_pre_training\", overwrite_output_dir=True, max_steps=100, per_device_train_batch_size=8, per_device_eval_batch_size=16, evaluation_strategy=\"steps\", save_steps=50, save_total_limit=2, logging_steps=50, seed=42, # fp16=True, dataloader_num_workers=2, disable_tqdm=False ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'], data_collator=data_collator, tokenizer=tokenizer, # è¿™ä¸ªæ ‡è®°å™¨æœ‰æ–°çš„tokens ) # è¿›è¡Œè®­ç»ƒ trainer.train() è®­ç»ƒç»“æœ Step Training Loss Validation Loss Runtime Samples Per Second 50 2.813800 2.409768 75.058500 13.323000 100 2.520700 2.342451 74.257200 13.467000 ","date":"2022-03-04","objectID":"/tools01/:1:2","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#å®ä¾‹åŒ–trainingargumentså’Œtrainer"},{"categories":["documentation"],"content":"ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ‘ä»¬å¯ä»¥ä¸ºHuggingFaceæ”¯æŒçš„ä»»ä½•å¾®è°ƒä»»åŠ¡æ’å…¥æˆ‘ä»¬çš„domain adaptationæ¨¡å‹ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨BC2GMæ•°æ®é›†ï¼ˆä¸€ä¸ªæµè¡Œçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼‰ä¸Šæ¯”è¾ƒä¸€ä¸ªå¼€ç®±å³ç”¨ï¼ˆOOBï¼‰æ¨¡å‹ä¸ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç”¨äºNERé¢„å¤„ç†å’Œè¯„ä¼°çš„å®ç”¨å‡½æ•°æ”¹ç¼–è‡ªHuggingFaceçš„NERå¾®è°ƒç¤ºä¾‹ç¬”è®°ã€‚ å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå½¢æˆNERæ•°æ®é›† from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # ç‰¹æ®Šæ ‡è®°æœ‰ä¸€ä¸ªå•è¯IDï¼Œæ˜¯Noneã€‚æˆ‘ä»¬å°†æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œå› æ­¤å®ƒä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¢«è‡ªåŠ¨å¿½ç•¥äº†ã€‚ if word_idx is None: label_ids.append(-100) # æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è®¾ç½®æ ‡ç­¾ã€‚ elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # å¯¹äºä¸€ä¸ªè¯ä¸­çš„å…¶ä»–æ ‡è®°ï¼Œæˆ‘ä»¬æ ¹æ®label_all_tokensçš„æ ‡å¿—ï¼Œå°†æ ‡ç­¾è®¾ç½®ä¸ºå½“å‰æ ‡ç­¾æˆ–-100ã€‚ else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # ç§»é™¤è¢«å¿½ç•¥çš„ç´¢å¼•ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰ã€‚ true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } å®‰è£… seqeval pip install seqeval seqevalæ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ ‡è®°è¯„ä¼°çš„Pythonæ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°åˆ†å—ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€éƒ¨åˆ†è¯­éŸ³æ ‡è®°ã€è¯­ä¹‰è§’è‰²æ ‡è®°ç­‰ã€‚ label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} å®ä¾‹åŒ–NERæ¨¡å‹åœ¨æ­¤ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„NERæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ: da_model: æˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­åˆšåˆšè®­ç»ƒçš„ä¸€ä¸ªDomain Adaptationçš„NERæ¨¡å‹ da_full_corpus_model: åŒæ ·çš„é¢†åŸŸé€‚åº”æ€§NERæ¨¡å‹ï¼Œåªæ˜¯å®ƒæ˜¯åœ¨å®Œæ•´çš„é¢†åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚ oob_model: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„BERT-NERæ¨¡å‹ï¼ˆæ²¡æœ‰ç»è¿‡Domain Adaptationï¼‰ã€‚ from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒ"},{"categories":["documentation"],"content":"ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ‘ä»¬å¯ä»¥ä¸ºHuggingFaceæ”¯æŒçš„ä»»ä½•å¾®è°ƒä»»åŠ¡æ’å…¥æˆ‘ä»¬çš„domain adaptationæ¨¡å‹ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨BC2GMæ•°æ®é›†ï¼ˆä¸€ä¸ªæµè¡Œçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼‰ä¸Šæ¯”è¾ƒä¸€ä¸ªå¼€ç®±å³ç”¨ï¼ˆOOBï¼‰æ¨¡å‹ä¸ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç”¨äºNERé¢„å¤„ç†å’Œè¯„ä¼°çš„å®ç”¨å‡½æ•°æ”¹ç¼–è‡ªHuggingFaceçš„NERå¾®è°ƒç¤ºä¾‹ç¬”è®°ã€‚ å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå½¢æˆNERæ•°æ®é›† from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # ç‰¹æ®Šæ ‡è®°æœ‰ä¸€ä¸ªå•è¯IDï¼Œæ˜¯Noneã€‚æˆ‘ä»¬å°†æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œå› æ­¤å®ƒä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¢«è‡ªåŠ¨å¿½ç•¥äº†ã€‚ if word_idx is None: label_ids.append(-100) # æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è®¾ç½®æ ‡ç­¾ã€‚ elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # å¯¹äºä¸€ä¸ªè¯ä¸­çš„å…¶ä»–æ ‡è®°ï¼Œæˆ‘ä»¬æ ¹æ®label_all_tokensçš„æ ‡å¿—ï¼Œå°†æ ‡ç­¾è®¾ç½®ä¸ºå½“å‰æ ‡ç­¾æˆ–-100ã€‚ else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # ç§»é™¤è¢«å¿½ç•¥çš„ç´¢å¼•ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰ã€‚ true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } å®‰è£… seqeval pip install seqeval seqevalæ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ ‡è®°è¯„ä¼°çš„Pythonæ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°åˆ†å—ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€éƒ¨åˆ†è¯­éŸ³æ ‡è®°ã€è¯­ä¹‰è§’è‰²æ ‡è®°ç­‰ã€‚ label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} å®ä¾‹åŒ–NERæ¨¡å‹åœ¨æ­¤ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„NERæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ: da_model: æˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­åˆšåˆšè®­ç»ƒçš„ä¸€ä¸ªDomain Adaptationçš„NERæ¨¡å‹ da_full_corpus_model: åŒæ ·çš„é¢†åŸŸé€‚åº”æ€§NERæ¨¡å‹ï¼Œåªæ˜¯å®ƒæ˜¯åœ¨å®Œæ•´çš„é¢†åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚ oob_model: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„BERT-NERæ¨¡å‹ï¼ˆæ²¡æœ‰ç»è¿‡Domain Adaptationï¼‰ã€‚ from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†å½¢æˆneræ•°æ®é›†"},{"categories":["documentation"],"content":"ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ‘ä»¬å¯ä»¥ä¸ºHuggingFaceæ”¯æŒçš„ä»»ä½•å¾®è°ƒä»»åŠ¡æ’å…¥æˆ‘ä»¬çš„domain adaptationæ¨¡å‹ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨BC2GMæ•°æ®é›†ï¼ˆä¸€ä¸ªæµè¡Œçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼‰ä¸Šæ¯”è¾ƒä¸€ä¸ªå¼€ç®±å³ç”¨ï¼ˆOOBï¼‰æ¨¡å‹ä¸ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç”¨äºNERé¢„å¤„ç†å’Œè¯„ä¼°çš„å®ç”¨å‡½æ•°æ”¹ç¼–è‡ªHuggingFaceçš„NERå¾®è°ƒç¤ºä¾‹ç¬”è®°ã€‚ å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå½¢æˆNERæ•°æ®é›† from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # ç‰¹æ®Šæ ‡è®°æœ‰ä¸€ä¸ªå•è¯IDï¼Œæ˜¯Noneã€‚æˆ‘ä»¬å°†æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œå› æ­¤å®ƒä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¢«è‡ªåŠ¨å¿½ç•¥äº†ã€‚ if word_idx is None: label_ids.append(-100) # æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è®¾ç½®æ ‡ç­¾ã€‚ elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # å¯¹äºä¸€ä¸ªè¯ä¸­çš„å…¶ä»–æ ‡è®°ï¼Œæˆ‘ä»¬æ ¹æ®label_all_tokensçš„æ ‡å¿—ï¼Œå°†æ ‡ç­¾è®¾ç½®ä¸ºå½“å‰æ ‡ç­¾æˆ–-100ã€‚ else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # ç§»é™¤è¢«å¿½ç•¥çš„ç´¢å¼•ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰ã€‚ true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } å®‰è£… seqeval pip install seqeval seqevalæ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ ‡è®°è¯„ä¼°çš„Pythonæ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°åˆ†å—ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€éƒ¨åˆ†è¯­éŸ³æ ‡è®°ã€è¯­ä¹‰è§’è‰²æ ‡è®°ç­‰ã€‚ label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} å®ä¾‹åŒ–NERæ¨¡å‹åœ¨æ­¤ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„NERæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ: da_model: æˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­åˆšåˆšè®­ç»ƒçš„ä¸€ä¸ªDomain Adaptationçš„NERæ¨¡å‹ da_full_corpus_model: åŒæ ·çš„é¢†åŸŸé€‚åº”æ€§NERæ¨¡å‹ï¼Œåªæ˜¯å®ƒæ˜¯åœ¨å®Œæ•´çš„é¢†åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚ oob_model: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„BERT-NERæ¨¡å‹ï¼ˆæ²¡æœ‰ç»è¿‡Domain Adaptationï¼‰ã€‚ from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#å®‰è£…-seqeval"},{"categories":["documentation"],"content":"ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ‘ä»¬å¯ä»¥ä¸ºHuggingFaceæ”¯æŒçš„ä»»ä½•å¾®è°ƒä»»åŠ¡æ’å…¥æˆ‘ä»¬çš„domain adaptationæ¨¡å‹ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨BC2GMæ•°æ®é›†ï¼ˆä¸€ä¸ªæµè¡Œçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼‰ä¸Šæ¯”è¾ƒä¸€ä¸ªå¼€ç®±å³ç”¨ï¼ˆOOBï¼‰æ¨¡å‹ä¸ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç”¨äºNERé¢„å¤„ç†å’Œè¯„ä¼°çš„å®ç”¨å‡½æ•°æ”¹ç¼–è‡ªHuggingFaceçš„NERå¾®è°ƒç¤ºä¾‹ç¬”è®°ã€‚ å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå½¢æˆNERæ•°æ®é›† from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # ç‰¹æ®Šæ ‡è®°æœ‰ä¸€ä¸ªå•è¯IDï¼Œæ˜¯Noneã€‚æˆ‘ä»¬å°†æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œå› æ­¤å®ƒä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¢«è‡ªåŠ¨å¿½ç•¥äº†ã€‚ if word_idx is None: label_ids.append(-100) # æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è®¾ç½®æ ‡ç­¾ã€‚ elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # å¯¹äºä¸€ä¸ªè¯ä¸­çš„å…¶ä»–æ ‡è®°ï¼Œæˆ‘ä»¬æ ¹æ®label_all_tokensçš„æ ‡å¿—ï¼Œå°†æ ‡ç­¾è®¾ç½®ä¸ºå½“å‰æ ‡ç­¾æˆ–-100ã€‚ else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # ç§»é™¤è¢«å¿½ç•¥çš„ç´¢å¼•ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰ã€‚ true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } å®‰è£… seqeval pip install seqeval seqevalæ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ ‡è®°è¯„ä¼°çš„Pythonæ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°åˆ†å—ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€éƒ¨åˆ†è¯­éŸ³æ ‡è®°ã€è¯­ä¹‰è§’è‰²æ ‡è®°ç­‰ã€‚ label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} å®ä¾‹åŒ–NERæ¨¡å‹åœ¨æ­¤ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„NERæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ: da_model: æˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­åˆšåˆšè®­ç»ƒçš„ä¸€ä¸ªDomain Adaptationçš„NERæ¨¡å‹ da_full_corpus_model: åŒæ ·çš„é¢†åŸŸé€‚åº”æ€§NERæ¨¡å‹ï¼Œåªæ˜¯å®ƒæ˜¯åœ¨å®Œæ•´çš„é¢†åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚ oob_model: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„BERT-NERæ¨¡å‹ï¼ˆæ²¡æœ‰ç»è¿‡Domain Adaptationï¼‰ã€‚ from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#å®ä¾‹åŒ–neræ¨¡å‹"},{"categories":["documentation"],"content":"ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ‘ä»¬å¯ä»¥ä¸ºHuggingFaceæ”¯æŒçš„ä»»ä½•å¾®è°ƒä»»åŠ¡æ’å…¥æˆ‘ä»¬çš„domain adaptationæ¨¡å‹ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨BC2GMæ•°æ®é›†ï¼ˆä¸€ä¸ªæµè¡Œçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼‰ä¸Šæ¯”è¾ƒä¸€ä¸ªå¼€ç®±å³ç”¨ï¼ˆOOBï¼‰æ¨¡å‹ä¸ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç”¨äºNERé¢„å¤„ç†å’Œè¯„ä¼°çš„å®ç”¨å‡½æ•°æ”¹ç¼–è‡ªHuggingFaceçš„NERå¾®è°ƒç¤ºä¾‹ç¬”è®°ã€‚ å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå½¢æˆNERæ•°æ®é›† from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # ç‰¹æ®Šæ ‡è®°æœ‰ä¸€ä¸ªå•è¯IDï¼Œæ˜¯Noneã€‚æˆ‘ä»¬å°†æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œå› æ­¤å®ƒä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¢«è‡ªåŠ¨å¿½ç•¥äº†ã€‚ if word_idx is None: label_ids.append(-100) # æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è®¾ç½®æ ‡ç­¾ã€‚ elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # å¯¹äºä¸€ä¸ªè¯ä¸­çš„å…¶ä»–æ ‡è®°ï¼Œæˆ‘ä»¬æ ¹æ®label_all_tokensçš„æ ‡å¿—ï¼Œå°†æ ‡ç­¾è®¾ç½®ä¸ºå½“å‰æ ‡ç­¾æˆ–-100ã€‚ else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # ç§»é™¤è¢«å¿½ç•¥çš„ç´¢å¼•ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰ã€‚ true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } å®‰è£… seqeval pip install seqeval seqevalæ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ ‡è®°è¯„ä¼°çš„Pythonæ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°åˆ†å—ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€éƒ¨åˆ†è¯­éŸ³æ ‡è®°ã€è¯­ä¹‰è§’è‰²æ ‡è®°ç­‰ã€‚ label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} å®ä¾‹åŒ–NERæ¨¡å‹åœ¨æ­¤ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„NERæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ: da_model: æˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­åˆšåˆšè®­ç»ƒçš„ä¸€ä¸ªDomain Adaptationçš„NERæ¨¡å‹ da_full_corpus_model: åŒæ ·çš„é¢†åŸŸé€‚åº”æ€§NERæ¨¡å‹ï¼Œåªæ˜¯å®ƒæ˜¯åœ¨å®Œæ•´çš„é¢†åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚ oob_model: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„BERT-NERæ¨¡å‹ï¼ˆæ²¡æœ‰ç»è¿‡Domain Adaptationï¼‰ã€‚ from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºæ•°æ®é›†trainingargumentså’Œtrainer"},{"categories":["documentation"],"content":"ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ‘ä»¬å¯ä»¥ä¸ºHuggingFaceæ”¯æŒçš„ä»»ä½•å¾®è°ƒä»»åŠ¡æ’å…¥æˆ‘ä»¬çš„domain adaptationæ¨¡å‹ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨BC2GMæ•°æ®é›†ï¼ˆä¸€ä¸ªæµè¡Œçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼‰ä¸Šæ¯”è¾ƒä¸€ä¸ªå¼€ç®±å³ç”¨ï¼ˆOOBï¼‰æ¨¡å‹ä¸ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç”¨äºNERé¢„å¤„ç†å’Œè¯„ä¼°çš„å®ç”¨å‡½æ•°æ”¹ç¼–è‡ªHuggingFaceçš„NERå¾®è°ƒç¤ºä¾‹ç¬”è®°ã€‚ å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå½¢æˆNERæ•°æ®é›† from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # ç‰¹æ®Šæ ‡è®°æœ‰ä¸€ä¸ªå•è¯IDï¼Œæ˜¯Noneã€‚æˆ‘ä»¬å°†æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œå› æ­¤å®ƒä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¢«è‡ªåŠ¨å¿½ç•¥äº†ã€‚ if word_idx is None: label_ids.append(-100) # æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è®¾ç½®æ ‡ç­¾ã€‚ elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # å¯¹äºä¸€ä¸ªè¯ä¸­çš„å…¶ä»–æ ‡è®°ï¼Œæˆ‘ä»¬æ ¹æ®label_all_tokensçš„æ ‡å¿—ï¼Œå°†æ ‡ç­¾è®¾ç½®ä¸ºå½“å‰æ ‡ç­¾æˆ–-100ã€‚ else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # ç§»é™¤è¢«å¿½ç•¥çš„ç´¢å¼•ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰ã€‚ true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } å®‰è£… seqeval pip install seqeval seqevalæ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ ‡è®°è¯„ä¼°çš„Pythonæ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°åˆ†å—ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€éƒ¨åˆ†è¯­éŸ³æ ‡è®°ã€è¯­ä¹‰è§’è‰²æ ‡è®°ç­‰ã€‚ label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} å®ä¾‹åŒ–NERæ¨¡å‹åœ¨æ­¤ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„NERæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ: da_model: æˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­åˆšåˆšè®­ç»ƒçš„ä¸€ä¸ªDomain Adaptationçš„NERæ¨¡å‹ da_full_corpus_model: åŒæ ·çš„é¢†åŸŸé€‚åº”æ€§NERæ¨¡å‹ï¼Œåªæ˜¯å®ƒæ˜¯åœ¨å®Œæ•´çš„é¢†åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚ oob_model: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„BERT-NERæ¨¡å‹ï¼ˆæ²¡æœ‰ç»è¿‡Domain Adaptationï¼‰ã€‚ from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#è®­ç»ƒå’Œè¯„ä¼°da_model"},{"categories":["documentation"],"content":"ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ‘ä»¬å¯ä»¥ä¸ºHuggingFaceæ”¯æŒçš„ä»»ä½•å¾®è°ƒä»»åŠ¡æ’å…¥æˆ‘ä»¬çš„domain adaptationæ¨¡å‹ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨BC2GMæ•°æ®é›†ï¼ˆä¸€ä¸ªæµè¡Œçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼‰ä¸Šæ¯”è¾ƒä¸€ä¸ªå¼€ç®±å³ç”¨ï¼ˆOOBï¼‰æ¨¡å‹ä¸ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç”¨äºNERé¢„å¤„ç†å’Œè¯„ä¼°çš„å®ç”¨å‡½æ•°æ”¹ç¼–è‡ªHuggingFaceçš„NERå¾®è°ƒç¤ºä¾‹ç¬”è®°ã€‚ å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå½¢æˆNERæ•°æ®é›† from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # ç‰¹æ®Šæ ‡è®°æœ‰ä¸€ä¸ªå•è¯IDï¼Œæ˜¯Noneã€‚æˆ‘ä»¬å°†æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œå› æ­¤å®ƒä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¢«è‡ªåŠ¨å¿½ç•¥äº†ã€‚ if word_idx is None: label_ids.append(-100) # æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è®¾ç½®æ ‡ç­¾ã€‚ elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # å¯¹äºä¸€ä¸ªè¯ä¸­çš„å…¶ä»–æ ‡è®°ï¼Œæˆ‘ä»¬æ ¹æ®label_all_tokensçš„æ ‡å¿—ï¼Œå°†æ ‡ç­¾è®¾ç½®ä¸ºå½“å‰æ ‡ç­¾æˆ–-100ã€‚ else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # ç§»é™¤è¢«å¿½ç•¥çš„ç´¢å¼•ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰ã€‚ true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } å®‰è£… seqeval pip install seqeval seqevalæ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ ‡è®°è¯„ä¼°çš„Pythonæ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°åˆ†å—ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€éƒ¨åˆ†è¯­éŸ³æ ‡è®°ã€è¯­ä¹‰è§’è‰²æ ‡è®°ç­‰ã€‚ label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} å®ä¾‹åŒ–NERæ¨¡å‹åœ¨æ­¤ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„NERæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ: da_model: æˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­åˆšåˆšè®­ç»ƒçš„ä¸€ä¸ªDomain Adaptationçš„NERæ¨¡å‹ da_full_corpus_model: åŒæ ·çš„é¢†åŸŸé€‚åº”æ€§NERæ¨¡å‹ï¼Œåªæ˜¯å®ƒæ˜¯åœ¨å®Œæ•´çš„é¢†åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚ oob_model: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„BERT-NERæ¨¡å‹ï¼ˆæ²¡æœ‰ç»è¿‡Domain Adaptationï¼‰ã€‚ from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#è®­ç»ƒå’Œè¯„ä¼°da_model_full_corpus"},{"categories":["documentation"],"content":"ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ‘ä»¬å¯ä»¥ä¸ºHuggingFaceæ”¯æŒçš„ä»»ä½•å¾®è°ƒä»»åŠ¡æ’å…¥æˆ‘ä»¬çš„domain adaptationæ¨¡å‹ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨BC2GMæ•°æ®é›†ï¼ˆä¸€ä¸ªæµè¡Œçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼‰ä¸Šæ¯”è¾ƒä¸€ä¸ªå¼€ç®±å³ç”¨ï¼ˆOOBï¼‰æ¨¡å‹ä¸ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç”¨äºNERé¢„å¤„ç†å’Œè¯„ä¼°çš„å®ç”¨å‡½æ•°æ”¹ç¼–è‡ªHuggingFaceçš„NERå¾®è°ƒç¤ºä¾‹ç¬”è®°ã€‚ å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå½¢æˆNERæ•°æ®é›† from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # ç‰¹æ®Šæ ‡è®°æœ‰ä¸€ä¸ªå•è¯IDï¼Œæ˜¯Noneã€‚æˆ‘ä»¬å°†æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œå› æ­¤å®ƒä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¢«è‡ªåŠ¨å¿½ç•¥äº†ã€‚ if word_idx is None: label_ids.append(-100) # æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è®¾ç½®æ ‡ç­¾ã€‚ elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # å¯¹äºä¸€ä¸ªè¯ä¸­çš„å…¶ä»–æ ‡è®°ï¼Œæˆ‘ä»¬æ ¹æ®label_all_tokensçš„æ ‡å¿—ï¼Œå°†æ ‡ç­¾è®¾ç½®ä¸ºå½“å‰æ ‡ç­¾æˆ–-100ã€‚ else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # ç§»é™¤è¢«å¿½ç•¥çš„ç´¢å¼•ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰ã€‚ true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } å®‰è£… seqeval pip install seqeval seqevalæ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ ‡è®°è¯„ä¼°çš„Pythonæ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°åˆ†å—ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€éƒ¨åˆ†è¯­éŸ³æ ‡è®°ã€è¯­ä¹‰è§’è‰²æ ‡è®°ç­‰ã€‚ label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} å®ä¾‹åŒ–NERæ¨¡å‹åœ¨æ­¤ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„NERæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ: da_model: æˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­åˆšåˆšè®­ç»ƒçš„ä¸€ä¸ªDomain Adaptationçš„NERæ¨¡å‹ da_full_corpus_model: åŒæ ·çš„é¢†åŸŸé€‚åº”æ€§NERæ¨¡å‹ï¼Œåªæ˜¯å®ƒæ˜¯åœ¨å®Œæ•´çš„é¢†åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚ oob_model: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„BERT-NERæ¨¡å‹ï¼ˆæ²¡æœ‰ç»è¿‡Domain Adaptationï¼‰ã€‚ from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#è®­ç»ƒå’Œè¯„ä¼°oob_model"},{"categories":["documentation"],"content":"ç»“æœæˆ‘ä»¬çœ‹åˆ°ï¼Œåœ¨è¿™ä¸‰ä¸ªæ¨¡å‹ä¸­ï¼Œda_full_corpus_modelï¼ˆåœ¨æ•´ä¸ªåŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè¿›è¡Œäº†åŸŸè°ƒæ•´ï¼‰åœ¨æµ‹è¯•F1å¾—åˆ†ä¸Šæ¯”oob_modelé«˜å‡º2%ä»¥ä¸Šã€‚äº‹å®ä¸Šï¼Œè¿™ä¸ªda_full_corpus_modelæ¨¡å‹æ˜¯æˆ‘ä»¬è®­ç»ƒçš„åœ¨BC2GMä¸Šä¼˜äºSOTAçš„è®¸å¤šé¢†åŸŸé€‚åº”æ¨¡å‹ä¹‹ä¸€ã€‚ æ­¤å¤–ï¼Œda_modelçš„è¡¨ç°ä¹Ÿä½äºoob_modelã€‚è¿™æ˜¯å¯ä»¥é¢„æœŸçš„ï¼Œå› ä¸ºda_modelåœ¨æœ¬æŒ‡å—ä¸­ç»å†äº†æœ€å°çš„é¢†åŸŸé¢„è®­ç»ƒã€‚ ","date":"2022-03-04","objectID":"/tools01/:1:4","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#ç»“æœ"},{"categories":["documentation"],"content":"æ€»ç»“åœ¨æœ¬æŒ‡å—ä¸­ï¼Œä½ å·²ç»çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ â€œDataSelector â€œå’Œ â€œVocabAugmentorâ€ï¼Œé€šè¿‡åˆ†åˆ«æ‰§è¡Œæ•°æ®é€‰æ‹©å’Œè¯æ±‡æ‰©å±•ï¼Œå¯¹å˜å‹å™¨æ¨¡å‹è¿›è¡Œé¢†åŸŸè°ƒæ•´ã€‚ ä½ è¿˜çœ‹åˆ°å®ƒä»¬ä¸HuggingFaceçš„æ‰€æœ‰äº§å“å…¼å®¹ã€‚å˜æ¢å™¨â€ã€â€œæ ‡è®°å™¨ â€œå’Œ â€œæ•°æ®é›†â€ã€‚ æœ€åè¡¨æ˜ï¼Œåœ¨å®Œæ•´çš„é¢†åŸŸå†…è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸé€‚åº”çš„æ¨¡å‹æ¯”å¼€ç®±å³ç”¨çš„æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚ ","date":"2022-03-04","objectID":"/tools01/:2:0","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#æ€»ç»“"},{"categories":["documentation"],"content":"å‚è€ƒTransformers-Domain-Adaptation Guide to Transformers Domain Adaptation ","date":"2022-03-04","objectID":"/tools01/:3:0","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#å‚è€ƒ"},{"categories":["documentation"],"content":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","date":"2022-03-04","objectID":"/paper02/","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/"},{"categories":["documentation"],"content":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking è®ºæ–‡è§£è¯»ã€‚ ","date":"2022-03-04","objectID":"/paper02/:0:0","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#"},{"categories":["documentation"],"content":"é¢˜ç›®Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking [ACL 2021 Short] [Code] ","date":"2022-03-04","objectID":"/paper02/:0:1","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#é¢˜ç›®"},{"categories":["documentation"],"content":"æ‘˜è¦å°†å¤–éƒ¨ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼ˆå¦‚UMLSï¼‰æ³¨å…¥é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä¸­ï¼Œå¯ä»¥æé«˜å…¶å¤„ç†ç‰¹æ®Šé¢†åŸŸå†…ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¦‚ç”Ÿç‰©åŒ»å­¦å®ä½“é“¾æ¥ä»»åŠ¡ï¼ˆBELï¼‰ï¼Œç„¶è€Œï¼Œè¿™ç§ä¸°å¯Œçš„ä¸“å®¶çŸ¥è¯†åªé€‚ç”¨äºå°‘æ•°è¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä½œè€…é€šè¿‡æå‡ºä¸€ä¸ªæ–°çš„è·¨è¯­è¨€ç”Ÿç‰©åŒ»å­¦å®ä½“è¿æ¥ä»»åŠ¡(XL-BEL)ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªæ–°çš„XL-BELåŸºå‡†ï¼Œè·¨è¶Š10ç§ä¸åŒçš„è¯­è¨€ï¼Œä½œè€…é¦–å…ˆç ”ç©¶äº†æ ‡å‡†çš„çŸ¥è¯†è¯Šæ–­ä»¥åŠçŸ¥è¯†å¢å¼ºçš„å•è¯­è¨€å’Œå¤šè¯­è¨€LMsåœ¨æ ‡å‡†çš„å•è¯­è¨€è‹±è¯­BELä»»åŠ¡ä¹‹å¤–çš„èƒ½åŠ›ã€‚è¿™äº›è¯„åˆ†æ˜¾ç¤ºäº†ä¸è‹±è¯­è¡¨ç°çš„å·¨å¤§å·®è·ã€‚ç„¶åï¼Œä½œè€…è§£å†³äº†å°†ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ä»èµ„æºä¸°å¯Œçš„è¯­è¨€è½¬ç§»åˆ°èµ„æºè´«ä¹çš„è¯­è¨€çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…ä¸ºXL-BELä»»åŠ¡æå‡ºå¹¶è¯„ä¼°äº†ä¸€ç³»åˆ—è·¨è¯­è¨€çš„è½¬ç§»æ–¹æ³•ï¼Œå¹¶è¯æ˜ä¸€èˆ¬é¢†åŸŸçš„æŠ“å­—çœ¼æœ‰åŠ©äºå°†ç°æœ‰çš„è‹±è¯­çŸ¥è¯†ä¼ æ’­ç»™å‡ ä¹æ²¡æœ‰in-domainæ•°æ®çš„è¯­è¨€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½œè€…æå‡ºçš„ç‰¹å®šé¢†åŸŸçš„è½¬ç§»æ–¹æ³•åœ¨æ‰€æœ‰ç›®æ ‡è¯­è¨€ä¸­äº§ç”Ÿäº†ä¸€è‡´çš„æ”¶ç›Šï¼Œæœ‰æ—¶ç”šè‡³è¾¾åˆ°äº†20%@1ç‚¹ï¼Œè€Œç›®æ ‡è¯­è¨€ä¸­æ²¡æœ‰ä»»ä½•é¢†åŸŸå†…çš„çŸ¥è¯†ï¼Œä¹Ÿæ²¡æœ‰ä»»ä½•é¢†åŸŸå†…çš„å¹³è¡Œæ•°æ®ã€‚ ","date":"2022-03-04","objectID":"/paper02/:0:2","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#æ‘˜è¦"},{"categories":["documentation"],"content":"è´¡çŒ® å¼ºè°ƒäº†å­¦ä¹ ï¼ˆç”Ÿç‰©åŒ»å­¦ï¼‰ä¸“ä¸šé¢†åŸŸçš„è·¨è¯­è¨€è¡¨å¾çš„æŒ‘æˆ˜ã€‚ æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å¤šè¯­ç§XL-BELä»»åŠ¡ï¼Œå¹¶æœ‰10ç§è¯­è¨€çš„ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚ å¯¹XL-BELä»»åŠ¡ä¸­ç°æœ‰çš„çŸ¥è¯†è¯Šæ–­å’ŒçŸ¥è¯†å¢å¼ºçš„å•è¯­å’Œå¤šè¯­LMsè¿›è¡Œäº†ç³»ç»Ÿæ€§çš„è¯„ä¼°ã€‚ åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸæå‡ºäº†ä¸€ä¸ªæ–°çš„SotAå¤šè¯­è¨€ç¼–ç å™¨ï¼Œå®ƒåœ¨XL-BELä¸­äº§ç”Ÿäº†å·¨å¤§çš„æ”¶ç›Šï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºè´«ä¹çš„è¯­è¨€ä¸Šï¼Œå¹¶æä¾›äº†å¼ºå¤§çš„åŸºå‡†æµ‹è¯•ç»“æœæ¥æŒ‡å¯¼æœªæ¥çš„å·¥ä½œã€‚ ","date":"2022-03-04","objectID":"/paper02/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#è´¡çŒ®"},{"categories":["documentation"],"content":"æ¨¡å‹","date":"2022-03-04","objectID":"/paper02/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#æ¨¡å‹"},{"categories":["documentation"],"content":"Language-Agnostic SAPè®©$(x,y)\\in \\mathcal{X}\\times \\mathcal{Y}$è¡¨ç¤ºä¸€ä¸ªåå­—å’Œå…¶åˆ†ç±»æ ‡ç­¾çš„å…ƒç»„ã€‚å½“ä»UMLSåŒä¹‰è¯å­¦ä¹ æ—¶ï¼Œ$\\mathcal{X}\\times \\mathcal{Y}$æ˜¯æ‰€æœ‰(nameï¼ŒCUI)å¯¹çš„é›†åˆï¼Œä¾‹(vaccination, C0042196)ã€‚è™½ç„¶Liuç­‰äººåªä½¿ç”¨è‹±æ–‡åç§°ï¼Œä½†ä½œè€…åœ¨æ­¤è€ƒè™‘å…¶ä»–UMLSè¯­è¨€çš„åç§°ã€‚ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¯¥æ¨¡å‹è¢«å¼•å¯¼ä¸ºåŒä¹‰è¯åˆ›å»ºç±»ä¼¼çš„è¡¨ç¤ºï¼Œè€Œä¸è®ºå…¶è¯­è¨€å¦‚ä½•ã€‚ è¯¥å­¦ä¹ æ–¹æ¡ˆåŒ…æ‹¬ï¼š1ï¼‰ä¸€ä¸ªåœ¨çº¿æŠ½æ ·ç¨‹åºæ¥é€‰æ‹©è®­ç»ƒå®ä¾‹ï¼›2ï¼‰ä¸€ä¸ªåº¦é‡å­¦ä¹ æŸå¤±ï¼Œé¼“åŠ±å…±äº«ç›¸åŒCUIçš„å­—ç¬¦ä¸²è·å¾—ç±»ä¼¼çš„è¡¨ç¤ºã€‚ Training Examplesç»™å®šä¸€ä¸ªç”±$N$ä¸ªä¾‹å­ç»„æˆçš„å°æ‰¹æ¬¡$\\mathcal{B} ={\\mathcal{X_B}}\\times{\\mathcal{Y_B}}={{(x_i, y_i)}_i^N}=1 $ï¼Œæˆ‘ä»¬ä»ä¸ºæ‰€æœ‰åå­—$x_i\\in\\mathcal{X_B}$æ„å»ºæ‰€æœ‰å¯èƒ½çš„ä¸‰è”ä½“å¼€å§‹ã€‚æ¯ä¸ªä¸‰è”ä½“çš„å½¢å¼æ˜¯$(x_a, x_p, x_n)$ï¼Œå…¶ä¸­$x_a$æ˜¯é”šæ ‡ç­¾ï¼Œæ˜¯$\\mathcal{X_B}$ä¸­çš„ä¸€ä¸ªä»»æ„åå­—ï¼›$x_p$æ˜¯$x_a$çš„æ­£åŒ¹é…ï¼ˆå³$y_a= y_p$ï¼‰ï¼Œ$x_n$æ˜¯$x_a$çš„è´ŸåŒ¹é…ï¼ˆå³$y_a\\not =y_n$ï¼‰ã€‚ç„¶åæˆ‘ä»¬è®©$f(\\cdot)$è¡¨ç¤ºç¼–ç å™¨ï¼ˆå³æœ¬æ–‡ä¸­çš„MBERTæˆ–XLMRï¼‰ï¼Œåœ¨æ„å»ºçš„ä¸‰è”ä½“ä¸­ï¼Œé€‰æ‹©æ‰€æœ‰æ»¡è¶³ä»¥ä¸‹çº¦æŸæ¡ä»¶çš„ä¸‰è”ä½“: $$ \\Vert f(x_a) -f(x_p) \\Vert_2 + \\lambda \\geq \\Vert f(x_a) -f(x_n) \\Vert_2 $$ å…¶ä¸­$\\lambda$æ˜¯ä¸€ä¸ªé¢„å®šä¹‰çš„ä½™é‡ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬åªè€ƒè™‘æ­£æ ·æœ¬æ¯”è´Ÿæ ·æœ¬å¤šå‡º$\\lambda$çš„ä¸‰è”ä½“ã€‚è¿™äº›\"ç¡¬\"ä¸‰è”ä½“å¯¹è¡¨ç¤ºå­¦ä¹ æ¥è¯´ä¿¡æ¯é‡æ›´å¤§ã€‚ç„¶åï¼Œæ¯ä¸ªè¢«é€‰ä¸­çš„ä¸‰è”ä½“éƒ½ä¼šè´¡çŒ®ä¸€ä¸ªæ­£æ•°å¯¹$(x_a,x_p)$å’Œä¸€ä¸ªè´Ÿæ•°å¯¹$(x_a,x_n)$ï¼Œæˆ‘ä»¬æ”¶é›†æ‰€æœ‰è¿™æ ·çš„æ­£æ•°å’Œè´Ÿæ•°ï¼Œå¹¶å°†å®ƒä»¬è¡¨ç¤ºä¸º$\\mathcal{P},\\mathcal{N}$ã€‚ Multi-Similarity Lossæˆ‘ä»¬è®¡ç®—æ‰€æœ‰åå­—ä»£è¡¨çš„æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¾—åˆ°ä¸€ä¸ªç›¸ä¼¼åº¦çŸ©é˜µ$\\mathcal{S}\\in\\mathscr{R}^{\\vert{\\mathcal{X_B}}\\vert\\times\\vert{\\mathcal{X_B}}\\vert}$ï¼Œå…¶ä¸­æ¯ä¸ªæ¡ç›®$\\mathcal{S_{ij}}$æ˜¯å°æ‰¹$\\mathcal{B}$ä¸­ç¬¬$i$ä¸ªå’Œç¬¬$j$ä¸ªåå­—ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç„¶åä½¿ç”¨å¤šé‡ç›¸ä¼¼åº¦æŸå¤±ä»ä¸‰è”ä½“å­¦ä¹ : \\begin{align} \\mathcal{L} = \\frac{1}{\\vert{\\mathcal{X_B}}\\vert}{\\displaystyle\\sum_{i=1}^{\\vert{\\mathcal{X_B}}\\vert}}(\\frac{1}{\\alpha}\\log{(1+\\sum_{n\\in\\mathcal{N}_i} e^{\\alpha(\\mathcal{S}_{in}-\\epsilon)})}+\\frac{1}{\\beta}\\log{(1+\\sum_{n\\in\\mathcal{P}_i} e^{\\alpha(\\mathcal{S}_{ip}-\\epsilon)})}) \\end{align} $\\alphaï¼Œ\\beta$æ˜¯æ¸©åº¦æ ‡åº¦ï¼›$\\epsilon$æ˜¯åº”ç”¨äºç›¸ä¼¼æ€§çŸ©é˜µçš„åç§»é‡ï¼›$\\mathcal{P}_i,\\mathcal{N}_i$æ˜¯ç¬¬$i$ä¸ªé”šçš„æ­£è´Ÿæ ·æœ¬çš„æŒ‡æ•°ã€‚ ","date":"2022-03-04","objectID":"/paper02/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#language-agnostic-sap"},{"categories":["documentation"],"content":"Language-Agnostic SAPè®©$(x,y)\\in \\mathcal{X}\\times \\mathcal{Y}$è¡¨ç¤ºä¸€ä¸ªåå­—å’Œå…¶åˆ†ç±»æ ‡ç­¾çš„å…ƒç»„ã€‚å½“ä»UMLSåŒä¹‰è¯å­¦ä¹ æ—¶ï¼Œ$\\mathcal{X}\\times \\mathcal{Y}$æ˜¯æ‰€æœ‰(nameï¼ŒCUI)å¯¹çš„é›†åˆï¼Œä¾‹(vaccination, C0042196)ã€‚è™½ç„¶Liuç­‰äººåªä½¿ç”¨è‹±æ–‡åç§°ï¼Œä½†ä½œè€…åœ¨æ­¤è€ƒè™‘å…¶ä»–UMLSè¯­è¨€çš„åç§°ã€‚ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¯¥æ¨¡å‹è¢«å¼•å¯¼ä¸ºåŒä¹‰è¯åˆ›å»ºç±»ä¼¼çš„è¡¨ç¤ºï¼Œè€Œä¸è®ºå…¶è¯­è¨€å¦‚ä½•ã€‚ è¯¥å­¦ä¹ æ–¹æ¡ˆåŒ…æ‹¬ï¼š1ï¼‰ä¸€ä¸ªåœ¨çº¿æŠ½æ ·ç¨‹åºæ¥é€‰æ‹©è®­ç»ƒå®ä¾‹ï¼›2ï¼‰ä¸€ä¸ªåº¦é‡å­¦ä¹ æŸå¤±ï¼Œé¼“åŠ±å…±äº«ç›¸åŒCUIçš„å­—ç¬¦ä¸²è·å¾—ç±»ä¼¼çš„è¡¨ç¤ºã€‚ Training Examplesç»™å®šä¸€ä¸ªç”±$N$ä¸ªä¾‹å­ç»„æˆçš„å°æ‰¹æ¬¡$\\mathcal{B} ={\\mathcal{X_B}}\\times{\\mathcal{Y_B}}={{(x_i, y_i)}_i^N}=1 $ï¼Œæˆ‘ä»¬ä»ä¸ºæ‰€æœ‰åå­—$x_i\\in\\mathcal{X_B}$æ„å»ºæ‰€æœ‰å¯èƒ½çš„ä¸‰è”ä½“å¼€å§‹ã€‚æ¯ä¸ªä¸‰è”ä½“çš„å½¢å¼æ˜¯$(x_a, x_p, x_n)$ï¼Œå…¶ä¸­$x_a$æ˜¯é”šæ ‡ç­¾ï¼Œæ˜¯$\\mathcal{X_B}$ä¸­çš„ä¸€ä¸ªä»»æ„åå­—ï¼›$x_p$æ˜¯$x_a$çš„æ­£åŒ¹é…ï¼ˆå³$y_a= y_p$ï¼‰ï¼Œ$x_n$æ˜¯$x_a$çš„è´ŸåŒ¹é…ï¼ˆå³$y_a\\not =y_n$ï¼‰ã€‚ç„¶åæˆ‘ä»¬è®©$f(\\cdot)$è¡¨ç¤ºç¼–ç å™¨ï¼ˆå³æœ¬æ–‡ä¸­çš„MBERTæˆ–XLMRï¼‰ï¼Œåœ¨æ„å»ºçš„ä¸‰è”ä½“ä¸­ï¼Œé€‰æ‹©æ‰€æœ‰æ»¡è¶³ä»¥ä¸‹çº¦æŸæ¡ä»¶çš„ä¸‰è”ä½“: $$ \\Vert f(x_a) -f(x_p) \\Vert_2 + \\lambda \\geq \\Vert f(x_a) -f(x_n) \\Vert_2 $$ å…¶ä¸­$\\lambda$æ˜¯ä¸€ä¸ªé¢„å®šä¹‰çš„ä½™é‡ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬åªè€ƒè™‘æ­£æ ·æœ¬æ¯”è´Ÿæ ·æœ¬å¤šå‡º$\\lambda$çš„ä¸‰è”ä½“ã€‚è¿™äº›\"ç¡¬\"ä¸‰è”ä½“å¯¹è¡¨ç¤ºå­¦ä¹ æ¥è¯´ä¿¡æ¯é‡æ›´å¤§ã€‚ç„¶åï¼Œæ¯ä¸ªè¢«é€‰ä¸­çš„ä¸‰è”ä½“éƒ½ä¼šè´¡çŒ®ä¸€ä¸ªæ­£æ•°å¯¹$(x_a,x_p)$å’Œä¸€ä¸ªè´Ÿæ•°å¯¹$(x_a,x_n)$ï¼Œæˆ‘ä»¬æ”¶é›†æ‰€æœ‰è¿™æ ·çš„æ­£æ•°å’Œè´Ÿæ•°ï¼Œå¹¶å°†å®ƒä»¬è¡¨ç¤ºä¸º$\\mathcal{P},\\mathcal{N}$ã€‚ Multi-Similarity Lossæˆ‘ä»¬è®¡ç®—æ‰€æœ‰åå­—ä»£è¡¨çš„æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¾—åˆ°ä¸€ä¸ªç›¸ä¼¼åº¦çŸ©é˜µ$\\mathcal{S}\\in\\mathscr{R}^{\\vert{\\mathcal{X_B}}\\vert\\times\\vert{\\mathcal{X_B}}\\vert}$ï¼Œå…¶ä¸­æ¯ä¸ªæ¡ç›®$\\mathcal{S_{ij}}$æ˜¯å°æ‰¹$\\mathcal{B}$ä¸­ç¬¬$i$ä¸ªå’Œç¬¬$j$ä¸ªåå­—ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç„¶åä½¿ç”¨å¤šé‡ç›¸ä¼¼åº¦æŸå¤±ä»ä¸‰è”ä½“å­¦ä¹ : \\begin{align} \\mathcal{L} = \\frac{1}{\\vert{\\mathcal{X_B}}\\vert}{\\displaystyle\\sum_{i=1}^{\\vert{\\mathcal{X_B}}\\vert}}(\\frac{1}{\\alpha}\\log{(1+\\sum_{n\\in\\mathcal{N}_i} e^{\\alpha(\\mathcal{S}_{in}-\\epsilon)})}+\\frac{1}{\\beta}\\log{(1+\\sum_{n\\in\\mathcal{P}_i} e^{\\alpha(\\mathcal{S}_{ip}-\\epsilon)})}) \\end{align} $\\alphaï¼Œ\\beta$æ˜¯æ¸©åº¦æ ‡åº¦ï¼›$\\epsilon$æ˜¯åº”ç”¨äºç›¸ä¼¼æ€§çŸ©é˜µçš„åç§»é‡ï¼›$\\mathcal{P}_i,\\mathcal{N}_i$æ˜¯ç¬¬$i$ä¸ªé”šçš„æ­£è´Ÿæ ·æœ¬çš„æŒ‡æ•°ã€‚ ","date":"2022-03-04","objectID":"/paper02/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#training-examples"},{"categories":["documentation"],"content":"Language-Agnostic SAPè®©$(x,y)\\in \\mathcal{X}\\times \\mathcal{Y}$è¡¨ç¤ºä¸€ä¸ªåå­—å’Œå…¶åˆ†ç±»æ ‡ç­¾çš„å…ƒç»„ã€‚å½“ä»UMLSåŒä¹‰è¯å­¦ä¹ æ—¶ï¼Œ$\\mathcal{X}\\times \\mathcal{Y}$æ˜¯æ‰€æœ‰(nameï¼ŒCUI)å¯¹çš„é›†åˆï¼Œä¾‹(vaccination, C0042196)ã€‚è™½ç„¶Liuç­‰äººåªä½¿ç”¨è‹±æ–‡åç§°ï¼Œä½†ä½œè€…åœ¨æ­¤è€ƒè™‘å…¶ä»–UMLSè¯­è¨€çš„åç§°ã€‚ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¯¥æ¨¡å‹è¢«å¼•å¯¼ä¸ºåŒä¹‰è¯åˆ›å»ºç±»ä¼¼çš„è¡¨ç¤ºï¼Œè€Œä¸è®ºå…¶è¯­è¨€å¦‚ä½•ã€‚ è¯¥å­¦ä¹ æ–¹æ¡ˆåŒ…æ‹¬ï¼š1ï¼‰ä¸€ä¸ªåœ¨çº¿æŠ½æ ·ç¨‹åºæ¥é€‰æ‹©è®­ç»ƒå®ä¾‹ï¼›2ï¼‰ä¸€ä¸ªåº¦é‡å­¦ä¹ æŸå¤±ï¼Œé¼“åŠ±å…±äº«ç›¸åŒCUIçš„å­—ç¬¦ä¸²è·å¾—ç±»ä¼¼çš„è¡¨ç¤ºã€‚ Training Examplesç»™å®šä¸€ä¸ªç”±$N$ä¸ªä¾‹å­ç»„æˆçš„å°æ‰¹æ¬¡$\\mathcal{B} ={\\mathcal{X_B}}\\times{\\mathcal{Y_B}}={{(x_i, y_i)}_i^N}=1 $ï¼Œæˆ‘ä»¬ä»ä¸ºæ‰€æœ‰åå­—$x_i\\in\\mathcal{X_B}$æ„å»ºæ‰€æœ‰å¯èƒ½çš„ä¸‰è”ä½“å¼€å§‹ã€‚æ¯ä¸ªä¸‰è”ä½“çš„å½¢å¼æ˜¯$(x_a, x_p, x_n)$ï¼Œå…¶ä¸­$x_a$æ˜¯é”šæ ‡ç­¾ï¼Œæ˜¯$\\mathcal{X_B}$ä¸­çš„ä¸€ä¸ªä»»æ„åå­—ï¼›$x_p$æ˜¯$x_a$çš„æ­£åŒ¹é…ï¼ˆå³$y_a= y_p$ï¼‰ï¼Œ$x_n$æ˜¯$x_a$çš„è´ŸåŒ¹é…ï¼ˆå³$y_a\\not =y_n$ï¼‰ã€‚ç„¶åæˆ‘ä»¬è®©$f(\\cdot)$è¡¨ç¤ºç¼–ç å™¨ï¼ˆå³æœ¬æ–‡ä¸­çš„MBERTæˆ–XLMRï¼‰ï¼Œåœ¨æ„å»ºçš„ä¸‰è”ä½“ä¸­ï¼Œé€‰æ‹©æ‰€æœ‰æ»¡è¶³ä»¥ä¸‹çº¦æŸæ¡ä»¶çš„ä¸‰è”ä½“: $$ \\Vert f(x_a) -f(x_p) \\Vert_2 + \\lambda \\geq \\Vert f(x_a) -f(x_n) \\Vert_2 $$ å…¶ä¸­$\\lambda$æ˜¯ä¸€ä¸ªé¢„å®šä¹‰çš„ä½™é‡ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬åªè€ƒè™‘æ­£æ ·æœ¬æ¯”è´Ÿæ ·æœ¬å¤šå‡º$\\lambda$çš„ä¸‰è”ä½“ã€‚è¿™äº›\"ç¡¬\"ä¸‰è”ä½“å¯¹è¡¨ç¤ºå­¦ä¹ æ¥è¯´ä¿¡æ¯é‡æ›´å¤§ã€‚ç„¶åï¼Œæ¯ä¸ªè¢«é€‰ä¸­çš„ä¸‰è”ä½“éƒ½ä¼šè´¡çŒ®ä¸€ä¸ªæ­£æ•°å¯¹$(x_a,x_p)$å’Œä¸€ä¸ªè´Ÿæ•°å¯¹$(x_a,x_n)$ï¼Œæˆ‘ä»¬æ”¶é›†æ‰€æœ‰è¿™æ ·çš„æ­£æ•°å’Œè´Ÿæ•°ï¼Œå¹¶å°†å®ƒä»¬è¡¨ç¤ºä¸º$\\mathcal{P},\\mathcal{N}$ã€‚ Multi-Similarity Lossæˆ‘ä»¬è®¡ç®—æ‰€æœ‰åå­—ä»£è¡¨çš„æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¾—åˆ°ä¸€ä¸ªç›¸ä¼¼åº¦çŸ©é˜µ$\\mathcal{S}\\in\\mathscr{R}^{\\vert{\\mathcal{X_B}}\\vert\\times\\vert{\\mathcal{X_B}}\\vert}$ï¼Œå…¶ä¸­æ¯ä¸ªæ¡ç›®$\\mathcal{S_{ij}}$æ˜¯å°æ‰¹$\\mathcal{B}$ä¸­ç¬¬$i$ä¸ªå’Œç¬¬$j$ä¸ªåå­—ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç„¶åä½¿ç”¨å¤šé‡ç›¸ä¼¼åº¦æŸå¤±ä»ä¸‰è”ä½“å­¦ä¹ : \\begin{align} \\mathcal{L} = \\frac{1}{\\vert{\\mathcal{X_B}}\\vert}{\\displaystyle\\sum_{i=1}^{\\vert{\\mathcal{X_B}}\\vert}}(\\frac{1}{\\alpha}\\log{(1+\\sum_{n\\in\\mathcal{N}_i} e^{\\alpha(\\mathcal{S}_{in}-\\epsilon)})}+\\frac{1}{\\beta}\\log{(1+\\sum_{n\\in\\mathcal{P}_i} e^{\\alpha(\\mathcal{S}_{ip}-\\epsilon)})}) \\end{align} $\\alphaï¼Œ\\beta$æ˜¯æ¸©åº¦æ ‡åº¦ï¼›$\\epsilon$æ˜¯åº”ç”¨äºç›¸ä¼¼æ€§çŸ©é˜µçš„åç§»é‡ï¼›$\\mathcal{P}_i,\\mathcal{N}_i$æ˜¯ç¬¬$i$ä¸ªé”šçš„æ­£è´Ÿæ ·æœ¬çš„æŒ‡æ•°ã€‚ ","date":"2022-03-04","objectID":"/paper02/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#multi-similarity-loss"},{"categories":["documentation"],"content":"å®éªŒç»“æœä¸è®¨è®º Fig-1.Multilingual UMLS Knowledge Always Helps Fig-1æ€»ç»“äº†åœ¨å„ç§å•è¯­ã€å¤šè¯­å’Œé¢†åŸŸå†…é¢„è®­ç»ƒç¼–ç å™¨ä¸Šåº”ç”¨åŸºäºUMLSçŸ¥è¯†çš„å¤šè¯­è¨€SAPå¾®è°ƒçš„ç»“æœï¼›æ³¨å…¥UMLSçŸ¥è¯†å¯¹æ¨¡å‹åœ¨XL-BELä¸Šçš„è¡¨ç°åœ¨æ‰€æœ‰è¯­è¨€å’Œæ‰€æœ‰åŸºç¡€ç¼–ç å™¨ä¸Šéƒ½æ˜¯æœ‰ç›Šçš„ã€‚ä½¿ç”¨å¤šè¯­è¨€UMLSåŒä¹‰è¯å¯¹ç”Ÿç‰©åŒ»å­¦$\\texttt{PUBMEDBERT}$ï¼ˆ$SapBERT_{all_syn}$ï¼‰è¿›è¡ŒSAP-fine-tuneï¼Œè€Œä¸æ˜¯åªä½¿ç”¨è‹±è¯­åŒä¹‰è¯ï¼ˆ$SapBERT$ï¼‰ï¼Œèƒ½å…¨é¢æé«˜å…¶æ€§èƒ½ã€‚å¯¹æ¯ç§è¯­è¨€çš„å•è¯­BERTè¿›è¡ŒSAP-ingè°ƒæ•´ï¼Œä¹Ÿåœ¨æ‰€æœ‰è¯­è¨€ä¸­äº§ç”Ÿäº†å·¨å¤§çš„æ”¶ç›Šï¼›å”¯ä¸€çš„ä¾‹å¤–æ˜¯æ³°è¯­ï¼ˆTHï¼‰ï¼Œå®ƒåœ¨UMLSä¸­æ²¡æœ‰ä½“ç°ã€‚å¯¹å¤šè¯­è¨€æ¨¡å‹MBERTå’ŒXLMRè¿›è¡Œå¾®è°ƒï¼Œä¼šå¸¦æ¥æ›´å¤§çš„ç›¸å¯¹æ”¶ç›Šã€‚ UMLSæ•°æ®åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šåå‘äºç½—æ›¼è¯­å’Œæ—¥è€³æ›¼è¯­ã€‚å› æ­¤ï¼Œå¯¹äºä¸è¿™äº›è¯­ç³»æ¯”è¾ƒç›¸ä¼¼çš„è¯­è¨€ï¼Œå•è¯­LMï¼ˆä¸ŠåŠéƒ¨åˆ†ï¼ŒFig-1ï¼‰ä¸å¤šè¯­LMï¼ˆä¸‹åŠéƒ¨åˆ†ï¼ŒFig-1ï¼‰ç›¸æ¯”è¡¨ç°ç›¸å½“æˆ–ä¼˜äºå¤šè¯­LMã€‚ç„¶è€Œï¼Œå¯¹äºå…¶ä»–ï¼ˆé¥è¿œçš„ï¼‰è¯­è¨€ï¼ˆå¦‚KOã€ZHã€JAã€THï¼‰ï¼Œæƒ…å†µåˆ™ç›¸åã€‚ä¾‹å¦‚ï¼Œåœ¨THä¸Šï¼ŒXLMR+SAPall_synæ¯”THBERT+SAPall_syné«˜å‡º20%@1çš„ç²¾ç¡®åº¦ã€‚ Fig-2.General Translation Knowledge is Useful Fig-2æ€»ç»“äº†æˆ‘ä»¬åœ¨ä¸€èˆ¬ç¿»è¯‘æ•°æ®ä¸Šç»§ç»­è®­ç»ƒçš„ç»“æœã€‚ åœ¨ä¹‹å‰åŸºäºUMLSçš„SAPä¹‹å é€šè¿‡è¿™ä¸ªå˜ä½“ï¼ŒåŸºç¡€å¤šè¯­è¨€LMæˆä¸ºå¼ºå¤§çš„å¤šè¯­è¨€ç”Ÿç‰©åŒ»å­¦ä¸“å®¶ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°åŸŸå¤–ç¿»è¯‘æ•°æ®çš„é¢å¤–å¼ºå¤§æå‡ï¼šä¾‹å¦‚ï¼Œå¯¹äºMBERTï¼Œé™¤ESå¤–ï¼Œæ‰€æœ‰è¯­è¨€çš„æå‡èŒƒå›´ä¸º2.4%è‡³12.7%ã€‚å¯¹äºXLMRï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†$XLMR+SAPen_{syn}$åœ¨RUã€TRã€KOã€THä¸Šçš„ç²¾ç¡®åº¦@1æå‡ï¼Œä»¥åŠ$XLMR+SAPall_{syn}$çš„ç±»ä¼¼ä½†è¾ƒå°çš„æå‡ã€‚ ","date":"2022-03-04","objectID":"/paper02/:0:6","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#å®éªŒç»“æœä¸è®¨è®º"},{"categories":["documentation"],"content":"ç»“è®ºæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è·¨è¯­è¨€ç”Ÿç‰©åŒ»å­¦å®ä½“ä»»åŠ¡ï¼ˆXL-BELï¼‰ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„è·¨è¯­è¨€å®ä½“è¡¨ç¤ºå»ºç«‹äº†ä¸€ä¸ªè¦†ç›–é¢å¹¿ä¸”å¯é çš„è¯„ä¼°åŸºå‡†ï¼Œå¹¶åœ¨XL-BELä¸Šè¯„ä¼°äº†å½“å‰çš„SotAç”Ÿç‰©åŒ»å­¦å®ä½“è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„è¿ç§»å­¦ä¹ æ–¹æ¡ˆï¼Œåˆ©ç”¨ä¸€èˆ¬é¢†åŸŸçš„ç¿»è¯‘æ¥æé«˜é¢†åŸŸä¸“ä¸šè¡¨ç¤ºæ¨¡å‹çš„è·¨è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿæ¿€åŠ±æœªæ¥æ›´å¤šå…³äºå¤šè¯­è¨€å’Œé¢†åŸŸä¸“ä¸šè¡¨ç¤ºå­¦ä¹ çš„ç ”ç©¶ã€‚ ","date":"2022-03-04","objectID":"/paper02/:0:7","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#ç»“è®º"},{"categories":["documentation"],"content":"ä»£ç https://github.com/cambridgeltl/sapbert ","date":"2022-03-04","objectID":"/paper02/:0:8","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#ä»£ç "},{"categories":["documentation"],"content":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","date":"2022-03-03","objectID":"/paper01/","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/"},{"categories":["documentation"],"content":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition è®ºæ–‡è§£è¯»ã€‚ ","date":"2022-03-03","objectID":"/paper01/:0:0","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#"},{"categories":["documentation"],"content":"é¢˜ç›®Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition [ACL 2021 Long] [Code] ","date":"2022-03-03","objectID":"/paper01/:0:1","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#é¢˜ç›®"},{"categories":["documentation"],"content":"æ‘˜è¦ä¼—åŒ…è¢«è®¤ä¸ºæ˜¯æœ‰æ•ˆç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªå‰ç»æ€§è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡ç¾¤ä½“åŠ³åŠ¨å»ºç«‹å¤§è§„æ¨¡çš„æ³¨é‡Šè®­ç»ƒæ•°æ®ã€‚ä»¥å‰çš„ç ”ç©¶é›†ä¸­åœ¨å‡å°‘ä¼—åŒ…æ³¨è§£çš„å™ªéŸ³å¯¹ç›‘ç£æ¨¡å¼çš„å½±å“ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡å–äº†ä¸åŒçš„è§‚ç‚¹ï¼Œå°†æ‰€æœ‰ä¼—åŒ…æ³¨é‡Šé‡æ–°è§†ä¸ºä¸ä¸ªåˆ«æ•°æ®æ ‡æ³¨å¸ˆæœ‰å…³çš„é»„é‡‘æ ‡å‡†ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å‘ç°ä¼—åŒ…å¯ä»¥ä¸é¢†åŸŸé€‚åº”æ€§ï¼ˆdomain adaptationï¼‰é«˜åº¦ç›¸ä¼¼ï¼Œé‚£ä¹ˆæœ€è¿‘çš„è·¨é¢†åŸŸæ–¹æ³•çš„è¿›å±•å‡ ä¹å¯ä»¥ç›´æ¥åº”ç”¨äºä¼—åŒ…ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸ºç ”ç©¶æ¡ˆä¾‹ï¼Œæå‡ºäº†ä¸€ä¸ªAnnotator-awareçš„è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å—åˆ°é¢†åŸŸé€‚åº”æ–¹æ³•çš„å¯å‘ï¼Œè¯•å›¾æ•æ‰æœ‰æ•ˆçš„Domain-awareçš„ç‰¹å¾ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ— ç›‘ç£å’Œæœ‰ç›‘ç£çš„ä¼—åŒ…å­¦ä¹ ï¼Œå‡è®¾æ²¡æœ‰æˆ–åªæœ‰å°è§„æ¨¡çš„ä¸“å®¶æ³¨é‡Šå¯ç”¨ï¼Œåœ¨ä¸€ä¸ªåŸºå‡†çš„ä¼—åŒ…NERæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯éå¸¸æœ‰æ•ˆçš„ï¼Œè¡¨ç°äº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨æœ‰ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªéœ€è¦å¾ˆå°è§„æ¨¡çš„ä¸“å®¶æ³¨é‡Šå°±å¯ä»¥è·å¾—ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½æå‡ã€‚ ","date":"2022-03-03","objectID":"/paper01/:0:2","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#æ‘˜è¦"},{"categories":["documentation"],"content":"è´¡çŒ® å¯¹ä¼—åŒ…å­¦ä¹ æå‡ºäº†ä¸åŒçš„çœ‹æ³•ï¼Œå¹¶å»ºè®®å°†ä¼—åŒ…å­¦ä¹ è½¬åŒ–ä¸ºé¢†åŸŸé€‚åº”é—®é¢˜ï¼ˆdomain adaptationï¼‰ï¼Œè¿™è‡ªç„¶è€Œç„¶åœ°å°†NLPçš„ä¸¤ä¸ªé‡è¦ä¸»é¢˜è”ç³»èµ·æ¥ã€‚ æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¼—åŒ…å­¦ä¹ æ–¹æ³•ã€‚å°½ç®¡è¯¥æ–¹æ³•åœ¨é¢†åŸŸé€‚åº”æ–¹é¢çš„æ–°é¢–æ€§æœ‰é™ï¼Œä½†å®ƒæ˜¯ç¬¬ä¸€é¡¹å…³äºä¼—åŒ…å­¦ä¹ çš„å·¥ä½œï¼Œå¹¶èƒ½åœ¨NERä¸Šå–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ é¦–æ¬¡å¼•å…¥äº†æœ‰ç›‘ç£çš„ä¼—åŒ…å­¦ä¹ ï¼Œè¿™æ˜¯ä»é¢†åŸŸé€‚åº”æ€§ä¸­å€Ÿæ¥çš„ï¼Œå°†æ˜¯NLPä»»åŠ¡çš„ä¸€ä¸ªå‰ç»æ€§è§£å†³æ–¹æ¡ˆã€‚ ","date":"2022-03-03","objectID":"/paper01/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#è´¡çŒ®"},{"categories":["documentation"],"content":"æ¨¡å‹æ¶æ„åŒ…æ‹¬å››ä¸ªéƒ¨åˆ†ï¼š (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.æ¨¡å‹ç»“æ„ è¯è¡¨ç¤ºå±‚ï¼ˆword representationï¼‰å‡è®¾å­˜åœ¨ä¸€ä¸ªåŒ…å«$n$ä¸ªå•è¯çš„å¥å­$w_1 \\dots w_n$,æˆ‘ä»¬é¦–å…ˆé€šè¿‡$\\texttt{Adapter} \\circ \\texttt{BERT}$å°†å…¶è½¬æ¢ä¸ºçŸ¢é‡è¡¨å¾. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ æ³¨æ„ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ$\\texttt{Adapter} \\circ \\texttt{BERT}$æ–¹æ³•ä¸å†éœ€è¦å¯¹åºå¤§çš„BERTå‚æ•°è¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡è°ƒæ•´è½»å¾—å¤šçš„é€‚é…å™¨å‚æ•°æ¥è·å¾—ç›¸å½“çš„æ€§èƒ½ã€‚å› æ­¤é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°å°†è¯çš„è¡¨ç¤ºæ³•æ‰©å±•ä¸ºannotator-awareçš„è¡¨ç¤ºæ³•ã€‚ æ³¨é‡Šè€…åˆ‡æ¢å™¨å±‚ï¼ˆannotator switcherï¼‰ä½œè€…ç›®æ ‡æ˜¯æœ‰æ•ˆåœ°å­¦ä¹ ä¸åŒæ•°æ®æ ‡æ³¨å¸ˆæ„è¯†åˆ°çš„è¯æ±‡ç‰¹å¾ï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºå¯¹ä¸ªåˆ«æ³¨é‡Šè€…çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚å› æ­¤ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ³¨é‡Šè€…åˆ‡æ¢å™¨ï¼Œä»¥æ”¯æŒå¸¦æœ‰æ³¨é‡Šè€…è¾“å…¥çš„$\\texttt{Adapter} \\circ \\texttt{BERT}$ï¼Œå…¶çµæ„Ÿæ¥è‡ªäºParameter Generation Network (PGN)ï¼Œå…¶å…³é”®æ€æƒ³æ˜¯ä½¿ç”¨å‚æ•°ç”Ÿæˆç½‘ç»œï¼ˆPGNï¼‰ï¼Œé€šè¿‡è¾“å…¥annotatorsåŠ¨æ€åœ°äº§ç”Ÿé€‚é…å™¨å‚æ•°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸åŒçš„annotatorsä¹‹é—´çµæ´»åœ°åˆ‡æ¢ã€‚ å…·ä½“æ¥è¯´ï¼Œå‡è®¾$V$æ˜¯æ‰€æœ‰é€‚é…å™¨å‚æ•°çš„çŸ¢é‡å½¢å¼ï¼Œé€šè¿‡æ‰“åŒ…æ“ä½œï¼Œä¹Ÿå¯ä»¥è§£åŒ…æ¢å¤æ‰€æœ‰çš„é€‚é…å™¨å‚æ•°ï¼ŒPGNæ¨¡å—å°±æ˜¯æ ¹æ®annotatorsçš„è¾“å…¥åŠ¨æ€ç”Ÿæˆ$\\texttt{Adapter} \\circ \\texttt{BERT}$çš„$V$ï¼Œå¦‚æ¨¡å‹å›¾ä¸­å³è¾¹çš„æ©™è‰²éƒ¨åˆ†æ‰€ç¤ºï¼Œåˆ‡æ¢å™¨switcherå¯ä»¥è¢«å½¢å¼åŒ–ä¸ºï¼š $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ å…¶ä¸­$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$ï¼Œ$\\textbf{x} =\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™$æ˜¯æ³¨é‡Šè€…$a$å¯¹$x=w_1 \\dots w_n$çš„annotator-awareçš„è¡¨ç¤ºï¼Œ$\\textbf{e}^a$æ˜¯annotatorçš„embeddingã€‚ BiLSTMç¼–ç å±‚ï¼ˆBiLSTM Encodingï¼‰$\\texttt{Adapter} \\circ \\texttt{BERT}$éœ€è¦ä¸€ä¸ªé¢å¤–çš„é¢å‘ä»»åŠ¡çš„æ¨¡å—æ¥è¿›è¡Œé«˜çº§ç‰¹å¾æå–ã€‚åœ¨è¿™é‡Œåˆ©ç”¨å•ä¸€çš„BiLSTMå±‚æ¥å®ç°ï¼š$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$ï¼Œç”¨äºä¸‹ä¸€æ­¥çš„æ¨ç†å’Œè®­ç»ƒã€‚ CRFå±‚ï¼ˆCRF inference and trainingï¼‰æœ€åä½¿ç”¨CRFæ¥è®¡ç®—å€™é€‰é¡ºåºè¾“å‡º$y = l_1 \\dots l_n$çš„å…¨å±€å¾—åˆ†ã€‚ $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ å…¶ä¸­$\\textbf{W}^{crf}ã€ \\textbf{b}^{crf}ã€ \\textbf{T}$æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚ç»™å®šä¸€ä¸ªè¾“å…¥$(xï¼Œa)$ï¼Œé€šè¿‡ç»´ç‰¹æ¯”ç®—æ³•è¿›è¡Œæ¨ç†,å¯¹äºè®­ç»ƒï¼Œå®šä¹‰äº†ä¸€ä¸ªå¥å­çº§åˆ«çš„äº¤å‰ç†µç›®æ ‡ã€‚ $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ å…¶ä¸­$y^a$æ˜¯$a$å¯¹$x$çš„é»„é‡‘æ ‡å‡†è¾“å‡ºï¼Œ$y$å±äºæ‰€æœ‰å¯èƒ½çš„å€™é€‰äººï¼Œ$p(y^a|x, a)$è¡¨ç¤ºå¥å­çº§çš„æ¦‚ç‡ã€‚ ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#æ¨¡å‹æ¶æ„"},{"categories":["documentation"],"content":"æ¨¡å‹æ¶æ„åŒ…æ‹¬å››ä¸ªéƒ¨åˆ†ï¼š (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.æ¨¡å‹ç»“æ„ è¯è¡¨ç¤ºå±‚ï¼ˆword representationï¼‰å‡è®¾å­˜åœ¨ä¸€ä¸ªåŒ…å«$n$ä¸ªå•è¯çš„å¥å­$w_1 \\dots w_n$,æˆ‘ä»¬é¦–å…ˆé€šè¿‡$\\texttt{Adapter} \\circ \\texttt{BERT}$å°†å…¶è½¬æ¢ä¸ºçŸ¢é‡è¡¨å¾. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ æ³¨æ„ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ$\\texttt{Adapter} \\circ \\texttt{BERT}$æ–¹æ³•ä¸å†éœ€è¦å¯¹åºå¤§çš„BERTå‚æ•°è¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡è°ƒæ•´è½»å¾—å¤šçš„é€‚é…å™¨å‚æ•°æ¥è·å¾—ç›¸å½“çš„æ€§èƒ½ã€‚å› æ­¤é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°å°†è¯çš„è¡¨ç¤ºæ³•æ‰©å±•ä¸ºannotator-awareçš„è¡¨ç¤ºæ³•ã€‚ æ³¨é‡Šè€…åˆ‡æ¢å™¨å±‚ï¼ˆannotator switcherï¼‰ä½œè€…ç›®æ ‡æ˜¯æœ‰æ•ˆåœ°å­¦ä¹ ä¸åŒæ•°æ®æ ‡æ³¨å¸ˆæ„è¯†åˆ°çš„è¯æ±‡ç‰¹å¾ï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºå¯¹ä¸ªåˆ«æ³¨é‡Šè€…çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚å› æ­¤ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ³¨é‡Šè€…åˆ‡æ¢å™¨ï¼Œä»¥æ”¯æŒå¸¦æœ‰æ³¨é‡Šè€…è¾“å…¥çš„$\\texttt{Adapter} \\circ \\texttt{BERT}$ï¼Œå…¶çµæ„Ÿæ¥è‡ªäºParameter Generation Network (PGN)ï¼Œå…¶å…³é”®æ€æƒ³æ˜¯ä½¿ç”¨å‚æ•°ç”Ÿæˆç½‘ç»œï¼ˆPGNï¼‰ï¼Œé€šè¿‡è¾“å…¥annotatorsåŠ¨æ€åœ°äº§ç”Ÿé€‚é…å™¨å‚æ•°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸åŒçš„annotatorsä¹‹é—´çµæ´»åœ°åˆ‡æ¢ã€‚ å…·ä½“æ¥è¯´ï¼Œå‡è®¾$V$æ˜¯æ‰€æœ‰é€‚é…å™¨å‚æ•°çš„çŸ¢é‡å½¢å¼ï¼Œé€šè¿‡æ‰“åŒ…æ“ä½œï¼Œä¹Ÿå¯ä»¥è§£åŒ…æ¢å¤æ‰€æœ‰çš„é€‚é…å™¨å‚æ•°ï¼ŒPGNæ¨¡å—å°±æ˜¯æ ¹æ®annotatorsçš„è¾“å…¥åŠ¨æ€ç”Ÿæˆ$\\texttt{Adapter} \\circ \\texttt{BERT}$çš„$V$ï¼Œå¦‚æ¨¡å‹å›¾ä¸­å³è¾¹çš„æ©™è‰²éƒ¨åˆ†æ‰€ç¤ºï¼Œåˆ‡æ¢å™¨switcherå¯ä»¥è¢«å½¢å¼åŒ–ä¸ºï¼š $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ å…¶ä¸­$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$ï¼Œ$\\textbf{x} =\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™$æ˜¯æ³¨é‡Šè€…$a$å¯¹$x=w_1 \\dots w_n$çš„annotator-awareçš„è¡¨ç¤ºï¼Œ$\\textbf{e}^a$æ˜¯annotatorçš„embeddingã€‚ BiLSTMç¼–ç å±‚ï¼ˆBiLSTM Encodingï¼‰$\\texttt{Adapter} \\circ \\texttt{BERT}$éœ€è¦ä¸€ä¸ªé¢å¤–çš„é¢å‘ä»»åŠ¡çš„æ¨¡å—æ¥è¿›è¡Œé«˜çº§ç‰¹å¾æå–ã€‚åœ¨è¿™é‡Œåˆ©ç”¨å•ä¸€çš„BiLSTMå±‚æ¥å®ç°ï¼š$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$ï¼Œç”¨äºä¸‹ä¸€æ­¥çš„æ¨ç†å’Œè®­ç»ƒã€‚ CRFå±‚ï¼ˆCRF inference and trainingï¼‰æœ€åä½¿ç”¨CRFæ¥è®¡ç®—å€™é€‰é¡ºåºè¾“å‡º$y = l_1 \\dots l_n$çš„å…¨å±€å¾—åˆ†ã€‚ $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ å…¶ä¸­$\\textbf{W}^{crf}ã€ \\textbf{b}^{crf}ã€ \\textbf{T}$æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚ç»™å®šä¸€ä¸ªè¾“å…¥$(xï¼Œa)$ï¼Œé€šè¿‡ç»´ç‰¹æ¯”ç®—æ³•è¿›è¡Œæ¨ç†,å¯¹äºè®­ç»ƒï¼Œå®šä¹‰äº†ä¸€ä¸ªå¥å­çº§åˆ«çš„äº¤å‰ç†µç›®æ ‡ã€‚ $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ å…¶ä¸­$y^a$æ˜¯$a$å¯¹$x$çš„é»„é‡‘æ ‡å‡†è¾“å‡ºï¼Œ$y$å±äºæ‰€æœ‰å¯èƒ½çš„å€™é€‰äººï¼Œ$p(y^a|x, a)$è¡¨ç¤ºå¥å­çº§çš„æ¦‚ç‡ã€‚ ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#è¯è¡¨ç¤ºå±‚word-representation"},{"categories":["documentation"],"content":"æ¨¡å‹æ¶æ„åŒ…æ‹¬å››ä¸ªéƒ¨åˆ†ï¼š (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.æ¨¡å‹ç»“æ„ è¯è¡¨ç¤ºå±‚ï¼ˆword representationï¼‰å‡è®¾å­˜åœ¨ä¸€ä¸ªåŒ…å«$n$ä¸ªå•è¯çš„å¥å­$w_1 \\dots w_n$,æˆ‘ä»¬é¦–å…ˆé€šè¿‡$\\texttt{Adapter} \\circ \\texttt{BERT}$å°†å…¶è½¬æ¢ä¸ºçŸ¢é‡è¡¨å¾. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ æ³¨æ„ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ$\\texttt{Adapter} \\circ \\texttt{BERT}$æ–¹æ³•ä¸å†éœ€è¦å¯¹åºå¤§çš„BERTå‚æ•°è¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡è°ƒæ•´è½»å¾—å¤šçš„é€‚é…å™¨å‚æ•°æ¥è·å¾—ç›¸å½“çš„æ€§èƒ½ã€‚å› æ­¤é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°å°†è¯çš„è¡¨ç¤ºæ³•æ‰©å±•ä¸ºannotator-awareçš„è¡¨ç¤ºæ³•ã€‚ æ³¨é‡Šè€…åˆ‡æ¢å™¨å±‚ï¼ˆannotator switcherï¼‰ä½œè€…ç›®æ ‡æ˜¯æœ‰æ•ˆåœ°å­¦ä¹ ä¸åŒæ•°æ®æ ‡æ³¨å¸ˆæ„è¯†åˆ°çš„è¯æ±‡ç‰¹å¾ï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºå¯¹ä¸ªåˆ«æ³¨é‡Šè€…çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚å› æ­¤ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ³¨é‡Šè€…åˆ‡æ¢å™¨ï¼Œä»¥æ”¯æŒå¸¦æœ‰æ³¨é‡Šè€…è¾“å…¥çš„$\\texttt{Adapter} \\circ \\texttt{BERT}$ï¼Œå…¶çµæ„Ÿæ¥è‡ªäºParameter Generation Network (PGN)ï¼Œå…¶å…³é”®æ€æƒ³æ˜¯ä½¿ç”¨å‚æ•°ç”Ÿæˆç½‘ç»œï¼ˆPGNï¼‰ï¼Œé€šè¿‡è¾“å…¥annotatorsåŠ¨æ€åœ°äº§ç”Ÿé€‚é…å™¨å‚æ•°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸åŒçš„annotatorsä¹‹é—´çµæ´»åœ°åˆ‡æ¢ã€‚ å…·ä½“æ¥è¯´ï¼Œå‡è®¾$V$æ˜¯æ‰€æœ‰é€‚é…å™¨å‚æ•°çš„çŸ¢é‡å½¢å¼ï¼Œé€šè¿‡æ‰“åŒ…æ“ä½œï¼Œä¹Ÿå¯ä»¥è§£åŒ…æ¢å¤æ‰€æœ‰çš„é€‚é…å™¨å‚æ•°ï¼ŒPGNæ¨¡å—å°±æ˜¯æ ¹æ®annotatorsçš„è¾“å…¥åŠ¨æ€ç”Ÿæˆ$\\texttt{Adapter} \\circ \\texttt{BERT}$çš„$V$ï¼Œå¦‚æ¨¡å‹å›¾ä¸­å³è¾¹çš„æ©™è‰²éƒ¨åˆ†æ‰€ç¤ºï¼Œåˆ‡æ¢å™¨switcherå¯ä»¥è¢«å½¢å¼åŒ–ä¸ºï¼š $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ å…¶ä¸­$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$ï¼Œ$\\textbf{x} =\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™$æ˜¯æ³¨é‡Šè€…$a$å¯¹$x=w_1 \\dots w_n$çš„annotator-awareçš„è¡¨ç¤ºï¼Œ$\\textbf{e}^a$æ˜¯annotatorçš„embeddingã€‚ BiLSTMç¼–ç å±‚ï¼ˆBiLSTM Encodingï¼‰$\\texttt{Adapter} \\circ \\texttt{BERT}$éœ€è¦ä¸€ä¸ªé¢å¤–çš„é¢å‘ä»»åŠ¡çš„æ¨¡å—æ¥è¿›è¡Œé«˜çº§ç‰¹å¾æå–ã€‚åœ¨è¿™é‡Œåˆ©ç”¨å•ä¸€çš„BiLSTMå±‚æ¥å®ç°ï¼š$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$ï¼Œç”¨äºä¸‹ä¸€æ­¥çš„æ¨ç†å’Œè®­ç»ƒã€‚ CRFå±‚ï¼ˆCRF inference and trainingï¼‰æœ€åä½¿ç”¨CRFæ¥è®¡ç®—å€™é€‰é¡ºåºè¾“å‡º$y = l_1 \\dots l_n$çš„å…¨å±€å¾—åˆ†ã€‚ $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ å…¶ä¸­$\\textbf{W}^{crf}ã€ \\textbf{b}^{crf}ã€ \\textbf{T}$æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚ç»™å®šä¸€ä¸ªè¾“å…¥$(xï¼Œa)$ï¼Œé€šè¿‡ç»´ç‰¹æ¯”ç®—æ³•è¿›è¡Œæ¨ç†,å¯¹äºè®­ç»ƒï¼Œå®šä¹‰äº†ä¸€ä¸ªå¥å­çº§åˆ«çš„äº¤å‰ç†µç›®æ ‡ã€‚ $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ å…¶ä¸­$y^a$æ˜¯$a$å¯¹$x$çš„é»„é‡‘æ ‡å‡†è¾“å‡ºï¼Œ$y$å±äºæ‰€æœ‰å¯èƒ½çš„å€™é€‰äººï¼Œ$p(y^a|x, a)$è¡¨ç¤ºå¥å­çº§çš„æ¦‚ç‡ã€‚ ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#æ³¨é‡Šè€…åˆ‡æ¢å™¨å±‚annotator-switcher"},{"categories":["documentation"],"content":"æ¨¡å‹æ¶æ„åŒ…æ‹¬å››ä¸ªéƒ¨åˆ†ï¼š (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.æ¨¡å‹ç»“æ„ è¯è¡¨ç¤ºå±‚ï¼ˆword representationï¼‰å‡è®¾å­˜åœ¨ä¸€ä¸ªåŒ…å«$n$ä¸ªå•è¯çš„å¥å­$w_1 \\dots w_n$,æˆ‘ä»¬é¦–å…ˆé€šè¿‡$\\texttt{Adapter} \\circ \\texttt{BERT}$å°†å…¶è½¬æ¢ä¸ºçŸ¢é‡è¡¨å¾. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ æ³¨æ„ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ$\\texttt{Adapter} \\circ \\texttt{BERT}$æ–¹æ³•ä¸å†éœ€è¦å¯¹åºå¤§çš„BERTå‚æ•°è¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡è°ƒæ•´è½»å¾—å¤šçš„é€‚é…å™¨å‚æ•°æ¥è·å¾—ç›¸å½“çš„æ€§èƒ½ã€‚å› æ­¤é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°å°†è¯çš„è¡¨ç¤ºæ³•æ‰©å±•ä¸ºannotator-awareçš„è¡¨ç¤ºæ³•ã€‚ æ³¨é‡Šè€…åˆ‡æ¢å™¨å±‚ï¼ˆannotator switcherï¼‰ä½œè€…ç›®æ ‡æ˜¯æœ‰æ•ˆåœ°å­¦ä¹ ä¸åŒæ•°æ®æ ‡æ³¨å¸ˆæ„è¯†åˆ°çš„è¯æ±‡ç‰¹å¾ï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºå¯¹ä¸ªåˆ«æ³¨é‡Šè€…çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚å› æ­¤ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ³¨é‡Šè€…åˆ‡æ¢å™¨ï¼Œä»¥æ”¯æŒå¸¦æœ‰æ³¨é‡Šè€…è¾“å…¥çš„$\\texttt{Adapter} \\circ \\texttt{BERT}$ï¼Œå…¶çµæ„Ÿæ¥è‡ªäºParameter Generation Network (PGN)ï¼Œå…¶å…³é”®æ€æƒ³æ˜¯ä½¿ç”¨å‚æ•°ç”Ÿæˆç½‘ç»œï¼ˆPGNï¼‰ï¼Œé€šè¿‡è¾“å…¥annotatorsåŠ¨æ€åœ°äº§ç”Ÿé€‚é…å™¨å‚æ•°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸åŒçš„annotatorsä¹‹é—´çµæ´»åœ°åˆ‡æ¢ã€‚ å…·ä½“æ¥è¯´ï¼Œå‡è®¾$V$æ˜¯æ‰€æœ‰é€‚é…å™¨å‚æ•°çš„çŸ¢é‡å½¢å¼ï¼Œé€šè¿‡æ‰“åŒ…æ“ä½œï¼Œä¹Ÿå¯ä»¥è§£åŒ…æ¢å¤æ‰€æœ‰çš„é€‚é…å™¨å‚æ•°ï¼ŒPGNæ¨¡å—å°±æ˜¯æ ¹æ®annotatorsçš„è¾“å…¥åŠ¨æ€ç”Ÿæˆ$\\texttt{Adapter} \\circ \\texttt{BERT}$çš„$V$ï¼Œå¦‚æ¨¡å‹å›¾ä¸­å³è¾¹çš„æ©™è‰²éƒ¨åˆ†æ‰€ç¤ºï¼Œåˆ‡æ¢å™¨switcherå¯ä»¥è¢«å½¢å¼åŒ–ä¸ºï¼š $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ å…¶ä¸­$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$ï¼Œ$\\textbf{x} =\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™$æ˜¯æ³¨é‡Šè€…$a$å¯¹$x=w_1 \\dots w_n$çš„annotator-awareçš„è¡¨ç¤ºï¼Œ$\\textbf{e}^a$æ˜¯annotatorçš„embeddingã€‚ BiLSTMç¼–ç å±‚ï¼ˆBiLSTM Encodingï¼‰$\\texttt{Adapter} \\circ \\texttt{BERT}$éœ€è¦ä¸€ä¸ªé¢å¤–çš„é¢å‘ä»»åŠ¡çš„æ¨¡å—æ¥è¿›è¡Œé«˜çº§ç‰¹å¾æå–ã€‚åœ¨è¿™é‡Œåˆ©ç”¨å•ä¸€çš„BiLSTMå±‚æ¥å®ç°ï¼š$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$ï¼Œç”¨äºä¸‹ä¸€æ­¥çš„æ¨ç†å’Œè®­ç»ƒã€‚ CRFå±‚ï¼ˆCRF inference and trainingï¼‰æœ€åä½¿ç”¨CRFæ¥è®¡ç®—å€™é€‰é¡ºåºè¾“å‡º$y = l_1 \\dots l_n$çš„å…¨å±€å¾—åˆ†ã€‚ $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ å…¶ä¸­$\\textbf{W}^{crf}ã€ \\textbf{b}^{crf}ã€ \\textbf{T}$æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚ç»™å®šä¸€ä¸ªè¾“å…¥$(xï¼Œa)$ï¼Œé€šè¿‡ç»´ç‰¹æ¯”ç®—æ³•è¿›è¡Œæ¨ç†,å¯¹äºè®­ç»ƒï¼Œå®šä¹‰äº†ä¸€ä¸ªå¥å­çº§åˆ«çš„äº¤å‰ç†µç›®æ ‡ã€‚ $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ å…¶ä¸­$y^a$æ˜¯$a$å¯¹$x$çš„é»„é‡‘æ ‡å‡†è¾“å‡ºï¼Œ$y$å±äºæ‰€æœ‰å¯èƒ½çš„å€™é€‰äººï¼Œ$p(y^a|x, a)$è¡¨ç¤ºå¥å­çº§çš„æ¦‚ç‡ã€‚ ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#bilstmç¼–ç å±‚bilstm-encoding"},{"categories":["documentation"],"content":"æ¨¡å‹æ¶æ„åŒ…æ‹¬å››ä¸ªéƒ¨åˆ†ï¼š (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.æ¨¡å‹ç»“æ„ è¯è¡¨ç¤ºå±‚ï¼ˆword representationï¼‰å‡è®¾å­˜åœ¨ä¸€ä¸ªåŒ…å«$n$ä¸ªå•è¯çš„å¥å­$w_1 \\dots w_n$,æˆ‘ä»¬é¦–å…ˆé€šè¿‡$\\texttt{Adapter} \\circ \\texttt{BERT}$å°†å…¶è½¬æ¢ä¸ºçŸ¢é‡è¡¨å¾. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ æ³¨æ„ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ$\\texttt{Adapter} \\circ \\texttt{BERT}$æ–¹æ³•ä¸å†éœ€è¦å¯¹åºå¤§çš„BERTå‚æ•°è¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡è°ƒæ•´è½»å¾—å¤šçš„é€‚é…å™¨å‚æ•°æ¥è·å¾—ç›¸å½“çš„æ€§èƒ½ã€‚å› æ­¤é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°å°†è¯çš„è¡¨ç¤ºæ³•æ‰©å±•ä¸ºannotator-awareçš„è¡¨ç¤ºæ³•ã€‚ æ³¨é‡Šè€…åˆ‡æ¢å™¨å±‚ï¼ˆannotator switcherï¼‰ä½œè€…ç›®æ ‡æ˜¯æœ‰æ•ˆåœ°å­¦ä¹ ä¸åŒæ•°æ®æ ‡æ³¨å¸ˆæ„è¯†åˆ°çš„è¯æ±‡ç‰¹å¾ï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºå¯¹ä¸ªåˆ«æ³¨é‡Šè€…çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚å› æ­¤ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ³¨é‡Šè€…åˆ‡æ¢å™¨ï¼Œä»¥æ”¯æŒå¸¦æœ‰æ³¨é‡Šè€…è¾“å…¥çš„$\\texttt{Adapter} \\circ \\texttt{BERT}$ï¼Œå…¶çµæ„Ÿæ¥è‡ªäºParameter Generation Network (PGN)ï¼Œå…¶å…³é”®æ€æƒ³æ˜¯ä½¿ç”¨å‚æ•°ç”Ÿæˆç½‘ç»œï¼ˆPGNï¼‰ï¼Œé€šè¿‡è¾“å…¥annotatorsåŠ¨æ€åœ°äº§ç”Ÿé€‚é…å™¨å‚æ•°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸åŒçš„annotatorsä¹‹é—´çµæ´»åœ°åˆ‡æ¢ã€‚ å…·ä½“æ¥è¯´ï¼Œå‡è®¾$V$æ˜¯æ‰€æœ‰é€‚é…å™¨å‚æ•°çš„çŸ¢é‡å½¢å¼ï¼Œé€šè¿‡æ‰“åŒ…æ“ä½œï¼Œä¹Ÿå¯ä»¥è§£åŒ…æ¢å¤æ‰€æœ‰çš„é€‚é…å™¨å‚æ•°ï¼ŒPGNæ¨¡å—å°±æ˜¯æ ¹æ®annotatorsçš„è¾“å…¥åŠ¨æ€ç”Ÿæˆ$\\texttt{Adapter} \\circ \\texttt{BERT}$çš„$V$ï¼Œå¦‚æ¨¡å‹å›¾ä¸­å³è¾¹çš„æ©™è‰²éƒ¨åˆ†æ‰€ç¤ºï¼Œåˆ‡æ¢å™¨switcherå¯ä»¥è¢«å½¢å¼åŒ–ä¸ºï¼š $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ å…¶ä¸­$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$ï¼Œ$\\textbf{x} =\\textbf{r}_1â€™ \\dots \\textbf{n}_1â€™$æ˜¯æ³¨é‡Šè€…$a$å¯¹$x=w_1 \\dots w_n$çš„annotator-awareçš„è¡¨ç¤ºï¼Œ$\\textbf{e}^a$æ˜¯annotatorçš„embeddingã€‚ BiLSTMç¼–ç å±‚ï¼ˆBiLSTM Encodingï¼‰$\\texttt{Adapter} \\circ \\texttt{BERT}$éœ€è¦ä¸€ä¸ªé¢å¤–çš„é¢å‘ä»»åŠ¡çš„æ¨¡å—æ¥è¿›è¡Œé«˜çº§ç‰¹å¾æå–ã€‚åœ¨è¿™é‡Œåˆ©ç”¨å•ä¸€çš„BiLSTMå±‚æ¥å®ç°ï¼š$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$ï¼Œç”¨äºä¸‹ä¸€æ­¥çš„æ¨ç†å’Œè®­ç»ƒã€‚ CRFå±‚ï¼ˆCRF inference and trainingï¼‰æœ€åä½¿ç”¨CRFæ¥è®¡ç®—å€™é€‰é¡ºåºè¾“å‡º$y = l_1 \\dots l_n$çš„å…¨å±€å¾—åˆ†ã€‚ $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ å…¶ä¸­$\\textbf{W}^{crf}ã€ \\textbf{b}^{crf}ã€ \\textbf{T}$æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚ç»™å®šä¸€ä¸ªè¾“å…¥$(xï¼Œa)$ï¼Œé€šè¿‡ç»´ç‰¹æ¯”ç®—æ³•è¿›è¡Œæ¨ç†,å¯¹äºè®­ç»ƒï¼Œå®šä¹‰äº†ä¸€ä¸ªå¥å­çº§åˆ«çš„äº¤å‰ç†µç›®æ ‡ã€‚ $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ å…¶ä¸­$y^a$æ˜¯$a$å¯¹$x$çš„é»„é‡‘æ ‡å‡†è¾“å‡ºï¼Œ$y$å±äºæ‰€æœ‰å¯èƒ½çš„å€™é€‰äººï¼Œ$p(y^a|x, a)$è¡¨ç¤ºå¥å­çº§çš„æ¦‚ç‡ã€‚ ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#crfå±‚crf-inference-and-training"},{"categories":["documentation"],"content":"ç»“æœä¸è®¨è®º Fig-1.æ— ç›‘ç£å­¦ä¹ çš„å®éªŒç»“æœ Fig-1æ˜¾ç¤ºäº†æ— ç›‘ç£æƒ…å†µä¸‹çš„æµ‹è¯•ç»“æœã€‚ä»æ•´ä½“ä¸Šçœ‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¡¨å¾å­¦ä¹ æ¨¡å‹é€šè¿‡å€Ÿç”¨äº†Domain Adaptationï¼Œå¯ä»¥è¾¾åˆ°æœ€å¥½çš„æ€§èƒ½ï¼ŒF1å¾—åˆ†è¾¾åˆ°77.95ï¼Œæ˜æ˜¾ä¼˜äºç¬¬äºŒå¥½çš„æ¨¡å‹LC-catã€‚ è¿™ä¸€ç»“æœè¡¨æ˜ä½œè€…æå‡ºçš„æ–¹æ³•æ¯”å…¶ä»–æ¨¡å‹æ›´æœ‰ä¼˜åŠ¿ã€‚é€šè¿‡æ·±å…¥ç ”ç©¶ç»“æœï¼Œå¯ä»¥å‘ç°ï¼Œannotator-awareæ¨¡å‹æ˜æ˜¾ä¼˜äºannotator-agnosticæ¨¡å‹ï¼Œè¡¨æ˜æ³¨é‡Šè€…ä¿¡æ¯å¯¹ä¼—åŒ…å­¦ä¹ æœ‰å¾ˆå¤§å¸®åŠ©ï¼Œè¿™ä¸ªè§‚å¯Ÿç»“æœè¿›ä¸€æ­¥æ˜¾ç¤ºäº†å°†ä¸åŒæ³¨é‡Šè€…ç±»æ¯”ä¸åŒé¢†åŸŸçš„åˆç†æ€§ï¼Œå› ä¸ºé¢†åŸŸä¿¡æ¯å¯¹äºé¢†åŸŸé€‚åº”ä¹Ÿæ˜¯æœ‰ç”¨çš„ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºçš„è¡¨å¾å­¦ä¹ æ–¹æ³•åœ¨annotator-awareæ¨¡å‹ä¸­çš„è¡¨ç°æ›´å¥½ï¼Œè¡¨æ˜æ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰æ³¨é‡Šè€…æ„ŸçŸ¥çš„ä¿¡æ¯ã€‚ Fig-2.æœ‰ç›‘ç£å­¦ä¹ çš„å®éªŒç»“æœ ä¸ºäº†ç ”ç©¶æœ‰ç›‘ç£æƒ…å†µï¼Œæˆ‘ä»¬å‡è®¾æ‰€æœ‰ä¼—åŒ…å¥å­çš„ä¸“å®¶æ³¨é‡Šæ˜¯å¯ç”¨çš„ã€‚é™¤äº†æ¢ç´¢å®Œæ•´çš„ä¸“å®¶æ³¨é‡Šï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å¦å¤–ä¸‰ç§ä¸åŒçš„æƒ…å†µï¼Œå³åœ¨æ— ç›‘ç£ç¯å¢ƒä¸‹é€æ­¥å¢åŠ ä¸“å®¶æ³¨é‡Šï¼Œç›®çš„æ˜¯ç ”ç©¶æˆ‘ä»¬çš„æ¨¡å‹åœ¨å°è§„æ¨¡ä¸“å®¶æ³¨é‡Šä¸‹çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾ä¸“å®¶æ³¨é‡Šçš„æ¯”ä¾‹ä¸º1%ã€5%ã€25%å’Œ100%ã€‚ Fig-2æ˜¾ç¤ºäº†æ‰€æœ‰çš„ç»“æœï¼ŒåŒ…æ‹¬å››ä¸ªbaselineså’Œä¸€ä¸ªåªåŸºäºä¸“å®¶æ³¨é‡Šçš„goldæ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚æ€»çš„æ¥è¯´ï¼Œä½œè€…æå‡ºçš„è¡¨å¾å­¦ä¹ æ¨¡å‹å¯ä»¥ä¸ºæ‰€æœ‰çš„åœºæ™¯å¸¦æ¥æœ€å¥½çš„è¡¨ç°ï¼Œè¯æ˜å®ƒåœ¨ç›‘ç£å­¦ä¹ ä¸­ä¹Ÿæ˜¯æœ‰æ•ˆçš„ã€‚ æ¥ä¸‹æ¥ï¼Œé€šè¿‡æ¯”è¾ƒannotator-agnosticæ¨¡å‹å’Œannotator-awareæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°annotator-awareæ¨¡å‹æ•ˆæœæ›´å¥½ï¼Œè¿™ä¸æ— ç›‘ç£çš„è®¾ç½®æ˜¯ä¸€è‡´çš„ã€‚ æ›´æœ‰è¶£çš„æ˜¯ï¼Œç»“æœæ˜¾ç¤ºåœ¨ä¸“å®¶æ³¨é‡Šè§„æ¨¡å¾ˆå°çš„æƒ…å†µä¸‹ï¼ˆ1%å’Œ5%ï¼‰ï¼ŒAllæ¯”goldå¥½ï¼Œè€Œåªæœ‰åœ¨æœ‰è¶³å¤Ÿçš„ä¸“å®¶æ³¨é‡Šæ—¶ï¼ˆ25%å’Œ100%ï¼‰ï¼Œè¿™ä¸€è¶‹åŠ¿æ‰ä¼šé€†è½¬ã€‚ è¯¥è§‚å¯Ÿè¡¨æ˜ï¼Œå½“é»„é‡‘æ³¨é‡Š(gold annotations)ä¸è¶³æ—¶ï¼Œä¼—åŒ…æ³¨é‡Š(crowdsourced annotations)æ€»æ˜¯æœ‰å¸®åŠ©çš„ï¼ŒåŒæ—¶ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“ç†è§£MVæ¯”goldå·®ï¼Œå› ä¸ºåè€…çš„è®­ç»ƒè¯­æ–™è´¨é‡æ›´é«˜ã€‚ æ­¤å¤–ï¼Œå¯ä»¥å‘ç°å³ä½¿æ˜¯å¢åŠ annotator-awareæœºåˆ¶çš„LCå’ŒLC-catæ¨¡å‹ä¹Ÿæ— æ³•è·å¾—ä¸gold annotationsç±»ä¼¼çš„æ•ˆæœï¼Œè¿™è¡¨æ˜ä»ä¼—åŒ…æ³¨é‡Šä¸­æç‚¼æ›´åŠ ä¼˜ç§€çš„æ•°æ®æ ‡æ³¨å¯èƒ½ä¸æ˜¯æœ€æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œè¡¨å¾å­¦ä¹ æ¨¡å‹å¯ä»¥æŒç»­ç»™å‡ºæ¯”gold annotationsæ›´å¥½çš„ç»“æœï¼Œè¡¨æ˜ä¼—åŒ…æ³¨é‡Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•æ€»æ˜¯æœ‰å¸®åŠ©ã€‚ ","date":"2022-03-03","objectID":"/paper01/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#ç»“æœä¸è®¨è®º"},{"categories":["documentation"],"content":"ä»£ç è§£è¯»å¾…å¤ç° ","date":"2022-03-03","objectID":"/paper01/:0:6","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#ä»£ç è§£è¯»"},{"categories":["documentation"],"content":"Begioç»å…¸è®ºæ–‡â€™A Neural Probabilistic Language Modelâ€˜","date":"2022-03-01","objectID":"/nnlm/","series":null,"tags":["Model","NLP","ç»å…¸è®ºæ–‡ç ”è¯»ç³»åˆ—"],"title":"01-NNLM(â€™A Neural Probabilistic Language Modelâ€˜) ","uri":"/nnlm/"},{"categories":["documentation"],"content":"A Neural Probabilistic Language Modelè¿™ç¯‡è®ºæ–‡æ˜¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¼€å±±ä¹‹ä½œï¼ŒYoshua Bengioç­‰äº2003å¹´æå‡ºçš„æ–¹æ³•ã€‚ ","date":"2022-03-01","objectID":"/nnlm/:1:0","series":null,"tags":["Model","NLP","ç»å…¸è®ºæ–‡ç ”è¯»ç³»åˆ—"],"title":"01-NNLM(â€™A Neural Probabilistic Language Modelâ€˜) ","uri":"/nnlm/#a-neural-probabilistic-language-model"},{"categories":["documentation"],"content":"è§‚ç‚¹ å°†è¯æ±‡è¡¨$V$ä¸­çš„æ¯ä¸ªå•è¯${w_i}$å…³è”åˆ°ä¸€ä¸ªåˆ†å¸ƒå¼å•è¯ç‰¹å¾å‘é‡$\\mathcal{R}^m$ã€‚ å°†å¥å­çš„è”åˆæ¦‚ç‡å‡½æ•°è¡¨ç¤ºä¸ºå¥å­åºåˆ—ä¸­å•è¯ç‰¹å¾å‘é‡çš„ç»„åˆã€‚ åŒæ—¶å­¦ä¹ å•è¯çš„ç‰¹å¾å‘é‡å’Œå¥å­è”åˆæ¦‚ç‡å‡½æ•°çš„å‚æ•°ã€‚ ","date":"2022-03-01","objectID":"/nnlm/:1:1","series":null,"tags":["Model","NLP","ç»å…¸è®ºæ–‡ç ”è¯»ç³»åˆ—"],"title":"01-NNLM(â€™A Neural Probabilistic Language Modelâ€˜) ","uri":"/nnlm/#è§‚ç‚¹"},{"categories":["documentation"],"content":"æ¨¡å‹å‡è®¾å­˜åœ¨å¥å­$w_1,\\dotsï¼Œw_i,\\dots,w_n$ï¼Œå…¶ä¸­$w_n \\in V$ï¼Œ$V$è¡¨ç¤ºè¯æ±‡é›†åˆï¼Œ$w_i$è¡¨ç¤ºå•è¯ï¼Œç›®æ ‡å‡½æ•°æ˜¯å­¦ä¹ $f(w_t,\\dots,w_{t-n+1})=\\hat{P}(w_t \\vert w_1^{t-1})$çš„å‚æ•°ã€‚ Bengioç­‰äººå°†æ¨¡å‹åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼š ä¸€ä¸ªæ˜ å°„å‡½æ•°$C$ï¼Œå°† $V$ä¸­çš„ç¬¬$i$ä¸ªå•è¯$w_i$æ˜ å°„æˆä¸ºä¸€ä¸ª ç‰¹å¾å‘é‡ $C(w_i)\\in \\mathcal{R}^m$ï¼Œå®ƒè¡¨ç¤ºè¯æ±‡è¡¨ä¸­ä¸æ¯ä¸ªå•è¯ç›¸å…³çš„åˆ†å¸ƒç‰¹å¾å‘é‡ã€‚ ä¸€ä¸ªä½¿ç”¨æ˜ å°„å‡½æ•°$C$è¡¨ç¤ºçš„æ¦‚ç‡å‡½æ•°$g$ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡ä¸­å•è¯çš„ç‰¹å¾å‘é‡çš„ä¹˜ç§¯ç»„æˆè”åˆæ¦‚ç‡æ¨¡å‹ï¼Œ$g$çš„è¾“å‡ºæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå®ƒçš„ç¬¬$i$ä¸ªå…ƒç´ ä¼°è®¡äº†æ¦‚ç‡ã€‚ $$ f(i,w_{t-1},\\dots,w_{t-n+1})=g(i,C(w_{t-1}),\\dots,C(w_{t-n+1})) $$ å‡½æ•°$f$æ˜¯è¿™ä¸¤ä¸ªæ˜ å°„($C$å’Œ$g$)çš„ç»„åˆï¼Œä¸Šä¸‹æ–‡ä¸­çš„æ‰€æœ‰å•è¯éƒ½å…±äº«$C$ã€‚ä¸è¿™ä¸¤ä¸ªéƒ¨åˆ†çš„æ¯ä¸ªéƒ¨åˆ†å…³è”ä¸€äº›å‚æ•°ã€‚ æ•°å­¦ç¬¦å·è¯´æ˜ï¼š $C(i)$ï¼šå•è¯$w$å¯¹åº”çš„è¯å‘é‡ï¼Œå…¶ä¸­$i$ä¸ºè¯$w$åœ¨æ•´ä¸ªè¯æ±‡è¡¨ä¸­çš„ç´¢å¼• $C$ï¼šè¯å‘é‡ï¼Œå¤§å°ä¸º$\\vert V \\vert \\times m$çš„çŸ©é˜µ $\\vert V \\vert$ï¼šè¯æ±‡è¡¨çš„å¤§å°ï¼Œå³é¢„æ–™åº“ä¸­å»é‡åçš„å•è¯ä¸ªæ•° $m$ï¼šè¯å‘é‡çš„ç»´åº¦ï¼Œä¸€èˆ¬å¤§äº50 $H$ï¼šéšè—å±‚çš„ weight $d$ï¼šéšè—å±‚çš„ bias $U$ï¼šè¾“å‡ºå±‚çš„ weight $b$ï¼šè¾“å‡ºå±‚çš„ bias $W$ï¼šè¾“å…¥å±‚åˆ°è¾“å‡ºå±‚çš„ weight $h$ï¼šéšè—å±‚ç¥ç»å…ƒä¸ªæ•° è®¡ç®—æµç¨‹ï¼š é¦–å…ˆå°†è¾“å…¥çš„$n-1$ä¸ªå•è¯ç´¢å¼•è½¬ä¸ºè¯å‘é‡ï¼Œç„¶åå°†è¿™$n-1$ä¸ªå‘é‡è¿›è¡Œ concatï¼Œå½¢æˆä¸€ä¸ª$(n-1)\\times w$ çš„çŸ©é˜µï¼Œç”¨$X$è¡¨ç¤º å°†$X$é€å…¥éšè—å±‚è¿›è¡Œè®¡ç®—ï¼Œ$\\textit{hidden}_\\text{out}=\\tanh{(d + X * H)}$ è¾“å‡ºå±‚å…±æœ‰$\\vert V \\vert$ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹$y_i$è¡¨ç¤ºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯$i$çš„æ¦‚ç‡ï¼Œ $y$çš„è®¡ç®—å…¬å¼ä¸º$y=b+X*W+\\textit{hidden}_\\text{out} * U$ ","date":"2022-03-01","objectID":"/nnlm/:1:2","series":null,"tags":["Model","NLP","ç»å…¸è®ºæ–‡ç ”è¯»ç³»åˆ—"],"title":"01-NNLM(â€™A Neural Probabilistic Language Modelâ€˜) ","uri":"/nnlm/#æ¨¡å‹"},{"categories":["documentation"],"content":"ä»£ç  # code by Tae Hwan Jung @graykode, modify by wmathor import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as Data dtype = torch.FloatTensor sentences = [ \"i like dog\", \"i love coffee\", \"i hate milk\"] word_list = \" \".join(sentences).split() # ['i', 'like', 'dog', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk'] word_list = list(set(word_list)) # ['i', 'like', 'dog', 'love', 'coffee', 'hate', 'milk'] word_dict = {w: i for i, w in enumerate(word_list)} # {'i':0, 'like':1, 'dog':2, 'love':3, 'coffee':4, 'hate':5, 'milk':6} number_dict = {i: w for i, w in enumerate(word_list)} # {0:'i', 1:'like', 2:'dog', 3:'love', 4:'coffee', 5:'hate', 6:'milk'} n_class = len(word_dict) # number of Vocabulary, just like |V|, in this task n_class=7 # NNLM(Neural Network Language Model) Parameter n_step = len(sentences[0].split())-1 # n-1 in paper, look back n_step words and predict next word. In this task n_step=2 n_hidden = 2 # h in paper m = 2 # m in paper, word embedding dim def make_batch(sentences): input_batch = [] target_batch = [] for sen in sentences: word = sen.split() input = [word_dict[n] for n in word[:-1]] # [0, 1], [0, 3], [0, 5] target = word_dict[word[-1]] # 2, 4, 6 input_batch.append(input) # [[0, 1], [0, 3], [0, 5]] target_batch.append(target) # [2, 4, 6] return input_batch, target_batch input_batch, target_batch = make_batch(sentences) input_batch = torch.LongTensor(input_batch) target_batch = torch.LongTensor(target_batch) dataset = Data.TensorDataset(input_batch, target_batch) loader = Data.DataLoader(dataset=dataset, batch_size=16, shuffle=True) class NNLM(nn.Module): def __init__(self): super(NNLM, self).__init__() self.C = nn.Embedding(n_class, m) self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(dtype)) self.W = nn.Parameter(torch.randn(n_step * m, n_class).type(dtype)) self.d = nn.Parameter(torch.randn(n_hidden).type(dtype)) self.U = nn.Parameter(torch.randn(n_hidden, n_class).type(dtype)) self.b = nn.Parameter(torch.randn(n_class).type(dtype)) def forward(self, X): ''' X: [batch_size, n_step] ''' X = self.C(X) # [batch_size, n_step] =\u003e [batch_size, n_step, m] X = X.view(-1, n_step * m) # [batch_size, n_step * m] hidden_out = torch.tanh(self.d + torch.mm(X, self.H)) # [batch_size, n_hidden] output = self.b + torch.mm(X, self.W) + torch.mm(hidden_out, self.U) # [batch_size, n_class] return output model = NNLM() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=1e-3) # Training for epoch in range(5000): for batch_x, batch_y in loader: optimizer.zero_grad() output = model(batch_x) # output : [batch_size, n_class], batch_y : [batch_size] (LongTensor, not one-hot) loss = criterion(output, batch_y) if (epoch + 1)%1000 == 0: print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss)) loss.backward() optimizer.step() # Predict predict = model(input_batch).data.max(1, keepdim=True)[1] # Test print([sen.split()[:n_step] for sen in sentences], '-\u003e', [number_dict[n.item()] for n in predict.squeeze()]) ","date":"2022-03-01","objectID":"/nnlm/:2:0","series":null,"tags":["Model","NLP","ç»å…¸è®ºæ–‡ç ”è¯»ç³»åˆ—"],"title":"01-NNLM(â€™A Neural Probabilistic Language Modelâ€˜) ","uri":"/nnlm/#ä»£ç "},{"categories":["documentation"],"content":"å‚è€ƒA Neural Probabilistic Language Model NNLM çš„ PyTorch å®ç° nlp-tutorial ","date":"2022-03-01","objectID":"/nnlm/:3:0","series":null,"tags":["Model","NLP","ç»å…¸è®ºæ–‡ç ”è¯»ç³»åˆ—"],"title":"01-NNLM(â€™A Neural Probabilistic Language Modelâ€˜) ","uri":"/nnlm/#å‚è€ƒ"},{"categories":null,"content":"å…³äºæˆ‘ä¸­å›½ç§‘å­¦é™¢å¤§å­¦è®¡ç®—æœºåº”ç”¨æŠ€æœ¯ç ”ç©¶ç”Ÿä¸€å¹´çº§åœ¨è¯»ï¼Œä¸»è¦ç ”ç©¶æ–¹å‘æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæ¬¢è¿é‚®ä»¶è”ç³»æˆ‘è¿›è¡Œäº¤æµğŸ‘ğŸ»ï¼Œç‰¹åˆ«æ¬¢è¿carryæˆ‘å‘è®ºæ–‡ï¼ ","date":"2022-02-28","objectID":"/about/:1:0","series":null,"tags":null,"title":"About","uri":"/about/#å…³äºæˆ‘"},{"categories":null,"content":"TagsğŸ‘¨â€ğŸ’» ç¨‹åºçŒ¿ ğŸ’» æŠ€æœ¯æå®¢ï¼Œçƒ­çˆ±å…³äºè®¡ç®—æœºçš„ä¸€åˆ‡ ğŸ¤ª å¼ºè¿«ç—‡ä¸æ‹–å»¶ç—‡æ‚£è€… ğŸ¤” æ•°ç çˆ±å¥½è€… ğŸ å…¨å®¶æ¡¶æ‹¥æœ‰è€…(bushi) ","date":"2022-02-28","objectID":"/about/:2:0","series":null,"tags":null,"title":"About","uri":"/about/#tags"},{"categories":null,"content":"å…³äºç‰ˆæƒæœ¬ç«™æ‰€æœ‰çš„åŸåˆ›æ–‡ç« å‡å— åˆ›ä½œå…±äº« ç½²å-éå•†ä¸šæ€§ 4.0 è®¸å¯åè®® / CC BY-NC 4.0 ä¿æŠ¤ã€‚ ","date":"2022-02-28","objectID":"/about/:0:0","series":null,"tags":null,"title":"About","uri":"/about/#å…³äºç‰ˆæƒ"},{"categories":["documentation"],"content":"æœ¬æ•™ç¨‹å¸Œæœ›ä¸ºå…¥é—¨æ•°æ®ç§‘å­¦ã€DeepLearningçš„åŒå­¦æä¾›Numpyçš„åŸºæœ¬æ“ä½œæŒ‡å—ã€‚","date":"2022-02-28","objectID":"/numpyguidebook/","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/"},{"categories":["documentation"],"content":"æœ¬æ•™ç¨‹å¸Œæœ›ä¸ºå…¥é—¨æ•°æ®ç§‘å­¦ã€DeepLearningçš„åŒå­¦æä¾›Numpyçš„åŸºæœ¬æ“ä½œæŒ‡å—ã€‚ ","date":"2022-02-28","objectID":"/numpyguidebook/:0:0","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#"},{"categories":["documentation"],"content":"Numpy å…¥é—¨æŒ‡å—","date":"2022-02-28","objectID":"/numpyguidebook/:1:0","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#numpy-å…¥é—¨æŒ‡å—"},{"categories":["documentation"],"content":"arrayåŸºæœ¬å±æ€§Numpyçš„ä¸»è¦å¯¹è±¡æ˜¯åŒæ„å¤šç»´æ•°ç»„ã€‚å®ƒæ˜¯ä¸€ä¸ªå…ƒç´ è¡¨ï¼Œæ‰€æœ‰ç±»å‹éƒ½ç›¸åŒï¼Œç”±éè´Ÿæ•´æ•°å…ƒç»„æ„æˆç´¢å¼•ã€‚ Numpyçš„æ•°ç»„ç±»è¢«è°ƒç”¨ä¸ºndarrayã€‚å­˜åœ¨ä»¥ä¸‹å±æ€§ï¼š ndarray.ndimï¼šæ•°ç»„çš„è½´ï¼ˆç»´åº¦ï¼‰çš„ä¸ªæ•°ã€‚ ndarray.shapeï¼šæ•°ç»„çš„ç»´åº¦ã€‚ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œè¡¨ç¤ºæ¯ä¸ªç»´åº¦ä¸­æ•°ç»„çš„å¤§å°ã€‚å¯¹äºæœ‰nè¡Œå’Œmåˆ—çš„çŸ©é˜µï¼Œshapeå°†æ˜¯(n,m)ï¼Œå³shapeå…ƒç»„é•¿åº¦å°±æ˜¯rankæˆ–è€…ç»´åº¦çš„ä¸ªæ•°ndimã€‚ ndarray.sizeï¼šæ•°ç»„å…ƒç´ çš„æ€»æ•°ã€‚ ndarray.dtypeï¼š ä¸€ä¸ªæè¿°æ•°ç»„ä¸­å…ƒç´ ç±»å‹çš„å¯¹è±¡ ã€‚ ndarray.itemsizeï¼šæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚å¤§å°ã€‚ä¾‹å¦‚ï¼Œå…ƒç´ ä¸º float64 ç±»å‹çš„æ•°ç»„çš„ itemsize ä¸º8ï¼ˆ=64/8ï¼‰ï¼Œè€Œ complex32 ç±»å‹çš„æ•°ç»„çš„ itemsize ä¸º4ï¼ˆ=32/8ï¼‰ã€‚å®ƒç­‰äº ndarray.dtype.itemsize ã€‚ import numpy as np #å¦‚ä½•å°†åˆ—è¡¨è½¬åŒ–ä¸ºçŸ©é˜µ array=np.array([[1,2,3], [2,3,4]]) print(array) #æŸ¥çœ‹ç»´åº¦ndim print('number of dim: ',array.ndim) ##output: number of dim: 2 #æŸ¥çœ‹å‡ è¡Œå‡ åˆ— print('shape: ',array.shape) ##output: shape: (2, 3) #æŸ¥çœ‹å…ƒç´ ä¸ªæ•° print('size: ',array.size) ##output: size: 6 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:1","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayåŸºæœ¬å±æ€§"},{"categories":["documentation"],"content":"åˆ›å»ºæ•°ç»„np.arrayä½¿ç”¨arrayå‡½æ•°ä»pythonå…ƒç»„ä¸­åˆ›å»ºæ•°ç»„, é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„æ•°ç»„çš„dtypeæ˜¯ float64 ç±»å‹çš„ã€‚ import numpy as np #åˆ›å»ºä¸€ç»´æ•°ç»„ï¼Œndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #åˆ›å»ºäºŒç»´æ•°ç»„ b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] æ³¨æ„ï¼šå¸¸è§é”™è¯¯æ˜¯ï¼Œè°ƒç”¨arrayæ—¶å€™ä¼ å…¥å¤šä¸ªæ•°å­—å‚æ•°ï¼Œè€Œä¸æä¾›å•ä¸ªæ•°å­—çš„åˆ—è¡¨ç±»å‹ä½œä¸ºå‚æ•°ã€‚ \u003e\u003e\u003e a = np.array(1,2,3,4) # WRONG \u003e\u003e\u003e a = np.array([1,2,3,4]) # RIGHT np.zerosåˆ›å»ºä¸€ä¸ªå…¨ä¸º0çš„æ•°ç»„ . \u003e\u003e\u003e np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.onesåˆ›å»ºä¸€ä¸ªå…¨ä¸º1çš„æ•°ç»„ . \u003e\u003e\u003e np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.emptyåˆ›å»ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶åˆå§‹å†…å®¹æ˜¯éšæœºçš„ï¼Œå–å†³äºå†…å­˜çš„çŠ¶æ€ã€‚ \u003e\u003e\u003e np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arangeè¯¥å‡½æ•°è¿”å›æŒ‡å®šèŒƒå›´å†…æ•°ç»„è€Œä¸æ˜¯åˆ—è¡¨ ã€‚ï¼ˆæ³¨æ„æ˜¯å·¦åŒ…å«å³[start,stop) ï¼‰ numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) ä¸»è¦å‚æ•°ï¼šstartâ€“å¼€å§‹ï¼›stepâ€“ç»“æŸï¼›step:æ­¥é•¿ \u003e\u003e\u003e np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) \u003e\u003e\u003e np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspaceå½“arangeä¸æµ®ç‚¹å‚æ•°ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œç”±äºæœ‰é™çš„æµ®ç‚¹ç²¾åº¦ï¼Œé€šå¸¸ä¸å¯èƒ½é¢„æµ‹æ‰€è·å¾—çš„å…ƒç´ çš„æ•°é‡ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œé€šå¸¸æœ€å¥½ä½¿ç”¨linspaceå‡½æ•°æ¥æ¥æ”¶æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ æ•°é‡çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯æ­¥é•¿ï¼ˆstepï¼‰ def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): \u003e\u003e\u003e from numpy import pi \u003e\u003e\u003e np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) \u003e\u003e\u003e x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points \u003e\u003e\u003e f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#åˆ›å»ºæ•°ç»„"},{"categories":["documentation"],"content":"åˆ›å»ºæ•°ç»„np.arrayä½¿ç”¨arrayå‡½æ•°ä»pythonå…ƒç»„ä¸­åˆ›å»ºæ•°ç»„, é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„æ•°ç»„çš„dtypeæ˜¯ float64 ç±»å‹çš„ã€‚ import numpy as np #åˆ›å»ºä¸€ç»´æ•°ç»„ï¼Œndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #åˆ›å»ºäºŒç»´æ•°ç»„ b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] æ³¨æ„ï¼šå¸¸è§é”™è¯¯æ˜¯ï¼Œè°ƒç”¨arrayæ—¶å€™ä¼ å…¥å¤šä¸ªæ•°å­—å‚æ•°ï¼Œè€Œä¸æä¾›å•ä¸ªæ•°å­—çš„åˆ—è¡¨ç±»å‹ä½œä¸ºå‚æ•°ã€‚ a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zerosåˆ›å»ºä¸€ä¸ªå…¨ä¸º0çš„æ•°ç»„ . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.onesåˆ›å»ºä¸€ä¸ªå…¨ä¸º1çš„æ•°ç»„ . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.emptyåˆ›å»ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶åˆå§‹å†…å®¹æ˜¯éšæœºçš„ï¼Œå–å†³äºå†…å­˜çš„çŠ¶æ€ã€‚ np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arangeè¯¥å‡½æ•°è¿”å›æŒ‡å®šèŒƒå›´å†…æ•°ç»„è€Œä¸æ˜¯åˆ—è¡¨ ã€‚ï¼ˆæ³¨æ„æ˜¯å·¦åŒ…å«å³[start,stop) ï¼‰ numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) ä¸»è¦å‚æ•°ï¼šstartâ€“å¼€å§‹ï¼›stepâ€“ç»“æŸï¼›step:æ­¥é•¿ np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspaceå½“arangeä¸æµ®ç‚¹å‚æ•°ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œç”±äºæœ‰é™çš„æµ®ç‚¹ç²¾åº¦ï¼Œé€šå¸¸ä¸å¯èƒ½é¢„æµ‹æ‰€è·å¾—çš„å…ƒç´ çš„æ•°é‡ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œé€šå¸¸æœ€å¥½ä½¿ç”¨linspaceå‡½æ•°æ¥æ¥æ”¶æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ æ•°é‡çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯æ­¥é•¿ï¼ˆstepï¼‰ def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nparray"},{"categories":["documentation"],"content":"åˆ›å»ºæ•°ç»„np.arrayä½¿ç”¨arrayå‡½æ•°ä»pythonå…ƒç»„ä¸­åˆ›å»ºæ•°ç»„, é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„æ•°ç»„çš„dtypeæ˜¯ float64 ç±»å‹çš„ã€‚ import numpy as np #åˆ›å»ºä¸€ç»´æ•°ç»„ï¼Œndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #åˆ›å»ºäºŒç»´æ•°ç»„ b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] æ³¨æ„ï¼šå¸¸è§é”™è¯¯æ˜¯ï¼Œè°ƒç”¨arrayæ—¶å€™ä¼ å…¥å¤šä¸ªæ•°å­—å‚æ•°ï¼Œè€Œä¸æä¾›å•ä¸ªæ•°å­—çš„åˆ—è¡¨ç±»å‹ä½œä¸ºå‚æ•°ã€‚ a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zerosåˆ›å»ºä¸€ä¸ªå…¨ä¸º0çš„æ•°ç»„ . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.onesåˆ›å»ºä¸€ä¸ªå…¨ä¸º1çš„æ•°ç»„ . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.emptyåˆ›å»ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶åˆå§‹å†…å®¹æ˜¯éšæœºçš„ï¼Œå–å†³äºå†…å­˜çš„çŠ¶æ€ã€‚ np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arangeè¯¥å‡½æ•°è¿”å›æŒ‡å®šèŒƒå›´å†…æ•°ç»„è€Œä¸æ˜¯åˆ—è¡¨ ã€‚ï¼ˆæ³¨æ„æ˜¯å·¦åŒ…å«å³[start,stop) ï¼‰ numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) ä¸»è¦å‚æ•°ï¼šstartâ€“å¼€å§‹ï¼›stepâ€“ç»“æŸï¼›step:æ­¥é•¿ np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspaceå½“arangeä¸æµ®ç‚¹å‚æ•°ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œç”±äºæœ‰é™çš„æµ®ç‚¹ç²¾åº¦ï¼Œé€šå¸¸ä¸å¯èƒ½é¢„æµ‹æ‰€è·å¾—çš„å…ƒç´ çš„æ•°é‡ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œé€šå¸¸æœ€å¥½ä½¿ç”¨linspaceå‡½æ•°æ¥æ¥æ”¶æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ æ•°é‡çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯æ­¥é•¿ï¼ˆstepï¼‰ def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npzeros"},{"categories":["documentation"],"content":"åˆ›å»ºæ•°ç»„np.arrayä½¿ç”¨arrayå‡½æ•°ä»pythonå…ƒç»„ä¸­åˆ›å»ºæ•°ç»„, é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„æ•°ç»„çš„dtypeæ˜¯ float64 ç±»å‹çš„ã€‚ import numpy as np #åˆ›å»ºä¸€ç»´æ•°ç»„ï¼Œndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #åˆ›å»ºäºŒç»´æ•°ç»„ b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] æ³¨æ„ï¼šå¸¸è§é”™è¯¯æ˜¯ï¼Œè°ƒç”¨arrayæ—¶å€™ä¼ å…¥å¤šä¸ªæ•°å­—å‚æ•°ï¼Œè€Œä¸æä¾›å•ä¸ªæ•°å­—çš„åˆ—è¡¨ç±»å‹ä½œä¸ºå‚æ•°ã€‚ a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zerosåˆ›å»ºä¸€ä¸ªå…¨ä¸º0çš„æ•°ç»„ . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.onesåˆ›å»ºä¸€ä¸ªå…¨ä¸º1çš„æ•°ç»„ . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.emptyåˆ›å»ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶åˆå§‹å†…å®¹æ˜¯éšæœºçš„ï¼Œå–å†³äºå†…å­˜çš„çŠ¶æ€ã€‚ np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arangeè¯¥å‡½æ•°è¿”å›æŒ‡å®šèŒƒå›´å†…æ•°ç»„è€Œä¸æ˜¯åˆ—è¡¨ ã€‚ï¼ˆæ³¨æ„æ˜¯å·¦åŒ…å«å³[start,stop) ï¼‰ numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) ä¸»è¦å‚æ•°ï¼šstartâ€“å¼€å§‹ï¼›stepâ€“ç»“æŸï¼›step:æ­¥é•¿ np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspaceå½“arangeä¸æµ®ç‚¹å‚æ•°ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œç”±äºæœ‰é™çš„æµ®ç‚¹ç²¾åº¦ï¼Œé€šå¸¸ä¸å¯èƒ½é¢„æµ‹æ‰€è·å¾—çš„å…ƒç´ çš„æ•°é‡ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œé€šå¸¸æœ€å¥½ä½¿ç”¨linspaceå‡½æ•°æ¥æ¥æ”¶æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ æ•°é‡çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯æ­¥é•¿ï¼ˆstepï¼‰ def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npones"},{"categories":["documentation"],"content":"åˆ›å»ºæ•°ç»„np.arrayä½¿ç”¨arrayå‡½æ•°ä»pythonå…ƒç»„ä¸­åˆ›å»ºæ•°ç»„, é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„æ•°ç»„çš„dtypeæ˜¯ float64 ç±»å‹çš„ã€‚ import numpy as np #åˆ›å»ºä¸€ç»´æ•°ç»„ï¼Œndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #åˆ›å»ºäºŒç»´æ•°ç»„ b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] æ³¨æ„ï¼šå¸¸è§é”™è¯¯æ˜¯ï¼Œè°ƒç”¨arrayæ—¶å€™ä¼ å…¥å¤šä¸ªæ•°å­—å‚æ•°ï¼Œè€Œä¸æä¾›å•ä¸ªæ•°å­—çš„åˆ—è¡¨ç±»å‹ä½œä¸ºå‚æ•°ã€‚ a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zerosåˆ›å»ºä¸€ä¸ªå…¨ä¸º0çš„æ•°ç»„ . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.onesåˆ›å»ºä¸€ä¸ªå…¨ä¸º1çš„æ•°ç»„ . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.emptyåˆ›å»ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶åˆå§‹å†…å®¹æ˜¯éšæœºçš„ï¼Œå–å†³äºå†…å­˜çš„çŠ¶æ€ã€‚ np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arangeè¯¥å‡½æ•°è¿”å›æŒ‡å®šèŒƒå›´å†…æ•°ç»„è€Œä¸æ˜¯åˆ—è¡¨ ã€‚ï¼ˆæ³¨æ„æ˜¯å·¦åŒ…å«å³[start,stop) ï¼‰ numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) ä¸»è¦å‚æ•°ï¼šstartâ€“å¼€å§‹ï¼›stepâ€“ç»“æŸï¼›step:æ­¥é•¿ np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspaceå½“arangeä¸æµ®ç‚¹å‚æ•°ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œç”±äºæœ‰é™çš„æµ®ç‚¹ç²¾åº¦ï¼Œé€šå¸¸ä¸å¯èƒ½é¢„æµ‹æ‰€è·å¾—çš„å…ƒç´ çš„æ•°é‡ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œé€šå¸¸æœ€å¥½ä½¿ç”¨linspaceå‡½æ•°æ¥æ¥æ”¶æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ æ•°é‡çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯æ­¥é•¿ï¼ˆstepï¼‰ def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npempty"},{"categories":["documentation"],"content":"åˆ›å»ºæ•°ç»„np.arrayä½¿ç”¨arrayå‡½æ•°ä»pythonå…ƒç»„ä¸­åˆ›å»ºæ•°ç»„, é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„æ•°ç»„çš„dtypeæ˜¯ float64 ç±»å‹çš„ã€‚ import numpy as np #åˆ›å»ºä¸€ç»´æ•°ç»„ï¼Œndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #åˆ›å»ºäºŒç»´æ•°ç»„ b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] æ³¨æ„ï¼šå¸¸è§é”™è¯¯æ˜¯ï¼Œè°ƒç”¨arrayæ—¶å€™ä¼ å…¥å¤šä¸ªæ•°å­—å‚æ•°ï¼Œè€Œä¸æä¾›å•ä¸ªæ•°å­—çš„åˆ—è¡¨ç±»å‹ä½œä¸ºå‚æ•°ã€‚ a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zerosåˆ›å»ºä¸€ä¸ªå…¨ä¸º0çš„æ•°ç»„ . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.onesåˆ›å»ºä¸€ä¸ªå…¨ä¸º1çš„æ•°ç»„ . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.emptyåˆ›å»ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶åˆå§‹å†…å®¹æ˜¯éšæœºçš„ï¼Œå–å†³äºå†…å­˜çš„çŠ¶æ€ã€‚ np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arangeè¯¥å‡½æ•°è¿”å›æŒ‡å®šèŒƒå›´å†…æ•°ç»„è€Œä¸æ˜¯åˆ—è¡¨ ã€‚ï¼ˆæ³¨æ„æ˜¯å·¦åŒ…å«å³[start,stop) ï¼‰ numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) ä¸»è¦å‚æ•°ï¼šstartâ€“å¼€å§‹ï¼›stepâ€“ç»“æŸï¼›step:æ­¥é•¿ np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspaceå½“arangeä¸æµ®ç‚¹å‚æ•°ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œç”±äºæœ‰é™çš„æµ®ç‚¹ç²¾åº¦ï¼Œé€šå¸¸ä¸å¯èƒ½é¢„æµ‹æ‰€è·å¾—çš„å…ƒç´ çš„æ•°é‡ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œé€šå¸¸æœ€å¥½ä½¿ç”¨linspaceå‡½æ•°æ¥æ¥æ”¶æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ æ•°é‡çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯æ­¥é•¿ï¼ˆstepï¼‰ def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nparange"},{"categories":["documentation"],"content":"åˆ›å»ºæ•°ç»„np.arrayä½¿ç”¨arrayå‡½æ•°ä»pythonå…ƒç»„ä¸­åˆ›å»ºæ•°ç»„, é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„æ•°ç»„çš„dtypeæ˜¯ float64 ç±»å‹çš„ã€‚ import numpy as np #åˆ›å»ºä¸€ç»´æ•°ç»„ï¼Œndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #åˆ›å»ºäºŒç»´æ•°ç»„ b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] æ³¨æ„ï¼šå¸¸è§é”™è¯¯æ˜¯ï¼Œè°ƒç”¨arrayæ—¶å€™ä¼ å…¥å¤šä¸ªæ•°å­—å‚æ•°ï¼Œè€Œä¸æä¾›å•ä¸ªæ•°å­—çš„åˆ—è¡¨ç±»å‹ä½œä¸ºå‚æ•°ã€‚ a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zerosåˆ›å»ºä¸€ä¸ªå…¨ä¸º0çš„æ•°ç»„ . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.onesåˆ›å»ºä¸€ä¸ªå…¨ä¸º1çš„æ•°ç»„ . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.emptyåˆ›å»ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶åˆå§‹å†…å®¹æ˜¯éšæœºçš„ï¼Œå–å†³äºå†…å­˜çš„çŠ¶æ€ã€‚ np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arangeè¯¥å‡½æ•°è¿”å›æŒ‡å®šèŒƒå›´å†…æ•°ç»„è€Œä¸æ˜¯åˆ—è¡¨ ã€‚ï¼ˆæ³¨æ„æ˜¯å·¦åŒ…å«å³[start,stop) ï¼‰ numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) ä¸»è¦å‚æ•°ï¼šstartâ€“å¼€å§‹ï¼›stepâ€“ç»“æŸï¼›step:æ­¥é•¿ np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspaceå½“arangeä¸æµ®ç‚¹å‚æ•°ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œç”±äºæœ‰é™çš„æµ®ç‚¹ç²¾åº¦ï¼Œé€šå¸¸ä¸å¯èƒ½é¢„æµ‹æ‰€è·å¾—çš„å…ƒç´ çš„æ•°é‡ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œé€šå¸¸æœ€å¥½ä½¿ç”¨linspaceå‡½æ•°æ¥æ¥æ”¶æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ æ•°é‡çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯æ­¥é•¿ï¼ˆstepï¼‰ def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nplinspace"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°\u003eã€\u003cã€==ã€\u003e=ã€ \u003c= ã€!=ï¼Œè¿”å›ä¸€ä¸ªå…¨ä¸ºå¸ƒå°”å€¼çš„æ•°ç»„ import numpy as np b=np.arange(4) ##outputï¼š[0 1 2 3] #å¿«é€ŸæŸ¥æ‰¾ç¬¦åˆè¦æ±‚çš„å€¼,é€»è¾‘åˆ¤æ–­ print(b==3,'\\n') #output :[False False False True] print(b!=3,'\\n') #outputï¼š[ True True True False] è½¬ç§© import numpy as np B=np.arange(14,2, -1).reshape((3,4)) # B :array([[14, 13, 12, 11], # [10, 9, 8, 7], # [ 6, 5, 4, 3]]) print(np.transpose(B)) #[[14 10 6] # [13 9 5] # [12 8 4] # [11 7 3]] print(B.T) #[[14 10 6] # [13 9 5] # [12 8 4] # [11 7 3]] np.sortå¯¹çŸ©é˜µä¸­çš„æ‰€æœ‰å€¼ä»å¤§åˆ°å°æ’åºã€‚ #æ’åºå‡½æ•°ï¼Œsort(),é’ˆå¯¹æ¯ä¸€è¡Œè¿›è¡Œä»å°åˆ°å¤§æ’åºæ“ä½œ B=np.arange(14,2, -1).reshape((3,4)) # B :array([[14, 13, 12, 11], # [10, 9, 8, 7], # [ 6, 5, 4, 3]]) print(np.sort(B)) # B':array([[11,12,13,14], # [ 7, 8, 9,10], # [ 3, 4, 5, 6]]) np.clipclipå‡½æ•°ï¼Œclip(Array,Array_min,Array_max)ï¼ŒArrayæŒ‡çš„æ˜¯å°†è¦è¢«æ‰§è¡Œç”¨çš„çŸ©é˜µï¼Œè€Œåé¢çš„æœ€å°å€¼æœ€å¤§å€¼åˆ™ç”¨äºè®©å‡½æ•°åˆ¤æ–­çŸ©é˜µä¸­å…ƒç´ æ˜¯å¦æœ‰æ¯”æœ€å°å€¼å°çš„æˆ–è€…æ¯”æœ€å¤§å€¼å¤§çš„å…ƒç´ ï¼Œå¹¶å°†è¿™äº›æŒ‡å®šçš„å…ƒç´ è½¬æ¢ä¸ºæœ€å°å€¼æˆ–è€…æœ€å¤§å€¼ã€‚ import numpy as np A=np.arange(2,14).reshape((3,4)) print(np.clip(A,5,9)) np.argminæŸ¥æ‰¾çŸ©é˜µä¸­çš„æœ€å°å€¼çš„ç´¢å¼•å€¼ np.argmaxæŸ¥æ‰¾çŸ©é˜µä¸­çš„æœ€å¤§å€¼çš„ç´¢å¼•å€¼ import numpy as np A=np.arange(2,14).reshape((3,4)) #[[ 2 3 4 5] # [ 6 7 8 9] # [10 11 12 13]] #numpyåŸºæœ¬è¿ç®— print(A) #æ±‚çŸ©é˜µä¸­æœ€å°å…ƒç´  print('æœ€å°å€¼çš„ç´¢å¼•å€¼',np.argmin(A)) ##æœ€å°å€¼çš„ç´¢å¼•å€¼ 0 #æ±‚çŸ©é˜µä¸­æœ€å¤§å…ƒç´  print('æœ€å¤§å€¼çš„ç´¢å¼•å€¼',np.argmax(A)) #æœ€å¤§å€¼çš„ç´¢å¼•å€¼ 11 np.meanæ±‚çŸ©é˜µæ‰€æœ‰å€¼çš„å‡å€¼,äº¦å†™æˆA.mean() åŒnp.average( ) np.average import numpy as np A=np.arange(2,14).reshape((3,4)) #æ±‚çŸ©é˜µçš„å‡å€¼ print('çŸ©é˜µå¹³å‡å€¼è¡¨ç¤ºä¹‹ä¸€',np.mean(A),'|',A.mean()) #çŸ©é˜µå¹³å‡å€¼è¡¨ç¤ºä¹‹ä¸€ 7.5 | 7.5 print('çŸ©é˜µå¹³å‡å€¼è¡¨ç¤ºä¹‹äºŒ',np.average(A)) #çŸ©é˜µå¹³å‡å€¼è¡¨ç¤ºä¹‹äºŒ 7.5 np.cumsum import numpy as np A=np.arange(2,14).reshape((3,4)) #æ±‚çŸ©é˜µné¡¹ç´¯åŠ  #eg: array([ [ 2, 3, 4, 5] # [ 6, 7, 8, 9] # [10,11,12,13] ]) # ---\u003e[2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], ---\u003e [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ \u003e\u003e\u003e b=np.array([2,4,6]) \u003e\u003e\u003e np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° \u003e\u003e\u003e c=np.array([4,9,16]) \u003e\u003e\u003e np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#æ•°ç»„åŸºæœ¬è¿ç®—"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#åŠ å‡è¿ç®—"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#ç‚¹ä¹˜å‰ä¹˜"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#ä¹˜æ–¹"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#é€»è¾‘è¿ç®—"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#è½¬ç§©"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npsort"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npclip"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npargmin"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npargmax"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npmean"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npaverage"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npcumsum"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npdiff"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npexp"},{"categories":["documentation"],"content":"æ•°ç»„åŸºæœ¬è¿ç®—åŠ å‡è¿ç®— import numpy as np #åŠ å‡è¿ç®— a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] ç‚¹ä¹˜ã€å‰ä¹˜ import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #å‰ä¹˜ c=a*b print(\"\\nå‰ä¹˜è¿ç®—:\",c) ##output:å‰ä¹˜è¿ç®—: [ 0 20 60 120] #ç‚¹ä¹˜ aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹ä¸€:\",c_dot) ##ç‚¹ä¹˜è¿ç®—ä¹‹ä¸€: [[2 4] ## [2 3]] print(\"\\nç‚¹ä¹˜è¿ç®—ä¹‹äºŒ:\",c_dot_2) ##ç‚¹ä¹˜è¿ç®—ä¹‹äºŒ: [[2 4] ## [2 3]] ä¹˜æ–¹ä½¿ç”¨a**bè¡¨ç¤ºaçš„bæ¬¡æ–¹ import numpy as np b=np.arange(4) #ä¹˜æ–¹è¿ç®— f=b**2 print(\"\\nä¹˜æ–¹è¿ç®—:\",f) #output:[0 1 4 9] é€»è¾‘è¿ç®—å¿«é€ŸæŸ¥æ‰¾æ•°ç»„ä¸­ç¬¦åˆæ¡ä»¶çš„å€¼ï¼Œæ¶‰åŠåˆ°ã€=ã€ [2 5 9 14 20 27 35 44 54 65 77 90] print('çŸ©é˜µå‰né¡¹ç´¯åŠ ',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #ç´¯å·®è¿ç®—å‡½æ•°diff,è®¡ç®—çš„ä¾¿æ˜¯æ¯ä¸€è¡Œä¸­åä¸€é¡¹ä¸å‰ä¸€é¡¹ä¹‹å·®. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.expæ±‚eçš„å¹‚æ¬¡æ–¹ã€‚ b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrtå¼€æ–¹å‡½æ•° c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npsqrt"},{"categories":["documentation"],"content":"ç´¢å¼•ã€åˆ‡ç‰‡å’Œè¿­ä»£ä¸€ç»´æ•°ç»„ä¸€ç»´çš„æ•°ç»„å¯ä»¥è¿›è¡Œç´¢å¼•ã€åˆ‡ç‰‡å’Œè¿­ä»£æ“ä½œã€‚ \u003e\u003e\u003e import numpy as np \u003e\u003e\u003e a=np.arange(10)**3 \u003e\u003e\u003e a array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) \u003e\u003e\u003e a[2] #è·å–ç¬¬äºŒä¸ªå€¼ 8 \u003e\u003e\u003e a[2:5] #è·å–ç¬¬äºŒåˆ°ç¬¬äº”ä¸ªå€¼ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 8, 27, 64], dtype=int32) \u003e\u003e\u003e a[:6:2]=-1000 #ä¿®æ”¹ç¬¬é›¶ä¸ªã€ç¬¬äºŒä¸ªã€ç¬¬å…­ä¸ªå€¼ä¸º-1000 \u003e\u003e\u003e a array([ -1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729], dtype=int32) \u003e\u003e\u003e a[ : :-1] #å€’åºa array([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000], dtype=int32) \u003e\u003e\u003e for i in a: ... print(i**(1/3.)) ... nan 1.0 nan 3.0 nan 5.0 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 å¤šç»´æ•°ç»„å¤šç»´æ•°ç»„çš„æ¯ä¸€ä¸ªè½´éƒ½æœ‰ä¸€ä¸ªç´¢å¼•ï¼Œè¿™äº›ç´¢å¼•ä»¥é€—å·çš„å½¢å¼åˆ†éš”çš„å…ƒç»„ç»™å‡ºï¼š \u003e\u003e\u003e def f(x,y): ... return 5*x+y ... \u003e\u003e\u003e b=np.fromfunction(f,(5,4),dtype=int) \u003e\u003e\u003e b array([[ 0, 1, 2, 3], [ 5, 6, 7, 8], [10, 11, 12, 13], [15, 16, 17, 18], [20, 21, 22, 23]]) \u003e\u003e\u003e b[2,3] #ç¬¬äºŒè¡Œç¬¬ä¸‰åˆ—çš„æ•°å­— 13 \u003e\u003e\u003e b[0:5,1] #ç¬¬0~5è¡Œç¬¬1åˆ—çš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 1, 6, 11, 16, 21]) \u003e\u003e\u003e b[ : ,1] #ç¬¬1åˆ—çš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 1, 6, 11, 16, 21]) \u003e\u003e\u003e b[1:3,:] #ç¬¬1~3è¡Œçš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([[ 5, 6, 7, 8], [10, 11, 12, 13]]) å¯¹å¤šç»´æ•°ç»„è¿›è¡Œè¿­ä»£ï¼ˆiteratingï¼‰æ˜¯ç›¸å¯¹äºç¬¬ä¸€ä¸ªè½´å®Œæˆçš„ã€‚ \u003e\u003e\u003e for row in b: ... print(row) ... [0 1 2 3] [5 6 7 8] [10 11 12 13] [15 16 17 18] [20 21 22 23] è¿­ä»£æ“ä½œå¦‚æœæƒ³è¦å¯¹æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œæ“ä½œï¼Œå¯ä»¥ä½¿ç”¨flatå±æ€§ï¼Œè¯¥å±æ€§æ˜¯æ•°ç»„çš„æ‰€æœ‰å…ƒç´ çš„è¿­ä»£å™¨ : \u003e\u003e\u003e for element in b.flat: ... print(element) ... 0 1 2 3 5 6 7 8 10 11 12 13 15 16 17 18 20 21 22 23 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:4","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#ç´¢å¼•åˆ‡ç‰‡å’Œè¿­ä»£"},{"categories":["documentation"],"content":"ç´¢å¼•ã€åˆ‡ç‰‡å’Œè¿­ä»£ä¸€ç»´æ•°ç»„ä¸€ç»´çš„æ•°ç»„å¯ä»¥è¿›è¡Œç´¢å¼•ã€åˆ‡ç‰‡å’Œè¿­ä»£æ“ä½œã€‚ import numpy as np a=np.arange(10)**3 a array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) a[2] #è·å–ç¬¬äºŒä¸ªå€¼ 8 a[2:5] #è·å–ç¬¬äºŒåˆ°ç¬¬äº”ä¸ªå€¼ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 8, 27, 64], dtype=int32) a[:6:2]=-1000 #ä¿®æ”¹ç¬¬é›¶ä¸ªã€ç¬¬äºŒä¸ªã€ç¬¬å…­ä¸ªå€¼ä¸º-1000 a array([ -1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729], dtype=int32) a[ : :-1] #å€’åºa array([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000], dtype=int32) for i in a: ... print(i**(1/3.)) ... nan 1.0 nan 3.0 nan 5.0 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 å¤šç»´æ•°ç»„å¤šç»´æ•°ç»„çš„æ¯ä¸€ä¸ªè½´éƒ½æœ‰ä¸€ä¸ªç´¢å¼•ï¼Œè¿™äº›ç´¢å¼•ä»¥é€—å·çš„å½¢å¼åˆ†éš”çš„å…ƒç»„ç»™å‡ºï¼š def f(x,y): ... return 5*x+y ... b=np.fromfunction(f,(5,4),dtype=int) b array([[ 0, 1, 2, 3], [ 5, 6, 7, 8], [10, 11, 12, 13], [15, 16, 17, 18], [20, 21, 22, 23]]) b[2,3] #ç¬¬äºŒè¡Œç¬¬ä¸‰åˆ—çš„æ•°å­— 13 b[0:5,1] #ç¬¬0~5è¡Œç¬¬1åˆ—çš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 1, 6, 11, 16, 21]) b[ : ,1] #ç¬¬1åˆ—çš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 1, 6, 11, 16, 21]) b[1:3,:] #ç¬¬1~3è¡Œçš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([[ 5, 6, 7, 8], [10, 11, 12, 13]]) å¯¹å¤šç»´æ•°ç»„è¿›è¡Œè¿­ä»£ï¼ˆiteratingï¼‰æ˜¯ç›¸å¯¹äºç¬¬ä¸€ä¸ªè½´å®Œæˆçš„ã€‚ for row in b: ... print(row) ... [0 1 2 3] [5 6 7 8] [10 11 12 13] [15 16 17 18] [20 21 22 23] è¿­ä»£æ“ä½œå¦‚æœæƒ³è¦å¯¹æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œæ“ä½œï¼Œå¯ä»¥ä½¿ç”¨flatå±æ€§ï¼Œè¯¥å±æ€§æ˜¯æ•°ç»„çš„æ‰€æœ‰å…ƒç´ çš„è¿­ä»£å™¨ : for element in b.flat: ... print(element) ... 0 1 2 3 5 6 7 8 10 11 12 13 15 16 17 18 20 21 22 23 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:4","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#ä¸€ç»´æ•°ç»„"},{"categories":["documentation"],"content":"ç´¢å¼•ã€åˆ‡ç‰‡å’Œè¿­ä»£ä¸€ç»´æ•°ç»„ä¸€ç»´çš„æ•°ç»„å¯ä»¥è¿›è¡Œç´¢å¼•ã€åˆ‡ç‰‡å’Œè¿­ä»£æ“ä½œã€‚ import numpy as np a=np.arange(10)**3 a array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) a[2] #è·å–ç¬¬äºŒä¸ªå€¼ 8 a[2:5] #è·å–ç¬¬äºŒåˆ°ç¬¬äº”ä¸ªå€¼ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 8, 27, 64], dtype=int32) a[:6:2]=-1000 #ä¿®æ”¹ç¬¬é›¶ä¸ªã€ç¬¬äºŒä¸ªã€ç¬¬å…­ä¸ªå€¼ä¸º-1000 a array([ -1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729], dtype=int32) a[ : :-1] #å€’åºa array([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000], dtype=int32) for i in a: ... print(i**(1/3.)) ... nan 1.0 nan 3.0 nan 5.0 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 å¤šç»´æ•°ç»„å¤šç»´æ•°ç»„çš„æ¯ä¸€ä¸ªè½´éƒ½æœ‰ä¸€ä¸ªç´¢å¼•ï¼Œè¿™äº›ç´¢å¼•ä»¥é€—å·çš„å½¢å¼åˆ†éš”çš„å…ƒç»„ç»™å‡ºï¼š def f(x,y): ... return 5*x+y ... b=np.fromfunction(f,(5,4),dtype=int) b array([[ 0, 1, 2, 3], [ 5, 6, 7, 8], [10, 11, 12, 13], [15, 16, 17, 18], [20, 21, 22, 23]]) b[2,3] #ç¬¬äºŒè¡Œç¬¬ä¸‰åˆ—çš„æ•°å­— 13 b[0:5,1] #ç¬¬0~5è¡Œç¬¬1åˆ—çš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 1, 6, 11, 16, 21]) b[ : ,1] #ç¬¬1åˆ—çš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 1, 6, 11, 16, 21]) b[1:3,:] #ç¬¬1~3è¡Œçš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([[ 5, 6, 7, 8], [10, 11, 12, 13]]) å¯¹å¤šç»´æ•°ç»„è¿›è¡Œè¿­ä»£ï¼ˆiteratingï¼‰æ˜¯ç›¸å¯¹äºç¬¬ä¸€ä¸ªè½´å®Œæˆçš„ã€‚ for row in b: ... print(row) ... [0 1 2 3] [5 6 7 8] [10 11 12 13] [15 16 17 18] [20 21 22 23] è¿­ä»£æ“ä½œå¦‚æœæƒ³è¦å¯¹æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œæ“ä½œï¼Œå¯ä»¥ä½¿ç”¨flatå±æ€§ï¼Œè¯¥å±æ€§æ˜¯æ•°ç»„çš„æ‰€æœ‰å…ƒç´ çš„è¿­ä»£å™¨ : for element in b.flat: ... print(element) ... 0 1 2 3 5 6 7 8 10 11 12 13 15 16 17 18 20 21 22 23 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:4","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#å¤šç»´æ•°ç»„"},{"categories":["documentation"],"content":"ç´¢å¼•ã€åˆ‡ç‰‡å’Œè¿­ä»£ä¸€ç»´æ•°ç»„ä¸€ç»´çš„æ•°ç»„å¯ä»¥è¿›è¡Œç´¢å¼•ã€åˆ‡ç‰‡å’Œè¿­ä»£æ“ä½œã€‚ import numpy as np a=np.arange(10)**3 a array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) a[2] #è·å–ç¬¬äºŒä¸ªå€¼ 8 a[2:5] #è·å–ç¬¬äºŒåˆ°ç¬¬äº”ä¸ªå€¼ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 8, 27, 64], dtype=int32) a[:6:2]=-1000 #ä¿®æ”¹ç¬¬é›¶ä¸ªã€ç¬¬äºŒä¸ªã€ç¬¬å…­ä¸ªå€¼ä¸º-1000 a array([ -1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729], dtype=int32) a[ : :-1] #å€’åºa array([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000], dtype=int32) for i in a: ... print(i**(1/3.)) ... nan 1.0 nan 3.0 nan 5.0 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 å¤šç»´æ•°ç»„å¤šç»´æ•°ç»„çš„æ¯ä¸€ä¸ªè½´éƒ½æœ‰ä¸€ä¸ªç´¢å¼•ï¼Œè¿™äº›ç´¢å¼•ä»¥é€—å·çš„å½¢å¼åˆ†éš”çš„å…ƒç»„ç»™å‡ºï¼š def f(x,y): ... return 5*x+y ... b=np.fromfunction(f,(5,4),dtype=int) b array([[ 0, 1, 2, 3], [ 5, 6, 7, 8], [10, 11, 12, 13], [15, 16, 17, 18], [20, 21, 22, 23]]) b[2,3] #ç¬¬äºŒè¡Œç¬¬ä¸‰åˆ—çš„æ•°å­— 13 b[0:5,1] #ç¬¬0~5è¡Œç¬¬1åˆ—çš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 1, 6, 11, 16, 21]) b[ : ,1] #ç¬¬1åˆ—çš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([ 1, 6, 11, 16, 21]) b[1:3,:] #ç¬¬1~3è¡Œçš„æ•°å­—ï¼Œä»¥æ•°ç»„å½¢å¼è¿”å› array([[ 5, 6, 7, 8], [10, 11, 12, 13]]) å¯¹å¤šç»´æ•°ç»„è¿›è¡Œè¿­ä»£ï¼ˆiteratingï¼‰æ˜¯ç›¸å¯¹äºç¬¬ä¸€ä¸ªè½´å®Œæˆçš„ã€‚ for row in b: ... print(row) ... [0 1 2 3] [5 6 7 8] [10 11 12 13] [15 16 17 18] [20 21 22 23] è¿­ä»£æ“ä½œå¦‚æœæƒ³è¦å¯¹æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œæ“ä½œï¼Œå¯ä»¥ä½¿ç”¨flatå±æ€§ï¼Œè¯¥å±æ€§æ˜¯æ•°ç»„çš„æ‰€æœ‰å…ƒç´ çš„è¿­ä»£å™¨ : for element in b.flat: ... print(element) ... 0 1 2 3 5 6 7 8 10 11 12 13 15 16 17 18 20 21 22 23 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:4","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#è¿­ä»£æ“ä½œ"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ \u003e\u003e\u003e a=np.floor(10*np.random.random((3,4))) \u003e\u003e\u003e a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) \u003e\u003e\u003e a.shape (3, 4) \u003e\u003e\u003e a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) \u003e\u003e\u003e a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ \u003e\u003e\u003e a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) \u003e\u003e\u003e a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ \u003e\u003e\u003e a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ \u003e\u003e\u003e a array([[0, 1], [2, 3], [4, 5]]) \u003e\u003e\u003e np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) \u003e\u003e\u003e a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ \u003e\u003e\u003e a=np.arange(12).reshape(3,4) \u003e\u003e\u003e a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) \u003e\u003e\u003e a.resize((2,6)) \u003e\u003e\u003e a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) \u003e\u003e\u003e a = np.array([[1, 2], [3, 4]]) \u003e\u003e\u003ea \u003e\u003e\u003earray([[1, 2], [3, 4]]) \u003e\u003e\u003e b = np.array([[5, 6]]) \u003e\u003e\u003e b array([[5, 6]]) \u003e\u003e\u003e np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) \u003e\u003e\u003e np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) \u003e\u003e\u003e np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ \u003e\u003e\u003e x = np.arange(16.0).reshape(4, 4) \u003e\u003e\u003e x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) \u003e\u003e\u003e np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ \u003e\u003e\u003e x = np.arange(16.0).reshape(4, 4) \u003e\u003e\u003e x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) \u003e\u003e\u003e np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ \u003e\u003e\u003e x = np.arange(8.0) \u003e\u003e\u003e np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayå½¢çŠ¶æ“ä½œ"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#æ”¹å˜æ•°ç»„çš„å½¢çŠ¶"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayravel"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayt"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayreshape"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayresize-"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#å †å æ•°ç»„"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npvstack"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nphstack"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npconcatenate"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#åˆ†å‰²æ•°ç»„"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#numpysplit"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nphsplit"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npvsplit"},{"categories":["documentation"],"content":"arrayå½¢çŠ¶æ“ä½œæ”¹å˜æ•°ç»„çš„å½¢çŠ¶array.ravel()åŒ–æˆ1*nçš„çŸ©é˜µã€‚ a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()ä½œç”¨ç­‰åŒäºarray.reshape(-1) array.Tè½¬ç½®çŸ©é˜µ ã€‚ a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()æ”¹å˜ä¸ºä»»æ„å½¢çŠ¶ ã€‚ a = np.arange(6).reshape((3, 2))#å°†1*6çŸ©é˜µè½¬ä¸º3*2çŸ©é˜µ a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #å°†3*2çŸ©é˜µè½¬ä¸º2*3çŸ©é˜µ array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshapeæ“ä½œä¸­å°†sizeæŒ‡å®šä¸º-1ï¼Œåˆ™ä¼šè‡ªåŠ¨è®¡ç®—å…¶ä»–çš„sizeå¤§å°ï¼š array([[0, 1, 2], [3, 4, 5]]) array.resize( )è¯¥æ–¹æ³•ä¼šç›´æ¥ä¿®æ”¹æ•°ç»„æœ¬èº«çš„shapeå’Œsizeã€‚ a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) å †å æ•°ç»„np.vstackå±äºä¸€ç§ä¸Šä¸‹åˆå¹¶çš„æƒ…å†µã€‚ import numpy as np #åˆå¹¶Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:å±äºä¸€ç§ä¸Šä¸‹åˆå¹¶ print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstackå±äºä¸€ç§å·¦å³åˆå¹¶çš„æƒ…å†µ import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #é’ˆå¯¹å¤šä¸ªçŸ©é˜µæˆ–åºåˆ—è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå€ŸåŠ© # np.concatenate((A,A,A,...),axis=0 æˆ– 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#åˆå¹¶åˆ— array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #åˆå¹¶è¡Œ array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) åˆ†å‰²æ•°ç»„numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #åˆ†å‰²å‡½æ•°np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#æŠŠå››åˆ—åˆ†æˆ2å—ï¼ˆ2åˆ—ä¸€å—ï¼‰ # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,è¡¨ç¤ºæŒ‰è¡Œåˆ†å‰²ï¼›axis=1,è¡¨ç¤ºæŒ‰åˆ—åˆ†å‰² print(np.split(A,3,axis=0)) #æŠŠä¸‰è¡ŒæŒ‰è¡Œåˆ†æˆ3å—ï¼ˆä¸€è¡Œä¸€å—ï¼‰ #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplitæŒ‰åˆ—æ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplitæŒ‰è¡Œæ‹†å¼€æ•°ç»„ã€‚ x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_splitå°†ä¸€ä¸ªæ•°ç»„æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰æˆ–è¿‘ä¼¼ç›¸ç­‰çš„å¤šä¸ªå­æ•°ç»„ã€‚å¦‚æœæ— æ³•è¿›è¡Œå‡ç­‰åˆ’åˆ†ï¼Œåˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚ x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nparray_split"},{"categories":["documentation"],"content":"æ‹·è´å’Œæ·±æ‹·è´å½“è®¡ç®—å’Œæ“ä½œæ•°ç»„æ—¶ï¼Œæœ‰æ—¶ä¼šå°†æ•°æ®å¤åˆ¶åˆ°æ–°æ•°ç»„ä¸­ï¼Œæœ‰æ—¶åˆ™ä¸ä¼š ã€‚ å­˜åœ¨ä»¥ä¸‹3ç§æƒ…å†µï¼š å®Œå…¨ä¸å¤åˆ¶ç®€å•åˆ†é…ä¸ä¼šå¤åˆ¶æ•°ç»„å¯¹è±¡æˆ–å…¶æ•°æ®ã€‚ import numpy as np a=np.arange(4) # =çš„èµ‹å€¼æ–¹å¼ä¼šå¸¦æœ‰å…³è”æ€§ b=a c=a d=b #æ”¹å˜açš„ç¬¬ä¸€ä¸ªå€¼ï¼Œbã€cã€dçš„ç¬¬ä¸€ä¸ªå€¼ä¹Ÿä¼šåŒæ—¶æ”¹å˜ã€‚ æµ…æ‹·è´ä¸åŒçš„æ•°ç»„å¯¹è±¡å¯ä»¥å…±äº«ç›¸åŒçš„æ•°æ®ã€‚viewæ–¹æ³•åˆ›å»ºä¸€ä¸ªæŸ¥çœ‹ç›¸åŒæ•°æ®çš„æ–°æ•°ç»„å¯¹è±¡ã€‚ \u003e\u003e\u003e import numpy as np \u003e\u003e\u003e a=np.arange(12).reshape(3,4) \u003e\u003e\u003e a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) \u003e\u003e\u003e c=a.view() \u003e\u003e\u003e c is a False \u003e\u003e\u003e c.base is a False \u003e\u003e\u003e c array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) \u003e\u003e\u003e c.shape = 2,6 \u003e\u003e\u003e c array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) \u003e\u003e\u003e a.shape (3, 4) \u003e\u003e\u003e c[0,4] = 1234 \u003e\u003e\u003e a array([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]]) \u003e\u003e\u003e c array([[ 0, 1, 2, 3, 1234, 5], [ 6, 7, 8, 9, 10, 11]]) æ·±æ‹·è´copy()è¯¥copyæ–¹æ³•ç”Ÿæˆæ•°ç»„åŠå…¶æ•°æ®çš„å®Œæ•´å‰¯æœ¬ã€‚ import numpy as np a=np.arange(4) #copy()çš„èµ‹å€¼æ–¹å¼æ²¡æœ‰å…³è”æ€§ b=a.copy() print(b) a[3]=45 print('a:',a) #a: [11 1 2 45] print('b:',b) #b: [11 1 2 3] @all right save,ZhangGehang. ","date":"2022-02-28","objectID":"/numpyguidebook/:1:6","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#æ‹·è´å’Œæ·±æ‹·è´"},{"categories":["documentation"],"content":"æ‹·è´å’Œæ·±æ‹·è´å½“è®¡ç®—å’Œæ“ä½œæ•°ç»„æ—¶ï¼Œæœ‰æ—¶ä¼šå°†æ•°æ®å¤åˆ¶åˆ°æ–°æ•°ç»„ä¸­ï¼Œæœ‰æ—¶åˆ™ä¸ä¼š ã€‚ å­˜åœ¨ä»¥ä¸‹3ç§æƒ…å†µï¼š å®Œå…¨ä¸å¤åˆ¶ç®€å•åˆ†é…ä¸ä¼šå¤åˆ¶æ•°ç»„å¯¹è±¡æˆ–å…¶æ•°æ®ã€‚ import numpy as np a=np.arange(4) # =çš„èµ‹å€¼æ–¹å¼ä¼šå¸¦æœ‰å…³è”æ€§ b=a c=a d=b #æ”¹å˜açš„ç¬¬ä¸€ä¸ªå€¼ï¼Œbã€cã€dçš„ç¬¬ä¸€ä¸ªå€¼ä¹Ÿä¼šåŒæ—¶æ”¹å˜ã€‚ æµ…æ‹·è´ä¸åŒçš„æ•°ç»„å¯¹è±¡å¯ä»¥å…±äº«ç›¸åŒçš„æ•°æ®ã€‚viewæ–¹æ³•åˆ›å»ºä¸€ä¸ªæŸ¥çœ‹ç›¸åŒæ•°æ®çš„æ–°æ•°ç»„å¯¹è±¡ã€‚ import numpy as np a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c=a.view() c is a False c.base is a False c array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c.shape = 2,6 c array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) a.shape (3, 4) c[0,4] = 1234 a array([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]]) c array([[ 0, 1, 2, 3, 1234, 5], [ 6, 7, 8, 9, 10, 11]]) æ·±æ‹·è´copy()è¯¥copyæ–¹æ³•ç”Ÿæˆæ•°ç»„åŠå…¶æ•°æ®çš„å®Œæ•´å‰¯æœ¬ã€‚ import numpy as np a=np.arange(4) #copy()çš„èµ‹å€¼æ–¹å¼æ²¡æœ‰å…³è”æ€§ b=a.copy() print(b) a[3]=45 print('a:',a) #a: [11 1 2 45] print('b:',b) #b: [11 1 2 3] @all right save,ZhangGehang. ","date":"2022-02-28","objectID":"/numpyguidebook/:1:6","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#å®Œå…¨ä¸å¤åˆ¶"},{"categories":["documentation"],"content":"æ‹·è´å’Œæ·±æ‹·è´å½“è®¡ç®—å’Œæ“ä½œæ•°ç»„æ—¶ï¼Œæœ‰æ—¶ä¼šå°†æ•°æ®å¤åˆ¶åˆ°æ–°æ•°ç»„ä¸­ï¼Œæœ‰æ—¶åˆ™ä¸ä¼š ã€‚ å­˜åœ¨ä»¥ä¸‹3ç§æƒ…å†µï¼š å®Œå…¨ä¸å¤åˆ¶ç®€å•åˆ†é…ä¸ä¼šå¤åˆ¶æ•°ç»„å¯¹è±¡æˆ–å…¶æ•°æ®ã€‚ import numpy as np a=np.arange(4) # =çš„èµ‹å€¼æ–¹å¼ä¼šå¸¦æœ‰å…³è”æ€§ b=a c=a d=b #æ”¹å˜açš„ç¬¬ä¸€ä¸ªå€¼ï¼Œbã€cã€dçš„ç¬¬ä¸€ä¸ªå€¼ä¹Ÿä¼šåŒæ—¶æ”¹å˜ã€‚ æµ…æ‹·è´ä¸åŒçš„æ•°ç»„å¯¹è±¡å¯ä»¥å…±äº«ç›¸åŒçš„æ•°æ®ã€‚viewæ–¹æ³•åˆ›å»ºä¸€ä¸ªæŸ¥çœ‹ç›¸åŒæ•°æ®çš„æ–°æ•°ç»„å¯¹è±¡ã€‚ import numpy as np a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c=a.view() c is a False c.base is a False c array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c.shape = 2,6 c array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) a.shape (3, 4) c[0,4] = 1234 a array([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]]) c array([[ 0, 1, 2, 3, 1234, 5], [ 6, 7, 8, 9, 10, 11]]) æ·±æ‹·è´copy()è¯¥copyæ–¹æ³•ç”Ÿæˆæ•°ç»„åŠå…¶æ•°æ®çš„å®Œæ•´å‰¯æœ¬ã€‚ import numpy as np a=np.arange(4) #copy()çš„èµ‹å€¼æ–¹å¼æ²¡æœ‰å…³è”æ€§ b=a.copy() print(b) a[3]=45 print('a:',a) #a: [11 1 2 45] print('b:',b) #b: [11 1 2 3] @all right save,ZhangGehang. ","date":"2022-02-28","objectID":"/numpyguidebook/:1:6","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#æµ…æ‹·è´"},{"categories":["documentation"],"content":"æ‹·è´å’Œæ·±æ‹·è´å½“è®¡ç®—å’Œæ“ä½œæ•°ç»„æ—¶ï¼Œæœ‰æ—¶ä¼šå°†æ•°æ®å¤åˆ¶åˆ°æ–°æ•°ç»„ä¸­ï¼Œæœ‰æ—¶åˆ™ä¸ä¼š ã€‚ å­˜åœ¨ä»¥ä¸‹3ç§æƒ…å†µï¼š å®Œå…¨ä¸å¤åˆ¶ç®€å•åˆ†é…ä¸ä¼šå¤åˆ¶æ•°ç»„å¯¹è±¡æˆ–å…¶æ•°æ®ã€‚ import numpy as np a=np.arange(4) # =çš„èµ‹å€¼æ–¹å¼ä¼šå¸¦æœ‰å…³è”æ€§ b=a c=a d=b #æ”¹å˜açš„ç¬¬ä¸€ä¸ªå€¼ï¼Œbã€cã€dçš„ç¬¬ä¸€ä¸ªå€¼ä¹Ÿä¼šåŒæ—¶æ”¹å˜ã€‚ æµ…æ‹·è´ä¸åŒçš„æ•°ç»„å¯¹è±¡å¯ä»¥å…±äº«ç›¸åŒçš„æ•°æ®ã€‚viewæ–¹æ³•åˆ›å»ºä¸€ä¸ªæŸ¥çœ‹ç›¸åŒæ•°æ®çš„æ–°æ•°ç»„å¯¹è±¡ã€‚ import numpy as np a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c=a.view() c is a False c.base is a False c array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c.shape = 2,6 c array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) a.shape (3, 4) c[0,4] = 1234 a array([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]]) c array([[ 0, 1, 2, 3, 1234, 5], [ 6, 7, 8, 9, 10, 11]]) æ·±æ‹·è´copy()è¯¥copyæ–¹æ³•ç”Ÿæˆæ•°ç»„åŠå…¶æ•°æ®çš„å®Œæ•´å‰¯æœ¬ã€‚ import numpy as np a=np.arange(4) #copy()çš„èµ‹å€¼æ–¹å¼æ²¡æœ‰å…³è”æ€§ b=a.copy() print(b) a[3]=45 print('a:',a) #a: [11 1 2 45] print('b:',b) #b: [11 1 2 3] @all right save,ZhangGehang. ","date":"2022-02-28","objectID":"/numpyguidebook/:1:6","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#æ·±æ‹·è´copy"}]