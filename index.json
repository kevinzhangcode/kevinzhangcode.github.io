[{"categories":["documentation"],"content":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","date":"2022-03-10","objectID":"/paper03/","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/"},{"categories":["documentation"],"content":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling 论文解读。 ","date":"2022-03-10","objectID":"/paper03/:0:0","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#"},{"categories":["documentation"],"content":"题目Cross-Domain NER using Cross-Domain Language Modeling [ACL 2019] [Code] ","date":"2022-03-10","objectID":"/paper03/:0:1","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#题目"},{"categories":["documentation"],"content":"摘要由于标签资源的限制，跨域命名实体识别（Cross-Domain NER）一直是一项具有挑战性的任务。大多数现有的工作都是在监督下进行的，即利用源域和目标域的标记数据。这类方法的一个缺点是，它们不能对没有NER数据的domain进行训练。为了解决这个问题，我们考虑使用跨域的语言模型(LMs)作为NER领域适应的桥梁，通过设计一个新的参数生成网络进行跨域和跨任务的知识转移。结果表明，我们的方法可以有效地从跨域LMs对比中提取域的差异，允许无监督的域适应，同时也给出了最先进的结果。 ","date":"2022-03-10","objectID":"/paper03/:0:2","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#摘要"},{"categories":["documentation"],"content":"模型模型的整体结构如图Fig-1所示。底部展示了两个领域和两个任务的组合。首先给定一个输入句子，通过一个共享的嵌入层计算单词表征，然后通过一个新的参数生成网络计算出一组特定任务和领域的BiLSTM参数，用于编码输入序列，最后不同的输出层被用于不同的任务和领域。 Fig-1.Model architecture Input Layer按照Yang等人（2018）的说法,给定一个输入$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$，来自以下4个数据集 源域NER训练集$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ 目标域NER训练集$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ 源域原始文本集$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 目标域原始文本集$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 每个词$x_i$被表示为其词嵌入和字符级CNN输出的连接： $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ 其中$\\mathbf{e}^w$代表一个共享的词嵌入查询表，$\\mathbf{e}^c$代表一个共享的字符嵌入查询表。$\\text{CNN}(\\cdot)$代表一个标准的$\\text{CNN}$，作用于一个词$x_i$的字符嵌入序列$\\mathbf{e}^c(x_i)$，$\\oplus$表示矢量连接。 Parameter Generation Network将$\\mathbf{v}$送入一个双向的LSTM层，为了实现跨领域和跨任务的知识转移，使用一个参数生成网络$f(\\cdot,\\cdot,\\cdot)$动态地生成$\\text{BiLSTM}$的参数，由此产生的参数被表示为$\\theta_{\\text{LSTM}}^{d,t}$，其中$d \\in {src,tgt}$，$t\\in {ner,lm}$ 分别代表领域标签和任务标签 $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ 参数解释： $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$表示输入词嵌入 $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$代表一组以三阶张量形式存在的元参数 $\\mathbf{I}_d^D\\in \\mathbb{R}^U$代表领域词嵌入 $\\mathbf{I}_d^D\\in \\mathbb{R}^V$代表任务词嵌入 $U$、 $V$分别代表领域和任务词嵌入的大小 $P^{(LSTM)}$是$\\text{BiLSTM}$参数的数量 $\\otimes$ 指张量收缩 给定输入$v$和参数$\\theta$，一个任务和特定领域$\\text{BiLSTM}$单元的隐藏输出可以统一写成: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$，$\\overleftarrow{\\mathbf{h}}_i^{d,t}$分别为前向和后向。 Output Layers标准CRFs被用作NER的输出层，在输入句子$\\mathbf{x}$上产生的标签序列$\\mathbf{y}=l_1,l_2,\\dots,l_i$的输出概率$p(\\mathbf{y}\\vert \\mathbf{x})$是 $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ 参数解释： $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$代表前向和后向的组合特征 $y’$代表一个任意的标签序列 $\\mathbf{w}^{li}_{CRF}$是$l_i$特有的模型参数 ${b_{CRF}^{(l_{i-1},l_i)}}$ 是 $l_{i-1}$ 和 $l_i$特有的偏置 考虑到不同领域的NER标签集可能不同，在Fig-1中分别用$\\text{CRF(S)}$和$\\text{CRF(T)}$来表示源域和目标域的$\\text{CRFs}$，使用一阶Viterbi算法来寻找高分的标签序列。 Language modeling前向$\\text{LM(LMf)}$ 使用前向LSTM隐藏状态$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$： 在给定$x_{1:i}$情况下来计算下一个词$x_{i+1}$的概率，表示为$p^f (x_{i+1}\\vert x_{1:i})$ 后向$\\text{LM(LMb)}$ 使用后向LSTM隐藏状态$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$： 在给定$x_{i:n}$情况下来计算上一个词$x_{i-1}$的概率，表示为$p^f (x_{i-1}\\vert x_{i:n})$ 考虑到计算效率，采用负采样Softmax（NSSoftmax）来计算前向和后向概率，具体如下： $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#模型"},{"categories":["documentation"],"content":"模型模型的整体结构如图Fig-1所示。底部展示了两个领域和两个任务的组合。首先给定一个输入句子，通过一个共享的嵌入层计算单词表征，然后通过一个新的参数生成网络计算出一组特定任务和领域的BiLSTM参数，用于编码输入序列，最后不同的输出层被用于不同的任务和领域。 Fig-1.Model architecture Input Layer按照Yang等人（2018）的说法,给定一个输入$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$，来自以下4个数据集 源域NER训练集$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ 目标域NER训练集$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ 源域原始文本集$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 目标域原始文本集$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 每个词$x_i$被表示为其词嵌入和字符级CNN输出的连接： $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ 其中$\\mathbf{e}^w$代表一个共享的词嵌入查询表，$\\mathbf{e}^c$代表一个共享的字符嵌入查询表。$\\text{CNN}(\\cdot)$代表一个标准的$\\text{CNN}$，作用于一个词$x_i$的字符嵌入序列$\\mathbf{e}^c(x_i)$，$\\oplus$表示矢量连接。 Parameter Generation Network将$\\mathbf{v}$送入一个双向的LSTM层，为了实现跨领域和跨任务的知识转移，使用一个参数生成网络$f(\\cdot,\\cdot,\\cdot)$动态地生成$\\text{BiLSTM}$的参数，由此产生的参数被表示为$\\theta_{\\text{LSTM}}^{d,t}$，其中$d \\in {src,tgt}$，$t\\in {ner,lm}$ 分别代表领域标签和任务标签 $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ 参数解释： $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$表示输入词嵌入 $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$代表一组以三阶张量形式存在的元参数 $\\mathbf{I}_d^D\\in \\mathbb{R}^U$代表领域词嵌入 $\\mathbf{I}_d^D\\in \\mathbb{R}^V$代表任务词嵌入 $U$、 $V$分别代表领域和任务词嵌入的大小 $P^{(LSTM)}$是$\\text{BiLSTM}$参数的数量 $\\otimes$ 指张量收缩 给定输入$v$和参数$\\theta$，一个任务和特定领域$\\text{BiLSTM}$单元的隐藏输出可以统一写成: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$，$\\overleftarrow{\\mathbf{h}}_i^{d,t}$分别为前向和后向。 Output Layers标准CRFs被用作NER的输出层，在输入句子$\\mathbf{x}$上产生的标签序列$\\mathbf{y}=l_1,l_2,\\dots,l_i$的输出概率$p(\\mathbf{y}\\vert \\mathbf{x})$是 $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ 参数解释： $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$代表前向和后向的组合特征 $y’$代表一个任意的标签序列 $\\mathbf{w}^{li}_{CRF}$是$l_i$特有的模型参数 ${b_{CRF}^{(l_{i-1},l_i)}}$ 是 $l_{i-1}$ 和 $l_i$特有的偏置 考虑到不同领域的NER标签集可能不同，在Fig-1中分别用$\\text{CRF(S)}$和$\\text{CRF(T)}$来表示源域和目标域的$\\text{CRFs}$，使用一阶Viterbi算法来寻找高分的标签序列。 Language modeling前向$\\text{LM(LMf)}$ 使用前向LSTM隐藏状态$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$： 在给定$x_{1:i}$情况下来计算下一个词$x_{i+1}$的概率，表示为$p^f (x_{i+1}\\vert x_{1:i})$ 后向$\\text{LM(LMb)}$ 使用后向LSTM隐藏状态$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$： 在给定$x_{i:n}$情况下来计算上一个词$x_{i-1}$的概率，表示为$p^f (x_{i-1}\\vert x_{i:n})$ 考虑到计算效率，采用负采样Softmax（NSSoftmax）来计算前向和后向概率，具体如下： $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#input-layer"},{"categories":["documentation"],"content":"模型模型的整体结构如图Fig-1所示。底部展示了两个领域和两个任务的组合。首先给定一个输入句子，通过一个共享的嵌入层计算单词表征，然后通过一个新的参数生成网络计算出一组特定任务和领域的BiLSTM参数，用于编码输入序列，最后不同的输出层被用于不同的任务和领域。 Fig-1.Model architecture Input Layer按照Yang等人（2018）的说法,给定一个输入$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$，来自以下4个数据集 源域NER训练集$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ 目标域NER训练集$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ 源域原始文本集$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 目标域原始文本集$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 每个词$x_i$被表示为其词嵌入和字符级CNN输出的连接： $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ 其中$\\mathbf{e}^w$代表一个共享的词嵌入查询表，$\\mathbf{e}^c$代表一个共享的字符嵌入查询表。$\\text{CNN}(\\cdot)$代表一个标准的$\\text{CNN}$，作用于一个词$x_i$的字符嵌入序列$\\mathbf{e}^c(x_i)$，$\\oplus$表示矢量连接。 Parameter Generation Network将$\\mathbf{v}$送入一个双向的LSTM层，为了实现跨领域和跨任务的知识转移，使用一个参数生成网络$f(\\cdot,\\cdot,\\cdot)$动态地生成$\\text{BiLSTM}$的参数，由此产生的参数被表示为$\\theta_{\\text{LSTM}}^{d,t}$，其中$d \\in {src,tgt}$，$t\\in {ner,lm}$ 分别代表领域标签和任务标签 $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ 参数解释： $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$表示输入词嵌入 $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$代表一组以三阶张量形式存在的元参数 $\\mathbf{I}_d^D\\in \\mathbb{R}^U$代表领域词嵌入 $\\mathbf{I}_d^D\\in \\mathbb{R}^V$代表任务词嵌入 $U$、 $V$分别代表领域和任务词嵌入的大小 $P^{(LSTM)}$是$\\text{BiLSTM}$参数的数量 $\\otimes$ 指张量收缩 给定输入$v$和参数$\\theta$，一个任务和特定领域$\\text{BiLSTM}$单元的隐藏输出可以统一写成: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$，$\\overleftarrow{\\mathbf{h}}_i^{d,t}$分别为前向和后向。 Output Layers标准CRFs被用作NER的输出层，在输入句子$\\mathbf{x}$上产生的标签序列$\\mathbf{y}=l_1,l_2,\\dots,l_i$的输出概率$p(\\mathbf{y}\\vert \\mathbf{x})$是 $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ 参数解释： $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$代表前向和后向的组合特征 $y’$代表一个任意的标签序列 $\\mathbf{w}^{li}_{CRF}$是$l_i$特有的模型参数 ${b_{CRF}^{(l_{i-1},l_i)}}$ 是 $l_{i-1}$ 和 $l_i$特有的偏置 考虑到不同领域的NER标签集可能不同，在Fig-1中分别用$\\text{CRF(S)}$和$\\text{CRF(T)}$来表示源域和目标域的$\\text{CRFs}$，使用一阶Viterbi算法来寻找高分的标签序列。 Language modeling前向$\\text{LM(LMf)}$ 使用前向LSTM隐藏状态$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$： 在给定$x_{1:i}$情况下来计算下一个词$x_{i+1}$的概率，表示为$p^f (x_{i+1}\\vert x_{1:i})$ 后向$\\text{LM(LMb)}$ 使用后向LSTM隐藏状态$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$： 在给定$x_{i:n}$情况下来计算上一个词$x_{i-1}$的概率，表示为$p^f (x_{i-1}\\vert x_{i:n})$ 考虑到计算效率，采用负采样Softmax（NSSoftmax）来计算前向和后向概率，具体如下： $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#parameter-generation-network"},{"categories":["documentation"],"content":"模型模型的整体结构如图Fig-1所示。底部展示了两个领域和两个任务的组合。首先给定一个输入句子，通过一个共享的嵌入层计算单词表征，然后通过一个新的参数生成网络计算出一组特定任务和领域的BiLSTM参数，用于编码输入序列，最后不同的输出层被用于不同的任务和领域。 Fig-1.Model architecture Input Layer按照Yang等人（2018）的说法,给定一个输入$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$，来自以下4个数据集 源域NER训练集$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ 目标域NER训练集$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ 源域原始文本集$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 目标域原始文本集$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 每个词$x_i$被表示为其词嵌入和字符级CNN输出的连接： $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ 其中$\\mathbf{e}^w$代表一个共享的词嵌入查询表，$\\mathbf{e}^c$代表一个共享的字符嵌入查询表。$\\text{CNN}(\\cdot)$代表一个标准的$\\text{CNN}$，作用于一个词$x_i$的字符嵌入序列$\\mathbf{e}^c(x_i)$，$\\oplus$表示矢量连接。 Parameter Generation Network将$\\mathbf{v}$送入一个双向的LSTM层，为了实现跨领域和跨任务的知识转移，使用一个参数生成网络$f(\\cdot,\\cdot,\\cdot)$动态地生成$\\text{BiLSTM}$的参数，由此产生的参数被表示为$\\theta_{\\text{LSTM}}^{d,t}$，其中$d \\in {src,tgt}$，$t\\in {ner,lm}$ 分别代表领域标签和任务标签 $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ 参数解释： $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$表示输入词嵌入 $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$代表一组以三阶张量形式存在的元参数 $\\mathbf{I}_d^D\\in \\mathbb{R}^U$代表领域词嵌入 $\\mathbf{I}_d^D\\in \\mathbb{R}^V$代表任务词嵌入 $U$、 $V$分别代表领域和任务词嵌入的大小 $P^{(LSTM)}$是$\\text{BiLSTM}$参数的数量 $\\otimes$ 指张量收缩 给定输入$v$和参数$\\theta$，一个任务和特定领域$\\text{BiLSTM}$单元的隐藏输出可以统一写成: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$，$\\overleftarrow{\\mathbf{h}}_i^{d,t}$分别为前向和后向。 Output Layers标准CRFs被用作NER的输出层，在输入句子$\\mathbf{x}$上产生的标签序列$\\mathbf{y}=l_1,l_2,\\dots,l_i$的输出概率$p(\\mathbf{y}\\vert \\mathbf{x})$是 $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ 参数解释： $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$代表前向和后向的组合特征 $y’$代表一个任意的标签序列 $\\mathbf{w}^{li}_{CRF}$是$l_i$特有的模型参数 ${b_{CRF}^{(l_{i-1},l_i)}}$ 是 $l_{i-1}$ 和 $l_i$特有的偏置 考虑到不同领域的NER标签集可能不同，在Fig-1中分别用$\\text{CRF(S)}$和$\\text{CRF(T)}$来表示源域和目标域的$\\text{CRFs}$，使用一阶Viterbi算法来寻找高分的标签序列。 Language modeling前向$\\text{LM(LMf)}$ 使用前向LSTM隐藏状态$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$： 在给定$x_{1:i}$情况下来计算下一个词$x_{i+1}$的概率，表示为$p^f (x_{i+1}\\vert x_{1:i})$ 后向$\\text{LM(LMb)}$ 使用后向LSTM隐藏状态$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$： 在给定$x_{i:n}$情况下来计算上一个词$x_{i-1}$的概率，表示为$p^f (x_{i-1}\\vert x_{i:n})$ 考虑到计算效率，采用负采样Softmax（NSSoftmax）来计算前向和后向概率，具体如下： $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#output-layers"},{"categories":["documentation"],"content":"模型模型的整体结构如图Fig-1所示。底部展示了两个领域和两个任务的组合。首先给定一个输入句子，通过一个共享的嵌入层计算单词表征，然后通过一个新的参数生成网络计算出一组特定任务和领域的BiLSTM参数，用于编码输入序列，最后不同的输出层被用于不同的任务和领域。 Fig-1.Model architecture Input Layer按照Yang等人（2018）的说法,给定一个输入$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$，来自以下4个数据集 源域NER训练集$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ 目标域NER训练集$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ 源域原始文本集$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 目标域原始文本集$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 每个词$x_i$被表示为其词嵌入和字符级CNN输出的连接： $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ 其中$\\mathbf{e}^w$代表一个共享的词嵌入查询表，$\\mathbf{e}^c$代表一个共享的字符嵌入查询表。$\\text{CNN}(\\cdot)$代表一个标准的$\\text{CNN}$，作用于一个词$x_i$的字符嵌入序列$\\mathbf{e}^c(x_i)$，$\\oplus$表示矢量连接。 Parameter Generation Network将$\\mathbf{v}$送入一个双向的LSTM层，为了实现跨领域和跨任务的知识转移，使用一个参数生成网络$f(\\cdot,\\cdot,\\cdot)$动态地生成$\\text{BiLSTM}$的参数，由此产生的参数被表示为$\\theta_{\\text{LSTM}}^{d,t}$，其中$d \\in {src,tgt}$，$t\\in {ner,lm}$ 分别代表领域标签和任务标签 $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ 参数解释： $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$表示输入词嵌入 $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$代表一组以三阶张量形式存在的元参数 $\\mathbf{I}_d^D\\in \\mathbb{R}^U$代表领域词嵌入 $\\mathbf{I}_d^D\\in \\mathbb{R}^V$代表任务词嵌入 $U$、 $V$分别代表领域和任务词嵌入的大小 $P^{(LSTM)}$是$\\text{BiLSTM}$参数的数量 $\\otimes$ 指张量收缩 给定输入$v$和参数$\\theta$，一个任务和特定领域$\\text{BiLSTM}$单元的隐藏输出可以统一写成: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$，$\\overleftarrow{\\mathbf{h}}_i^{d,t}$分别为前向和后向。 Output Layers标准CRFs被用作NER的输出层，在输入句子$\\mathbf{x}$上产生的标签序列$\\mathbf{y}=l_1,l_2,\\dots,l_i$的输出概率$p(\\mathbf{y}\\vert \\mathbf{x})$是 $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ 参数解释： $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$代表前向和后向的组合特征 $y’$代表一个任意的标签序列 $\\mathbf{w}^{li}_{CRF}$是$l_i$特有的模型参数 ${b_{CRF}^{(l_{i-1},l_i)}}$ 是 $l_{i-1}$ 和 $l_i$特有的偏置 考虑到不同领域的NER标签集可能不同，在Fig-1中分别用$\\text{CRF(S)}$和$\\text{CRF(T)}$来表示源域和目标域的$\\text{CRFs}$，使用一阶Viterbi算法来寻找高分的标签序列。 Language modeling前向$\\text{LM(LMf)}$ 使用前向LSTM隐藏状态$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$： 在给定$x_{1:i}$情况下来计算下一个词$x_{i+1}$的概率，表示为$p^f (x_{i+1}\\vert x_{1:i})$ 后向$\\text{LM(LMb)}$ 使用后向LSTM隐藏状态$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$： 在给定$x_{i:n}$情况下来计算上一个词$x_{i-1}$的概率，表示为$p^f (x_{i-1}\\vert x_{i:n})$ 考虑到计算效率，采用负采样Softmax（NSSoftmax）来计算前向和后向概率，具体如下： $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#language-modeling"},{"categories":["documentation"],"content":"模型模型的整体结构如图Fig-1所示。底部展示了两个领域和两个任务的组合。首先给定一个输入句子，通过一个共享的嵌入层计算单词表征，然后通过一个新的参数生成网络计算出一组特定任务和领域的BiLSTM参数，用于编码输入序列，最后不同的输出层被用于不同的任务和领域。 Fig-1.Model architecture Input Layer按照Yang等人（2018）的说法,给定一个输入$\\mathbf{x}=[x_1,x_2,\\cdots,x_n]$，来自以下4个数据集 源域NER训练集$S_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^m$ 目标域NER训练集$T_{ner}=\\{\\{x_i,y_i\\}\\}_{i=1}^n$ 源域原始文本集$S_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 目标域原始文本集$T_{lm}=\\{\\{x_i\\}\\}_{i=1}^p$ 每个词$x_i$被表示为其词嵌入和字符级CNN输出的连接： $$ \\mathbf{v}_i =[\\mathbf{e}^w(x_i)\\oplus \\text{CNN}(\\mathbf{e}^c(x_i))] $$ 其中$\\mathbf{e}^w$代表一个共享的词嵌入查询表，$\\mathbf{e}^c$代表一个共享的字符嵌入查询表。$\\text{CNN}(\\cdot)$代表一个标准的$\\text{CNN}$，作用于一个词$x_i$的字符嵌入序列$\\mathbf{e}^c(x_i)$，$\\oplus$表示矢量连接。 Parameter Generation Network将$\\mathbf{v}$送入一个双向的LSTM层，为了实现跨领域和跨任务的知识转移，使用一个参数生成网络$f(\\cdot,\\cdot,\\cdot)$动态地生成$\\text{BiLSTM}$的参数，由此产生的参数被表示为$\\theta_{\\text{LSTM}}^{d,t}$，其中$d \\in {src,tgt}$，$t\\in {ner,lm}$ 分别代表领域标签和任务标签 $$ \\theta_{\\text{LSTM}}^{d,t} = \\mathbf{W} \\otimes \\mathbf{I}_d^D \\otimes \\mathbf{I}_t^T $$ 参数解释： $\\mathbf{v}=[{\\mathbf{v}_1},{\\mathbf{v}_2},\\dots,{\\mathbf{v}_n}]$表示输入词嵌入 $ \\mathbf{W}\\in \\mathbb{R}^{P^{(LSTM)}\\times V \\times U}$代表一组以三阶张量形式存在的元参数 $\\mathbf{I}_d^D\\in \\mathbb{R}^U$代表领域词嵌入 $\\mathbf{I}_d^D\\in \\mathbb{R}^V$代表任务词嵌入 $U$、 $V$分别代表领域和任务词嵌入的大小 $P^{(LSTM)}$是$\\text{BiLSTM}$参数的数量 $\\otimes$ 指张量收缩 给定输入$v$和参数$\\theta$，一个任务和特定领域$\\text{BiLSTM}$单元的隐藏输出可以统一写成: $$\\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned}$$ \\begin{aligned} \\overrightarrow{\\mathbf{h}}_i^{d,t}=\\text{LSTM}(\\overrightarrow{\\mathbf{h}}_{i-1}^{d,t},\\mathbf{v}_i,\\overrightarrow{\\theta}_{\\text{LSTM}}^{d,t})\\\\ \\overleftarrow{\\mathbf{h}}_{i}^{d,t}=\\text{LSTM}({\\overleftarrow{\\mathbf{h}}}_{i-1}^{d,t},\\mathbf{v}_i,\\overleftarrow{\\theta}_{\\text{LSTM}}^{d,t}) \\end{aligned} $\\overrightarrow{\\mathbf{h}}_i^{d,t}$，$\\overleftarrow{\\mathbf{h}}_i^{d,t}$分别为前向和后向。 Output Layers标准CRFs被用作NER的输出层，在输入句子$\\mathbf{x}$上产生的标签序列$\\mathbf{y}=l_1,l_2,\\dots,l_i$的输出概率$p(\\mathbf{y}\\vert \\mathbf{x})$是 $$ p(\\boldsymbol{y} \\mid \\boldsymbol{x})=\\frac{\\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp \\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot \\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ $$p(\\boldsymbol{y} \\mid\\boldsymbol{x})=\\frac{\\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{\\mathrm{CRF}}^{l_{i}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}, l_{i}\\right)}\\right)\\right\\}}{\\sum_{\\boldsymbol{y}^{\\prime}} \\exp\\left\\{\\sum_{i}\\left(\\mathbf{w}_{i}^{l_{\\mathrm{CRF}}^{\\prime}} \\cdot\\mathbf{h}_{i}+b_{\\mathrm{CRF}}^{\\left(l_{i-1}^{\\prime}, l_{i}^{\\prime}\\right)}\\right)\\right\\}} $$ 参数解释： $\\mathbf{h}=[\\overrightarrow{\\mathbf{h}}_1 \\otimes \\overleftarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n \\otimes \\overleftarrow{\\mathbf{h}}_n]$代表前向和后向的组合特征 $y’$代表一个任意的标签序列 $\\mathbf{w}^{li}_{CRF}$是$l_i$特有的模型参数 ${b_{CRF}^{(l_{i-1},l_i)}}$ 是 $l_{i-1}$ 和 $l_i$特有的偏置 考虑到不同领域的NER标签集可能不同，在Fig-1中分别用$\\text{CRF(S)}$和$\\text{CRF(T)}$来表示源域和目标域的$\\text{CRFs}$，使用一阶Viterbi算法来寻找高分的标签序列。 Language modeling前向$\\text{LM(LMf)}$ 使用前向LSTM隐藏状态$\\overrightarrow{\\mathbf{h}}=[\\overrightarrow{\\mathbf{h}}_1,\\dots,\\overrightarrow{\\mathbf{h}}_n]$： 在给定$x_{1:i}$情况下来计算下一个词$x_{i+1}$的概率，表示为$p^f (x_{i+1}\\vert x_{1:i})$ 后向$\\text{LM(LMb)}$ 使用后向LSTM隐藏状态$\\overleftarrow{\\mathbf{h}}=[\\overleftarrow{\\mathbf{h}}_1,\\dots,\\overleftarrow{\\mathbf{h}}_n]$： 在给定$x_{i:n}$情况下来计算上一个词$x_{i-1}$的概率，表示为$p^f (x_{i-1}\\vert x_{i:n})$ 考虑到计算效率，采用负采样Softmax（NSSoftmax）来计算前向和后向概率，具体如下： $$\\begin{aligned} \u0026p^{f}\\left(x_{i+1} \\midx_{1: i}\\right)=\\frac{1}{Z} \\exp\\left\\{\\mathbf{w}_{\\#x_{i+1}}^{\\top} \\overrightarrow{\\mathbf{h}}_{i}+b_{\\#x_{i+1}}\\right\\}\\\\\u0026p^{b}\\left(x_{i-","date":"2022-03-10","objectID":"/paper03/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#multi-task-learning-algorithm"},{"categories":["documentation"],"content":"实验结果与讨论作者在三个跨领域数据集上进行了实验，在有监督的领域适应和无监督的领域适应设置下，将提出的方法与一系列转移学习基线进行比较。 ","date":"2022-03-10","objectID":"/paper03/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#实验结果与讨论"},{"categories":["documentation"],"content":"结论通过从原始文本中提取领域差异的知识来进行NER领域适应。为了实现这一目标，作者通过一个新的参数生成网络进行跨领域语言建模，该网络将领域和任务知识分解为两组嵌入向量。在三个数据集上的实验表明，方法在有监督的领域适应方法中是非常有效的，同时允许在无监督的领域适应中进行zero-shot学习。 ","date":"2022-03-10","objectID":"/paper03/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#结论"},{"categories":["documentation"],"content":"代码https://github.com/jiachenwestlake/Cross-Domain_NER ","date":"2022-03-10","objectID":"/paper03/:0:6","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2019-Cross-Domain NER using Cross-Domain Language Modeling","uri":"/paper03/#代码"},{"categories":["documentation"],"content":"Transformers Domain Adaptation","date":"2022-03-04","objectID":"/tools01/","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/"},{"categories":["documentation"],"content":"Transformers Domain Adaptation本指南说明了端到端Domain Adaptation工作流程，其中我们为生物医学NLP应用程序适应领域转换模型。 它展示了我们在研究中研究的两种领域自适应技术: 数据选择 (Data Selection) 词汇量增加 (Vocabulary Augmentation) 接下来，我们将演示这样一个Domain Adaptation的Transformer模型是如何与🤗transformer的训练流程兼容的，以及它如何优于开箱即用的(无Domain Adaptation的)模型,这些技术应用于BERT-small，但是代码库被编写成可推广到HuggingFace支持的其他Transformer类。 警告对于本指南，由于内存和时间的限制，我们使用域内语料库的一个小得多的子集(\u003c0.05%)。 ","date":"2022-03-04","objectID":"/tools01/:1:0","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#transformers-domain-adaptation"},{"categories":["documentation"],"content":"Transformers Domain Adaptation本指南说明了端到端Domain Adaptation工作流程，其中我们为生物医学NLP应用程序适应领域转换模型。 它展示了我们在研究中研究的两种领域自适应技术: 数据选择 (Data Selection) 词汇量增加 (Vocabulary Augmentation) 接下来，我们将演示这样一个Domain Adaptation的Transformer模型是如何与🤗transformer的训练流程兼容的，以及它如何优于开箱即用的(无Domain Adaptation的)模型,这些技术应用于BERT-small，但是代码库被编写成可推广到HuggingFace支持的其他Transformer类。 警告对于本指南，由于内存和时间的限制，我们使用域内语料库的一个小得多的子集(","date":"2022-03-04","objectID":"/tools01/:1:0","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#警告"},{"categories":["documentation"],"content":"准备工作安装依赖程序使用pip安装transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple 下载demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # 解压文件 unzip ./files.zip 常量我们首先定义一些常量，包括适当的模型卡和文本语料库的相关路径。 在domain adaptation的背景下，有两种类型的语料库。 微调语料库(Fine-Tuning Corpus) 给定一个NLP任务（如文本分类、摘要等），这个数据集的文本部分就是微调语料库。 在域语料库 (In-Domain Corpus) 这是一个无监督的文本数据集，用于领域预训练。文本领域与微调语料库的领域相同，甚至更广泛。 # 预训练模型名称 model_card = 'bert-base-uncased' # Domain-pre-training corpora 领域预训练语料 dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' 加载模型和tokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) 数据选择在领域预训练中，并不是所有的领域内语料库的数据都可能是有帮助的或相关的。对于不相关的文件，在最好的情况下，它不会降低领域适应模型的性能；在最坏的情况下，模型会倒退并失去宝贵的预训练信息即灾难性的遗忘。 因此，我们使用Ruder \u0026 Plank设计的各种相似性和多样性指标，从域内语料库中选择可能与下游微调数据集相关的文档。 from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # 将文本数据加载到内存中 fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # 在微调语料库进行fit selector.fit(fine_tuning_texts) # 从域内训练语料库中选择相关文件 selected_corpus = selector.transform(training_texts) # 在`dpt_corpus_train_data_selected`下将选定的语料库保存到磁盘 Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); 由于我们在DataSelector中指定了keep=0.5，所以选择的语料库应该是域内语料库的一半大小，包含前50%最相关的文档。 print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P\u003c0. 05). The rate of Chla and Chlb decreased but the content of Chla,Chlb and Chl( a+b) increased gradually with continued shading. The maximum value of photosynthetically active radiation appeared at 10: 00-12: 00 am in a day. The maximum value of net photosynthetic rate appeared at 8: 30-9: 00 am and the minimum value appeared at 14: 00-14: 30 pm at each phenological stage if plants grew in full sunlight. However,when plants grew in shading,the maximum value of net photosynthetic rate appeared at about 10: 30 am and the minimum value appeared at 12: 20-12: 50 pm at each phenological stage. At expanding leaf stage and flowering stage,the average of net photosynthetic rate of leaves in full sunlight was remarkably higher than those in shading and it decreased greatly with decreasing of irradiance gradually( P \u003c 0. 05). However,at fruiting stage,the average of net photosynthetic rate of leaves in full sunlight was lower than those","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#准备工作"},{"categories":["documentation"],"content":"准备工作安装依赖程序使用pip安装transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple 下载demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # 解压文件 unzip ./files.zip 常量我们首先定义一些常量，包括适当的模型卡和文本语料库的相关路径。 在domain adaptation的背景下，有两种类型的语料库。 微调语料库(Fine-Tuning Corpus) 给定一个NLP任务（如文本分类、摘要等），这个数据集的文本部分就是微调语料库。 在域语料库 (In-Domain Corpus) 这是一个无监督的文本数据集，用于领域预训练。文本领域与微调语料库的领域相同，甚至更广泛。 # 预训练模型名称 model_card = 'bert-base-uncased' # Domain-pre-training corpora 领域预训练语料 dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' 加载模型和tokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) 数据选择在领域预训练中，并不是所有的领域内语料库的数据都可能是有帮助的或相关的。对于不相关的文件，在最好的情况下，它不会降低领域适应模型的性能；在最坏的情况下，模型会倒退并失去宝贵的预训练信息即灾难性的遗忘。 因此，我们使用Ruder \u0026 Plank设计的各种相似性和多样性指标，从域内语料库中选择可能与下游微调数据集相关的文档。 from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # 将文本数据加载到内存中 fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # 在微调语料库进行fit selector.fit(fine_tuning_texts) # 从域内训练语料库中选择相关文件 selected_corpus = selector.transform(training_texts) # 在`dpt_corpus_train_data_selected`下将选定的语料库保存到磁盘 Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); 由于我们在DataSelector中指定了keep=0.5，所以选择的语料库应该是域内语料库的一半大小，包含前50%最相关的文档。 print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#安装依赖程序"},{"categories":["documentation"],"content":"准备工作安装依赖程序使用pip安装transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple 下载demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # 解压文件 unzip ./files.zip 常量我们首先定义一些常量，包括适当的模型卡和文本语料库的相关路径。 在domain adaptation的背景下，有两种类型的语料库。 微调语料库(Fine-Tuning Corpus) 给定一个NLP任务（如文本分类、摘要等），这个数据集的文本部分就是微调语料库。 在域语料库 (In-Domain Corpus) 这是一个无监督的文本数据集，用于领域预训练。文本领域与微调语料库的领域相同，甚至更广泛。 # 预训练模型名称 model_card = 'bert-base-uncased' # Domain-pre-training corpora 领域预训练语料 dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' 加载模型和tokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) 数据选择在领域预训练中，并不是所有的领域内语料库的数据都可能是有帮助的或相关的。对于不相关的文件，在最好的情况下，它不会降低领域适应模型的性能；在最坏的情况下，模型会倒退并失去宝贵的预训练信息即灾难性的遗忘。 因此，我们使用Ruder \u0026 Plank设计的各种相似性和多样性指标，从域内语料库中选择可能与下游微调数据集相关的文档。 from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # 将文本数据加载到内存中 fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # 在微调语料库进行fit selector.fit(fine_tuning_texts) # 从域内训练语料库中选择相关文件 selected_corpus = selector.transform(training_texts) # 在`dpt_corpus_train_data_selected`下将选定的语料库保存到磁盘 Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); 由于我们在DataSelector中指定了keep=0.5，所以选择的语料库应该是域内语料库的一半大小，包含前50%最相关的文档。 print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#下载demo-files"},{"categories":["documentation"],"content":"准备工作安装依赖程序使用pip安装transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple 下载demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # 解压文件 unzip ./files.zip 常量我们首先定义一些常量，包括适当的模型卡和文本语料库的相关路径。 在domain adaptation的背景下，有两种类型的语料库。 微调语料库(Fine-Tuning Corpus) 给定一个NLP任务（如文本分类、摘要等），这个数据集的文本部分就是微调语料库。 在域语料库 (In-Domain Corpus) 这是一个无监督的文本数据集，用于领域预训练。文本领域与微调语料库的领域相同，甚至更广泛。 # 预训练模型名称 model_card = 'bert-base-uncased' # Domain-pre-training corpora 领域预训练语料 dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' 加载模型和tokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) 数据选择在领域预训练中，并不是所有的领域内语料库的数据都可能是有帮助的或相关的。对于不相关的文件，在最好的情况下，它不会降低领域适应模型的性能；在最坏的情况下，模型会倒退并失去宝贵的预训练信息即灾难性的遗忘。 因此，我们使用Ruder \u0026 Plank设计的各种相似性和多样性指标，从域内语料库中选择可能与下游微调数据集相关的文档。 from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # 将文本数据加载到内存中 fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # 在微调语料库进行fit selector.fit(fine_tuning_texts) # 从域内训练语料库中选择相关文件 selected_corpus = selector.transform(training_texts) # 在`dpt_corpus_train_data_selected`下将选定的语料库保存到磁盘 Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); 由于我们在DataSelector中指定了keep=0.5，所以选择的语料库应该是域内语料库的一半大小，包含前50%最相关的文档。 print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#常量"},{"categories":["documentation"],"content":"准备工作安装依赖程序使用pip安装transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple 下载demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # 解压文件 unzip ./files.zip 常量我们首先定义一些常量，包括适当的模型卡和文本语料库的相关路径。 在domain adaptation的背景下，有两种类型的语料库。 微调语料库(Fine-Tuning Corpus) 给定一个NLP任务（如文本分类、摘要等），这个数据集的文本部分就是微调语料库。 在域语料库 (In-Domain Corpus) 这是一个无监督的文本数据集，用于领域预训练。文本领域与微调语料库的领域相同，甚至更广泛。 # 预训练模型名称 model_card = 'bert-base-uncased' # Domain-pre-training corpora 领域预训练语料 dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' 加载模型和tokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) 数据选择在领域预训练中，并不是所有的领域内语料库的数据都可能是有帮助的或相关的。对于不相关的文件，在最好的情况下，它不会降低领域适应模型的性能；在最坏的情况下，模型会倒退并失去宝贵的预训练信息即灾难性的遗忘。 因此，我们使用Ruder \u0026 Plank设计的各种相似性和多样性指标，从域内语料库中选择可能与下游微调数据集相关的文档。 from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # 将文本数据加载到内存中 fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # 在微调语料库进行fit selector.fit(fine_tuning_texts) # 从域内训练语料库中选择相关文件 selected_corpus = selector.transform(training_texts) # 在`dpt_corpus_train_data_selected`下将选定的语料库保存到磁盘 Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); 由于我们在DataSelector中指定了keep=0.5，所以选择的语料库应该是域内语料库的一半大小，包含前50%最相关的文档。 print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#加载模型和tokenizer"},{"categories":["documentation"],"content":"准备工作安装依赖程序使用pip安装transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple 下载demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # 解压文件 unzip ./files.zip 常量我们首先定义一些常量，包括适当的模型卡和文本语料库的相关路径。 在domain adaptation的背景下，有两种类型的语料库。 微调语料库(Fine-Tuning Corpus) 给定一个NLP任务（如文本分类、摘要等），这个数据集的文本部分就是微调语料库。 在域语料库 (In-Domain Corpus) 这是一个无监督的文本数据集，用于领域预训练。文本领域与微调语料库的领域相同，甚至更广泛。 # 预训练模型名称 model_card = 'bert-base-uncased' # Domain-pre-training corpora 领域预训练语料 dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' 加载模型和tokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) 数据选择在领域预训练中，并不是所有的领域内语料库的数据都可能是有帮助的或相关的。对于不相关的文件，在最好的情况下，它不会降低领域适应模型的性能；在最坏的情况下，模型会倒退并失去宝贵的预训练信息即灾难性的遗忘。 因此，我们使用Ruder \u0026 Plank设计的各种相似性和多样性指标，从域内语料库中选择可能与下游微调数据集相关的文档。 from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # 将文本数据加载到内存中 fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # 在微调语料库进行fit selector.fit(fine_tuning_texts) # 从域内训练语料库中选择相关文件 selected_corpus = selector.transform(training_texts) # 在`dpt_corpus_train_data_selected`下将选定的语料库保存到磁盘 Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); 由于我们在DataSelector中指定了keep=0.5，所以选择的语料库应该是域内语料库的一半大小，包含前50%最相关的文档。 print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#数据选择"},{"categories":["documentation"],"content":"准备工作安装依赖程序使用pip安装transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple 下载demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # 解压文件 unzip ./files.zip 常量我们首先定义一些常量，包括适当的模型卡和文本语料库的相关路径。 在domain adaptation的背景下，有两种类型的语料库。 微调语料库(Fine-Tuning Corpus) 给定一个NLP任务（如文本分类、摘要等），这个数据集的文本部分就是微调语料库。 在域语料库 (In-Domain Corpus) 这是一个无监督的文本数据集，用于领域预训练。文本领域与微调语料库的领域相同，甚至更广泛。 # 预训练模型名称 model_card = 'bert-base-uncased' # Domain-pre-training corpora 领域预训练语料 dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' 加载模型和tokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) 数据选择在领域预训练中，并不是所有的领域内语料库的数据都可能是有帮助的或相关的。对于不相关的文件，在最好的情况下，它不会降低领域适应模型的性能；在最坏的情况下，模型会倒退并失去宝贵的预训练信息即灾难性的遗忘。 因此，我们使用Ruder \u0026 Plank设计的各种相似性和多样性指标，从域内语料库中选择可能与下游微调数据集相关的文档。 from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # 将文本数据加载到内存中 fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # 在微调语料库进行fit selector.fit(fine_tuning_texts) # 从域内训练语料库中选择相关文件 selected_corpus = selector.transform(training_texts) # 在`dpt_corpus_train_data_selected`下将选定的语料库保存到磁盘 Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); 由于我们在DataSelector中指定了keep=0.5，所以选择的语料库应该是域内语料库的一半大小，包含前50%最相关的文档。 print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#词汇扩充"},{"categories":["documentation"],"content":"准备工作安装依赖程序使用pip安装transformers-domain-adaptation pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple 下载demo files wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip # 解压文件 unzip ./files.zip 常量我们首先定义一些常量，包括适当的模型卡和文本语料库的相关路径。 在domain adaptation的背景下，有两种类型的语料库。 微调语料库(Fine-Tuning Corpus) 给定一个NLP任务（如文本分类、摘要等），这个数据集的文本部分就是微调语料库。 在域语料库 (In-Domain Corpus) 这是一个无监督的文本数据集，用于领域预训练。文本领域与微调语料库的领域相同，甚至更广泛。 # 预训练模型名称 model_card = 'bert-base-uncased' # Domain-pre-training corpora 领域预训练语料 dpt_corpus_train = './data/pubmed_subset_train.txt' dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt' dpt_corpus_val = './data/pubmed_subset_val.txt' # Fine-tuning corpora # If there are multiple downstream NLP tasks/corpora, you can concatenate those files together ft_corpus_train = './data/BC2GM_train.txt' 加载模型和tokenizer from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained(model_card) tokenizer = AutoTokenizer.from_pretrained(model_card) 数据选择在领域预训练中，并不是所有的领域内语料库的数据都可能是有帮助的或相关的。对于不相关的文件，在最好的情况下，它不会降低领域适应模型的性能；在最坏的情况下，模型会倒退并失去宝贵的预训练信息即灾难性的遗忘。 因此，我们使用Ruder \u0026 Plank设计的各种相似性和多样性指标，从域内语料库中选择可能与下游微调数据集相关的文档。 from pathlib import Path from transformers_domain_adaptation import DataSelector selector = DataSelector( keep=0.5, # TODO Replace with `keep` tokenizer=tokenizer, similarity_metrics=['euclidean'], diversity_metrics=[ \"type_token_ratio\", \"entropy\", ], ) # 将文本数据加载到内存中 fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines() training_texts = Path(dpt_corpus_train).read_text().splitlines() # 在微调语料库进行fit selector.fit(fine_tuning_texts) # 从域内训练语料库中选择相关文件 selected_corpus = selector.transform(training_texts) # 在`dpt_corpus_train_data_selected`下将选定的语料库保存到磁盘 Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus)); 由于我们在DataSelector中指定了keep=0.5，所以选择的语料库应该是域内语料库的一半大小，包含前50%最相关的文档。 print(len(training_texts), len(selected_corpus)) # (10000, 5000) print(selected_corpus[0]) Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P","date":"2022-03-04","objectID":"/tools01/:1:1","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#用新的词汇术语更新模型和tokenizer"},{"categories":["documentation"],"content":"Domain Pre-TrainingDomain PreTraining是Domain Adaptation的第三步，我们在领域内语料库上用同样的预训练程序继续训练Transformer模型。 创建数据集 import itertools as it from pathlib import Path from typing import Sequence, Union, Generator from datasets import load_dataset from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments datasets = load_dataset( 'text', data_files={ \"train\": dpt_corpus_train_data_selected, \"val\": dpt_corpus_val } ) tokenized_datasets = datasets.map( lambda examples: tokenizer(examples['text'], truncation=True, max_length=model.config.max_position_embeddings), batched=True ) data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm=True, mlm_probability=0.15 ) 实例化TrainingArguments和Trainer training_args = TrainingArguments( output_dir=\"./results/domain_pre_training\", overwrite_output_dir=True, max_steps=100, per_device_train_batch_size=8, per_device_eval_batch_size=16, evaluation_strategy=\"steps\", save_steps=50, save_total_limit=2, logging_steps=50, seed=42, # fp16=True, dataloader_num_workers=2, disable_tqdm=False ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'], data_collator=data_collator, tokenizer=tokenizer, # 这个标记器有新的tokens ) # 进行训练 trainer.train() 训练结果 Step Training Loss Validation Loss Runtime Samples Per Second 50 2.813800 2.409768 75.058500 13.323000 100 2.520700 2.342451 74.257200 13.467000 ","date":"2022-03-04","objectID":"/tools01/:1:2","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#domain-pre-training"},{"categories":["documentation"],"content":"Domain Pre-TrainingDomain PreTraining是Domain Adaptation的第三步，我们在领域内语料库上用同样的预训练程序继续训练Transformer模型。 创建数据集 import itertools as it from pathlib import Path from typing import Sequence, Union, Generator from datasets import load_dataset from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments datasets = load_dataset( 'text', data_files={ \"train\": dpt_corpus_train_data_selected, \"val\": dpt_corpus_val } ) tokenized_datasets = datasets.map( lambda examples: tokenizer(examples['text'], truncation=True, max_length=model.config.max_position_embeddings), batched=True ) data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm=True, mlm_probability=0.15 ) 实例化TrainingArguments和Trainer training_args = TrainingArguments( output_dir=\"./results/domain_pre_training\", overwrite_output_dir=True, max_steps=100, per_device_train_batch_size=8, per_device_eval_batch_size=16, evaluation_strategy=\"steps\", save_steps=50, save_total_limit=2, logging_steps=50, seed=42, # fp16=True, dataloader_num_workers=2, disable_tqdm=False ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'], data_collator=data_collator, tokenizer=tokenizer, # 这个标记器有新的tokens ) # 进行训练 trainer.train() 训练结果 Step Training Loss Validation Loss Runtime Samples Per Second 50 2.813800 2.409768 75.058500 13.323000 100 2.520700 2.342451 74.257200 13.467000 ","date":"2022-03-04","objectID":"/tools01/:1:2","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#创建数据集"},{"categories":["documentation"],"content":"Domain Pre-TrainingDomain PreTraining是Domain Adaptation的第三步，我们在领域内语料库上用同样的预训练程序继续训练Transformer模型。 创建数据集 import itertools as it from pathlib import Path from typing import Sequence, Union, Generator from datasets import load_dataset from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments datasets = load_dataset( 'text', data_files={ \"train\": dpt_corpus_train_data_selected, \"val\": dpt_corpus_val } ) tokenized_datasets = datasets.map( lambda examples: tokenizer(examples['text'], truncation=True, max_length=model.config.max_position_embeddings), batched=True ) data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm=True, mlm_probability=0.15 ) 实例化TrainingArguments和Trainer training_args = TrainingArguments( output_dir=\"./results/domain_pre_training\", overwrite_output_dir=True, max_steps=100, per_device_train_batch_size=8, per_device_eval_batch_size=16, evaluation_strategy=\"steps\", save_steps=50, save_total_limit=2, logging_steps=50, seed=42, # fp16=True, dataloader_num_workers=2, disable_tqdm=False ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'], data_collator=data_collator, tokenizer=tokenizer, # 这个标记器有新的tokens ) # 进行训练 trainer.train() 训练结果 Step Training Loss Validation Loss Runtime Samples Per Second 50 2.813800 2.409768 75.058500 13.323000 100 2.520700 2.342451 74.257200 13.467000 ","date":"2022-03-04","objectID":"/tools01/:1:2","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#实例化trainingarguments和trainer"},{"categories":["documentation"],"content":"为特定任务进行微调我们可以为HuggingFace支持的任何微调任务插入我们的domain adaptation模型。在本指南中，我们将在BC2GM数据集（一个流行的生物医学基准数据集）上比较一个开箱即用（OOB）模型与一个领域适应模型在命名实体识别方面的表现。用于NER预处理和评估的实用函数改编自HuggingFace的NER微调示例笔记。 对原始数据集进行预处理，形成NER数据集 from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # 特殊标记有一个单词ID，是None。我们将标签设置为-100，因此它们在损失函数中被自动忽略了。 if word_idx is None: label_ids.append(-100) # 我们为每个词的第一个标记设置标签。 elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # 对于一个词中的其他标记，我们根据label_all_tokens的标志，将标签设置为当前标签或-100。 else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # 移除被忽略的索引（特殊标记）。 true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } 安装 seqeval pip install seqeval seqeval是一个用于序列标记评估的Python框架，可以评估分块任务的性能，如命名实体识别、部分语音标记、语义角色标记等。 label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} 实例化NER模型在此，我们实例化了三个特定任务的NER模型进行比较: da_model: 我们在本指南中刚刚训练的一个Domain Adaptation的NER模型 da_full_corpus_model: 同样的领域适应性NER模型，只是它是在完整的领域内训练语料库上训练的。 oob_model: 一个开箱即用的BERT-NER模型（没有经过Domain Adaptation）。 from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#为特定任务进行微调"},{"categories":["documentation"],"content":"为特定任务进行微调我们可以为HuggingFace支持的任何微调任务插入我们的domain adaptation模型。在本指南中，我们将在BC2GM数据集（一个流行的生物医学基准数据集）上比较一个开箱即用（OOB）模型与一个领域适应模型在命名实体识别方面的表现。用于NER预处理和评估的实用函数改编自HuggingFace的NER微调示例笔记。 对原始数据集进行预处理，形成NER数据集 from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # 特殊标记有一个单词ID，是None。我们将标签设置为-100，因此它们在损失函数中被自动忽略了。 if word_idx is None: label_ids.append(-100) # 我们为每个词的第一个标记设置标签。 elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # 对于一个词中的其他标记，我们根据label_all_tokens的标志，将标签设置为当前标签或-100。 else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # 移除被忽略的索引（特殊标记）。 true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } 安装 seqeval pip install seqeval seqeval是一个用于序列标记评估的Python框架，可以评估分块任务的性能，如命名实体识别、部分语音标记、语义角色标记等。 label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} 实例化NER模型在此，我们实例化了三个特定任务的NER模型进行比较: da_model: 我们在本指南中刚刚训练的一个Domain Adaptation的NER模型 da_full_corpus_model: 同样的领域适应性NER模型，只是它是在完整的领域内训练语料库上训练的。 oob_model: 一个开箱即用的BERT-NER模型（没有经过Domain Adaptation）。 from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#对原始数据集进行预处理形成ner数据集"},{"categories":["documentation"],"content":"为特定任务进行微调我们可以为HuggingFace支持的任何微调任务插入我们的domain adaptation模型。在本指南中，我们将在BC2GM数据集（一个流行的生物医学基准数据集）上比较一个开箱即用（OOB）模型与一个领域适应模型在命名实体识别方面的表现。用于NER预处理和评估的实用函数改编自HuggingFace的NER微调示例笔记。 对原始数据集进行预处理，形成NER数据集 from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # 特殊标记有一个单词ID，是None。我们将标签设置为-100，因此它们在损失函数中被自动忽略了。 if word_idx is None: label_ids.append(-100) # 我们为每个词的第一个标记设置标签。 elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # 对于一个词中的其他标记，我们根据label_all_tokens的标志，将标签设置为当前标签或-100。 else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # 移除被忽略的索引（特殊标记）。 true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } 安装 seqeval pip install seqeval seqeval是一个用于序列标记评估的Python框架，可以评估分块任务的性能，如命名实体识别、部分语音标记、语义角色标记等。 label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} 实例化NER模型在此，我们实例化了三个特定任务的NER模型进行比较: da_model: 我们在本指南中刚刚训练的一个Domain Adaptation的NER模型 da_full_corpus_model: 同样的领域适应性NER模型，只是它是在完整的领域内训练语料库上训练的。 oob_model: 一个开箱即用的BERT-NER模型（没有经过Domain Adaptation）。 from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#安装-seqeval"},{"categories":["documentation"],"content":"为特定任务进行微调我们可以为HuggingFace支持的任何微调任务插入我们的domain adaptation模型。在本指南中，我们将在BC2GM数据集（一个流行的生物医学基准数据集）上比较一个开箱即用（OOB）模型与一个领域适应模型在命名实体识别方面的表现。用于NER预处理和评估的实用函数改编自HuggingFace的NER微调示例笔记。 对原始数据集进行预处理，形成NER数据集 from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # 特殊标记有一个单词ID，是None。我们将标签设置为-100，因此它们在损失函数中被自动忽略了。 if word_idx is None: label_ids.append(-100) # 我们为每个词的第一个标记设置标签。 elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # 对于一个词中的其他标记，我们根据label_all_tokens的标志，将标签设置为当前标签或-100。 else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # 移除被忽略的索引（特殊标记）。 true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } 安装 seqeval pip install seqeval seqeval是一个用于序列标记评估的Python框架，可以评估分块任务的性能，如命名实体识别、部分语音标记、语义角色标记等。 label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} 实例化NER模型在此，我们实例化了三个特定任务的NER模型进行比较: da_model: 我们在本指南中刚刚训练的一个Domain Adaptation的NER模型 da_full_corpus_model: 同样的领域适应性NER模型，只是它是在完整的领域内训练语料库上训练的。 oob_model: 一个开箱即用的BERT-NER模型（没有经过Domain Adaptation）。 from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#实例化ner模型"},{"categories":["documentation"],"content":"为特定任务进行微调我们可以为HuggingFace支持的任何微调任务插入我们的domain adaptation模型。在本指南中，我们将在BC2GM数据集（一个流行的生物医学基准数据集）上比较一个开箱即用（OOB）模型与一个领域适应模型在命名实体识别方面的表现。用于NER预处理和评估的实用函数改编自HuggingFace的NER微调示例笔记。 对原始数据集进行预处理，形成NER数据集 from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # 特殊标记有一个单词ID，是None。我们将标签设置为-100，因此它们在损失函数中被自动忽略了。 if word_idx is None: label_ids.append(-100) # 我们为每个词的第一个标记设置标签。 elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # 对于一个词中的其他标记，我们根据label_all_tokens的标志，将标签设置为当前标签或-100。 else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # 移除被忽略的索引（特殊标记）。 true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } 安装 seqeval pip install seqeval seqeval是一个用于序列标记评估的Python框架，可以评估分块任务的性能，如命名实体识别、部分语音标记、语义角色标记等。 label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} 实例化NER模型在此，我们实例化了三个特定任务的NER模型进行比较: da_model: 我们在本指南中刚刚训练的一个Domain Adaptation的NER模型 da_full_corpus_model: 同样的领域适应性NER模型，只是它是在完整的领域内训练语料库上训练的。 oob_model: 一个开箱即用的BERT-NER模型（没有经过Domain Adaptation）。 from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#为每个模型创建数据集trainingarguments和trainer"},{"categories":["documentation"],"content":"为特定任务进行微调我们可以为HuggingFace支持的任何微调任务插入我们的domain adaptation模型。在本指南中，我们将在BC2GM数据集（一个流行的生物医学基准数据集）上比较一个开箱即用（OOB）模型与一个领域适应模型在命名实体识别方面的表现。用于NER预处理和评估的实用函数改编自HuggingFace的NER微调示例笔记。 对原始数据集进行预处理，形成NER数据集 from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # 特殊标记有一个单词ID，是None。我们将标签设置为-100，因此它们在损失函数中被自动忽略了。 if word_idx is None: label_ids.append(-100) # 我们为每个词的第一个标记设置标签。 elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # 对于一个词中的其他标记，我们根据label_all_tokens的标志，将标签设置为当前标签或-100。 else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # 移除被忽略的索引（特殊标记）。 true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } 安装 seqeval pip install seqeval seqeval是一个用于序列标记评估的Python框架，可以评估分块任务的性能，如命名实体识别、部分语音标记、语义角色标记等。 label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} 实例化NER模型在此，我们实例化了三个特定任务的NER模型进行比较: da_model: 我们在本指南中刚刚训练的一个Domain Adaptation的NER模型 da_full_corpus_model: 同样的领域适应性NER模型，只是它是在完整的领域内训练语料库上训练的。 oob_model: 一个开箱即用的BERT-NER模型（没有经过Domain Adaptation）。 from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#训练和评估da_model"},{"categories":["documentation"],"content":"为特定任务进行微调我们可以为HuggingFace支持的任何微调任务插入我们的domain adaptation模型。在本指南中，我们将在BC2GM数据集（一个流行的生物医学基准数据集）上比较一个开箱即用（OOB）模型与一个领域适应模型在命名实体识别方面的表现。用于NER预处理和评估的实用函数改编自HuggingFace的NER微调示例笔记。 对原始数据集进行预处理，形成NER数据集 from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # 特殊标记有一个单词ID，是None。我们将标签设置为-100，因此它们在损失函数中被自动忽略了。 if word_idx is None: label_ids.append(-100) # 我们为每个词的第一个标记设置标签。 elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # 对于一个词中的其他标记，我们根据label_all_tokens的标志，将标签设置为当前标签或-100。 else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # 移除被忽略的索引（特殊标记）。 true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } 安装 seqeval pip install seqeval seqeval是一个用于序列标记评估的Python框架，可以评估分块任务的性能，如命名实体识别、部分语音标记、语义角色标记等。 label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} 实例化NER模型在此，我们实例化了三个特定任务的NER模型进行比较: da_model: 我们在本指南中刚刚训练的一个Domain Adaptation的NER模型 da_full_corpus_model: 同样的领域适应性NER模型，只是它是在完整的领域内训练语料库上训练的。 oob_model: 一个开箱即用的BERT-NER模型（没有经过Domain Adaptation）。 from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#训练和评估da_model_full_corpus"},{"categories":["documentation"],"content":"为特定任务进行微调我们可以为HuggingFace支持的任何微调任务插入我们的domain adaptation模型。在本指南中，我们将在BC2GM数据集（一个流行的生物医学基准数据集）上比较一个开箱即用（OOB）模型与一个领域适应模型在命名实体识别方面的表现。用于NER预处理和评估的实用函数改编自HuggingFace的NER微调示例笔记。 对原始数据集进行预处理，形成NER数据集 from typing import NamedTuple from functools import partial from typing_extensions import Literal import numpy as np from datasets import Dataset, load_dataset, load_metric class Example(NamedTuple): token: str label: str def load_ner_dataset(mode: Literal['train', 'val', 'test']): file = f\"data/BC2GM_{mode}.tsv\" examples = [] with open(file) as f: token = [] label = [] for line in f: if line.strip() == \"\": examples.append(Example(token=token, label=label)) token = [] label = [] continue t, l = line.strip().split(\"\\t\") token.append(t) label.append(l) res = list(zip(*[(ex.token, ex.label) for ex in examples])) d = {'token': res[0], 'labels': res[1]} return Dataset.from_dict(d) def tokenize_and_align_labels(examples, tokenizer): tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True) label_to_id = dict(map(reversed, enumerate(label_list))) labels = [] for i, label in enumerate(examples[\"labels\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # 特殊标记有一个单词ID，是None。我们将标签设置为-100，因此它们在损失函数中被自动忽略了。 if word_idx is None: label_ids.append(-100) # 我们为每个词的第一个标记设置标签。 elif word_idx != previous_word_idx: label_ids.append(label_to_id[label[word_idx]]) # 对于一个词中的其他标记，我们根据label_all_tokens的标志，将标签设置为当前标签或-100。 else: label_ids.append(label_to_id[label[word_idx]]) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs def compute_metrics(p): predictions, labels = p predictions = np.argmax(predictions, axis=2) # 移除被忽略的索引（特殊标记）。 true_predictions = [ [label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] true_labels = [ [label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] results = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], } 安装 seqeval pip install seqeval seqeval是一个用于序列标记评估的Python框架，可以评估分块任务的性能，如命名实体识别、部分语音标记、语义角色标记等。 label_list = [\"O\", \"B\", \"I\"] metric = load_metric('seqeval') train_dataset = load_ner_dataset('train') val_dataset = load_ner_dataset('val') test_dataset = load_ner_dataset('test') print(train_dataset[0:1]) print(val_dataset[0:1]) print(test_dataset[0:1]) # {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]} # {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]} # {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]} 实例化NER模型在此，我们实例化了三个特定任务的NER模型进行比较: da_model: 我们在本指南中刚刚训练的一个Domain Adaptation的NER模型 da_full_corpus_model: 同样的领域适应性NER模型，只是它是在完整的领域内训练语料库上训练的。 oob_model: 一个开箱即用的BERT-NER模型（没有经过Domain Adaptation）。 from transformer","date":"2022-03-04","objectID":"/tools01/:1:3","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#训练和评估oob_model"},{"categories":["documentation"],"content":"结果我们看到，在这三个模型中，da_full_corpus_model（在整个域内训练语料库上进行了域调整）在测试F1得分上比oob_model高出2%以上。事实上，这个da_full_corpus_model模型是我们训练的在BC2GM上优于SOTA的许多领域适应模型之一。 此外，da_model的表现也低于oob_model。这是可以预期的，因为da_model在本指南中经历了最小的领域预训练。 ","date":"2022-03-04","objectID":"/tools01/:1:4","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#结果"},{"categories":["documentation"],"content":"总结在本指南中，你已经看到了如何使用 “DataSelector “和 “VocabAugmentor”，通过分别执行数据选择和词汇扩展，对变压器模型进行领域调整。 你还看到它们与HuggingFace的所有产品兼容。变换器”、“标记器 “和 “数据集”。 最后表明，在完整的领域内语料库上进行领域适应的模型比开箱即用的模型表现更好。 ","date":"2022-03-04","objectID":"/tools01/:2:0","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#总结"},{"categories":["documentation"],"content":"参考Transformers-Domain-Adaptation Guide to Transformers Domain Adaptation ","date":"2022-03-04","objectID":"/tools01/:3:0","series":null,"tags":["NER","NLP","Domain Adaptation","Tools"],"title":"Transformers Domain Adaptation","uri":"/tools01/#参考"},{"categories":["documentation"],"content":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","date":"2022-03-04","objectID":"/paper02/","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/"},{"categories":["documentation"],"content":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking 论文解读。 ","date":"2022-03-04","objectID":"/paper02/:0:0","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#"},{"categories":["documentation"],"content":"题目Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking [ACL 2021 Short] [Code] ","date":"2022-03-04","objectID":"/paper02/:0:1","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#题目"},{"categories":["documentation"],"content":"摘要将外部特定领域的知识（如UMLS）注入预训练的语言模型（LM）中，可以提高其处理特殊领域内任务的能力，如生物医学实体链接任务（BEL），然而，这种丰富的专家知识只适用于少数语言（如英语）。在这项工作中，作者通过提出一个新的跨语言生物医学实体连接任务(XL-BEL)，并建立一个新的XL-BEL基准，跨越10种不同的语言，作者首先研究了标准的知识诊断以及知识增强的单语言和多语言LMs在标准的单语言英语BEL任务之外的能力。这些评分显示了与英语表现的巨大差距。然后，作者解决了将特定领域的知识从资源丰富的语言转移到资源贫乏的语言的挑战。为此，作者为XL-BEL任务提出并评估了一系列跨语言的转移方法，并证明一般领域的抓字眼有助于将现有的英语知识传播给几乎没有in-domain数据的语言。值得注意的是，作者提出的特定领域的转移方法在所有目标语言中产生了一致的收益，有时甚至达到了20%@1点，而目标语言中没有任何领域内的知识，也没有任何领域内的平行数据。 ","date":"2022-03-04","objectID":"/paper02/:0:2","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#摘要"},{"categories":["documentation"],"content":"贡献 强调了学习（生物医学）专业领域的跨语言表征的挑战。 提出了一个新颖的多语种XL-BEL任务，并有10种语言的综合评估基准。 对XL-BEL任务中现有的知识诊断和知识增强的单语和多语LMs进行了系统性的评估。 在生物医学领域提出了一个新的SotA多语言编码器，它在XL-BEL中产生了巨大的收益，特别是在资源贫乏的语言上，并提供了强大的基准测试结果来指导未来的工作。 ","date":"2022-03-04","objectID":"/paper02/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#贡献"},{"categories":["documentation"],"content":"模型","date":"2022-03-04","objectID":"/paper02/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#模型"},{"categories":["documentation"],"content":"Language-Agnostic SAP让$(x,y)\\in \\mathcal{X}\\times \\mathcal{Y}$表示一个名字和其分类标签的元组。当从UMLS同义词学习时，$\\mathcal{X}\\times \\mathcal{Y}$是所有(name，CUI)对的集合，例(vaccination, C0042196)。虽然Liu等人只使用英文名称，但作者在此考虑其他UMLS语言的名称。 在训练过程中，该模型被引导为同义词创建类似的表示，而不论其语言如何。 该学习方案包括：1）一个在线抽样程序来选择训练实例；2）一个度量学习损失，鼓励共享相同CUI的字符串获得类似的表示。 Training Examples给定一个由$N$个例子组成的小批次$\\mathcal{B} ={\\mathcal{X_B}}\\times{\\mathcal{Y_B}}={{(x_i, y_i)}_i^N}=1 $，我们从为所有名字$x_i\\in\\mathcal{X_B}$构建所有可能的三联体开始。每个三联体的形式是$(x_a, x_p, x_n)$，其中$x_a$是锚标签，是$\\mathcal{X_B}$中的一个任意名字；$x_p$是$x_a$的正匹配（即$y_a= y_p$），$x_n$是$x_a$的负匹配（即$y_a\\not =y_n$）。然后我们让$f(\\cdot)$表示编码器（即本文中的MBERT或XLMR），在构建的三联体中，选择所有满足以下约束条件的三联体: $$ \\Vert f(x_a) -f(x_p) \\Vert_2 + \\lambda \\geq \\Vert f(x_a) -f(x_n) \\Vert_2 $$ 其中$\\lambda$是一个预定义的余量。换句话说，我们只考虑正样本比负样本多出$\\lambda$的三联体。这些\"硬\"三联体对表示学习来说信息量更大。然后，每个被选中的三联体都会贡献一个正数对$(x_a,x_p)$和一个负数对$(x_a,x_n)$，我们收集所有这样的正数和负数，并将它们表示为$\\mathcal{P},\\mathcal{N}$。 Multi-Similarity Loss我们计算所有名字代表的成对余弦相似度，得到一个相似度矩阵$\\mathcal{S}\\in\\mathscr{R}^{\\vert{\\mathcal{X_B}}\\vert\\times\\vert{\\mathcal{X_B}}\\vert}$，其中每个条目$\\mathcal{S_{ij}}$是小批$\\mathcal{B}$中第$i$个和第$j$个名字之间的余弦相似度，然后使用多重相似度损失从三联体学习: \\begin{align} \\mathcal{L} = \\frac{1}{\\vert{\\mathcal{X_B}}\\vert}{\\displaystyle\\sum_{i=1}^{\\vert{\\mathcal{X_B}}\\vert}}(\\frac{1}{\\alpha}\\log{(1+\\sum_{n\\in\\mathcal{N}_i} e^{\\alpha(\\mathcal{S}_{in}-\\epsilon)})}+\\frac{1}{\\beta}\\log{(1+\\sum_{n\\in\\mathcal{P}_i} e^{\\alpha(\\mathcal{S}_{ip}-\\epsilon)})}) \\end{align} $\\alpha，\\beta$是温度标度；$\\epsilon$是应用于相似性矩阵的偏移量；$\\mathcal{P}_i,\\mathcal{N}_i$是第$i$个锚的正负样本的指数。 ","date":"2022-03-04","objectID":"/paper02/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#language-agnostic-sap"},{"categories":["documentation"],"content":"Language-Agnostic SAP让$(x,y)\\in \\mathcal{X}\\times \\mathcal{Y}$表示一个名字和其分类标签的元组。当从UMLS同义词学习时，$\\mathcal{X}\\times \\mathcal{Y}$是所有(name，CUI)对的集合，例(vaccination, C0042196)。虽然Liu等人只使用英文名称，但作者在此考虑其他UMLS语言的名称。 在训练过程中，该模型被引导为同义词创建类似的表示，而不论其语言如何。 该学习方案包括：1）一个在线抽样程序来选择训练实例；2）一个度量学习损失，鼓励共享相同CUI的字符串获得类似的表示。 Training Examples给定一个由$N$个例子组成的小批次$\\mathcal{B} ={\\mathcal{X_B}}\\times{\\mathcal{Y_B}}={{(x_i, y_i)}_i^N}=1 $，我们从为所有名字$x_i\\in\\mathcal{X_B}$构建所有可能的三联体开始。每个三联体的形式是$(x_a, x_p, x_n)$，其中$x_a$是锚标签，是$\\mathcal{X_B}$中的一个任意名字；$x_p$是$x_a$的正匹配（即$y_a= y_p$），$x_n$是$x_a$的负匹配（即$y_a\\not =y_n$）。然后我们让$f(\\cdot)$表示编码器（即本文中的MBERT或XLMR），在构建的三联体中，选择所有满足以下约束条件的三联体: $$ \\Vert f(x_a) -f(x_p) \\Vert_2 + \\lambda \\geq \\Vert f(x_a) -f(x_n) \\Vert_2 $$ 其中$\\lambda$是一个预定义的余量。换句话说，我们只考虑正样本比负样本多出$\\lambda$的三联体。这些\"硬\"三联体对表示学习来说信息量更大。然后，每个被选中的三联体都会贡献一个正数对$(x_a,x_p)$和一个负数对$(x_a,x_n)$，我们收集所有这样的正数和负数，并将它们表示为$\\mathcal{P},\\mathcal{N}$。 Multi-Similarity Loss我们计算所有名字代表的成对余弦相似度，得到一个相似度矩阵$\\mathcal{S}\\in\\mathscr{R}^{\\vert{\\mathcal{X_B}}\\vert\\times\\vert{\\mathcal{X_B}}\\vert}$，其中每个条目$\\mathcal{S_{ij}}$是小批$\\mathcal{B}$中第$i$个和第$j$个名字之间的余弦相似度，然后使用多重相似度损失从三联体学习: \\begin{align} \\mathcal{L} = \\frac{1}{\\vert{\\mathcal{X_B}}\\vert}{\\displaystyle\\sum_{i=1}^{\\vert{\\mathcal{X_B}}\\vert}}(\\frac{1}{\\alpha}\\log{(1+\\sum_{n\\in\\mathcal{N}_i} e^{\\alpha(\\mathcal{S}_{in}-\\epsilon)})}+\\frac{1}{\\beta}\\log{(1+\\sum_{n\\in\\mathcal{P}_i} e^{\\alpha(\\mathcal{S}_{ip}-\\epsilon)})}) \\end{align} $\\alpha，\\beta$是温度标度；$\\epsilon$是应用于相似性矩阵的偏移量；$\\mathcal{P}_i,\\mathcal{N}_i$是第$i$个锚的正负样本的指数。 ","date":"2022-03-04","objectID":"/paper02/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#training-examples"},{"categories":["documentation"],"content":"Language-Agnostic SAP让$(x,y)\\in \\mathcal{X}\\times \\mathcal{Y}$表示一个名字和其分类标签的元组。当从UMLS同义词学习时，$\\mathcal{X}\\times \\mathcal{Y}$是所有(name，CUI)对的集合，例(vaccination, C0042196)。虽然Liu等人只使用英文名称，但作者在此考虑其他UMLS语言的名称。 在训练过程中，该模型被引导为同义词创建类似的表示，而不论其语言如何。 该学习方案包括：1）一个在线抽样程序来选择训练实例；2）一个度量学习损失，鼓励共享相同CUI的字符串获得类似的表示。 Training Examples给定一个由$N$个例子组成的小批次$\\mathcal{B} ={\\mathcal{X_B}}\\times{\\mathcal{Y_B}}={{(x_i, y_i)}_i^N}=1 $，我们从为所有名字$x_i\\in\\mathcal{X_B}$构建所有可能的三联体开始。每个三联体的形式是$(x_a, x_p, x_n)$，其中$x_a$是锚标签，是$\\mathcal{X_B}$中的一个任意名字；$x_p$是$x_a$的正匹配（即$y_a= y_p$），$x_n$是$x_a$的负匹配（即$y_a\\not =y_n$）。然后我们让$f(\\cdot)$表示编码器（即本文中的MBERT或XLMR），在构建的三联体中，选择所有满足以下约束条件的三联体: $$ \\Vert f(x_a) -f(x_p) \\Vert_2 + \\lambda \\geq \\Vert f(x_a) -f(x_n) \\Vert_2 $$ 其中$\\lambda$是一个预定义的余量。换句话说，我们只考虑正样本比负样本多出$\\lambda$的三联体。这些\"硬\"三联体对表示学习来说信息量更大。然后，每个被选中的三联体都会贡献一个正数对$(x_a,x_p)$和一个负数对$(x_a,x_n)$，我们收集所有这样的正数和负数，并将它们表示为$\\mathcal{P},\\mathcal{N}$。 Multi-Similarity Loss我们计算所有名字代表的成对余弦相似度，得到一个相似度矩阵$\\mathcal{S}\\in\\mathscr{R}^{\\vert{\\mathcal{X_B}}\\vert\\times\\vert{\\mathcal{X_B}}\\vert}$，其中每个条目$\\mathcal{S_{ij}}$是小批$\\mathcal{B}$中第$i$个和第$j$个名字之间的余弦相似度，然后使用多重相似度损失从三联体学习: \\begin{align} \\mathcal{L} = \\frac{1}{\\vert{\\mathcal{X_B}}\\vert}{\\displaystyle\\sum_{i=1}^{\\vert{\\mathcal{X_B}}\\vert}}(\\frac{1}{\\alpha}\\log{(1+\\sum_{n\\in\\mathcal{N}_i} e^{\\alpha(\\mathcal{S}_{in}-\\epsilon)})}+\\frac{1}{\\beta}\\log{(1+\\sum_{n\\in\\mathcal{P}_i} e^{\\alpha(\\mathcal{S}_{ip}-\\epsilon)})}) \\end{align} $\\alpha，\\beta$是温度标度；$\\epsilon$是应用于相似性矩阵的偏移量；$\\mathcal{P}_i,\\mathcal{N}_i$是第$i$个锚的正负样本的指数。 ","date":"2022-03-04","objectID":"/paper02/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#multi-similarity-loss"},{"categories":["documentation"],"content":"实验结果与讨论 Fig-1.Multilingual UMLS Knowledge Always Helps Fig-1总结了在各种单语、多语和领域内预训练编码器上应用基于UMLS知识的多语言SAP微调的结果；注入UMLS知识对模型在XL-BEL上的表现在所有语言和所有基础编码器上都是有益的。使用多语言UMLS同义词对生物医学$\\texttt{PUBMEDBERT}$（$SapBERT_{all_syn}$）进行SAP-fine-tune，而不是只使用英语同义词（$SapBERT$），能全面提高其性能。对每种语言的单语BERT进行SAP-ing调整，也在所有语言中产生了巨大的收益；唯一的例外是泰语（TH），它在UMLS中没有体现。对多语言模型MBERT和XLMR进行微调，会带来更大的相对收益。 UMLS数据在很大程度上偏向于罗曼语和日耳曼语。因此，对于与这些语系比较相似的语言，单语LM（上半部分，Fig-1）与多语LM（下半部分，Fig-1）相比表现相当或优于多语LM。然而，对于其他（遥远的）语言（如KO、ZH、JA、TH），情况则相反。例如，在TH上，XLMR+SAPall_syn比THBERT+SAPall_syn高出20%@1的精确度。 Fig-2.General Translation Knowledge is Useful Fig-2总结了我们在一般翻译数据上继续训练的结果。 在之前基于UMLS的SAP之后 通过这个变体，基础多语言LM成为强大的多语言生物医学专家。我们观察到域外翻译数据的额外强大提升：例如，对于MBERT，除ES外，所有语言的提升范围为2.4%至12.7%。对于XLMR，我们报告了$XLMR+SAPen_{syn}$在RU、TR、KO、TH上的精确度@1提升，以及$XLMR+SAPall_{syn}$的类似但较小的提升。 ","date":"2022-03-04","objectID":"/paper02/:0:6","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#实验结果与讨论"},{"categories":["documentation"],"content":"结论我们引入了一个新的跨语言生物医学实体任务（XL-BEL），为生物医学领域的跨语言实体表示建立了一个覆盖面广且可靠的评估基准，并在XL-BEL上评估了当前的SotA生物医学实体表示。我们还提出了一个有效的迁移学习方案，利用一般领域的翻译来提高领域专业表示模型的跨语言能力。我们希望我们的工作能够激励未来更多关于多语言和领域专业表示学习的研究。 ","date":"2022-03-04","objectID":"/paper02/:0:7","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#结论"},{"categories":["documentation"],"content":"代码https://github.com/cambridgeltl/sapbert ","date":"2022-03-04","objectID":"/paper02/:0:8","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking","uri":"/paper02/#代码"},{"categories":["documentation"],"content":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","date":"2022-03-03","objectID":"/paper01/","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/"},{"categories":["documentation"],"content":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition 论文解读。 ","date":"2022-03-03","objectID":"/paper01/:0:0","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#"},{"categories":["documentation"],"content":"题目Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition [ACL 2021 Long] [Code] ","date":"2022-03-03","objectID":"/paper01/:0:1","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#题目"},{"categories":["documentation"],"content":"摘要众包被认为是有效监督学习的一个前瞻性解决方案，旨在通过群体劳动建立大规模的注释训练数据。以前的研究集中在减少众包注解的噪音对监督模式的影响。在这项工作中，我们采取了不同的观点，将所有众包注释重新视为与个别数据标注师有关的黄金标准。通过这种方式，我们发现众包可以与领域适应性（domain adaptation）高度相似，那么最近的跨领域方法的进展几乎可以直接应用于众包。在这里，我们以命名实体识别（NER）为研究案例，提出了一个Annotator-aware的表示学习模型，该模型受到领域适应方法的启发，试图捕捉有效的Domain-aware的特征。我们研究了无监督和有监督的众包学习，假设没有或只有小规模的专家注释可用，在一个基准的众包NER数据集上的实验结果表明，我们的方法是非常有效的，表现了一个新的最先进的性能。此外，在有监督的情况下，我们只需要很小规模的专家注释就可以获得令人印象深刻的性能提升。 ","date":"2022-03-03","objectID":"/paper01/:0:2","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#摘要"},{"categories":["documentation"],"content":"贡献 对众包学习提出了不同的看法，并建议将众包学习转化为领域适应问题（domain adaptation），这自然而然地将NLP的两个重要主题联系起来。 提出了一种新型的众包学习方法。尽管该方法在领域适应方面的新颖性有限，但它是第一项关于众包学习的工作，并能在NER上取得最先进的性能。 首次引入了有监督的众包学习，这是从领域适应性中借来的，将是NLP任务的一个前瞻性解决方案。 ","date":"2022-03-03","objectID":"/paper01/:0:3","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#贡献"},{"categories":["documentation"],"content":"模型架构包括四个部分： (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.模型结构 词表示层（word representation）假设存在一个包含$n$个单词的句子$w_1 \\dots w_n$,我们首先通过$\\texttt{Adapter} \\circ \\texttt{BERT}$将其转换为矢量表征. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ 注意：值得注意的是，$\\texttt{Adapter} \\circ \\texttt{BERT}$方法不再需要对庞大的BERT参数进行微调，而是通过调整轻得多的适配器参数来获得相当的性能。因此通过这种方式，可以很容易地将词的表示法扩展为annotator-aware的表示法。 注释者切换器层（annotator switcher）作者目标是有效地学习不同数据标注师意识到的词汇特征，这可以被视为对个别注释者的上下文理解。因此，引入了一个注释者切换器，以支持带有注释者输入的$\\texttt{Adapter} \\circ \\texttt{BERT}$，其灵感来自于Parameter Generation Network (PGN)，其关键思想是使用参数生成网络（PGN），通过输入annotators动态地产生适配器参数。通过这种方式，模型可以在不同的annotators之间灵活地切换。 具体来说，假设$V$是所有适配器参数的矢量形式，通过打包操作，也可以解包恢复所有的适配器参数，PGN模块就是根据annotators的输入动态生成$\\texttt{Adapter} \\circ \\texttt{BERT}$的$V$，如模型图中右边的橙色部分所示，切换器switcher可以被形式化为： $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1’ \\dots \\textbf{n}_1’\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ 其中$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$，$\\textbf{x} =\\textbf{r}_1’ \\dots \\textbf{n}_1’$是注释者$a$对$x=w_1 \\dots w_n$的annotator-aware的表示，$\\textbf{e}^a$是annotator的embedding。 BiLSTM编码层（BiLSTM Encoding）$\\texttt{Adapter} \\circ \\texttt{BERT}$需要一个额外的面向任务的模块来进行高级特征提取。在这里利用单一的BiLSTM层来实现：$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$，用于下一步的推理和训练。 CRF层（CRF inference and training）最后使用CRF来计算候选顺序输出$y = l_1 \\dots l_n$的全局得分。 $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ 其中$\\textbf{W}^{crf}、 \\textbf{b}^{crf}、 \\textbf{T}$是模型的参数。给定一个输入$(x，a)$，通过维特比算法进行推理,对于训练，定义了一个句子级别的交叉熵目标。 $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ 其中$y^a$是$a$对$x$的黄金标准输出，$y$属于所有可能的候选人，$p(y^a|x, a)$表示句子级的概率。 ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#模型架构"},{"categories":["documentation"],"content":"模型架构包括四个部分： (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.模型结构 词表示层（word representation）假设存在一个包含$n$个单词的句子$w_1 \\dots w_n$,我们首先通过$\\texttt{Adapter} \\circ \\texttt{BERT}$将其转换为矢量表征. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ 注意：值得注意的是，$\\texttt{Adapter} \\circ \\texttt{BERT}$方法不再需要对庞大的BERT参数进行微调，而是通过调整轻得多的适配器参数来获得相当的性能。因此通过这种方式，可以很容易地将词的表示法扩展为annotator-aware的表示法。 注释者切换器层（annotator switcher）作者目标是有效地学习不同数据标注师意识到的词汇特征，这可以被视为对个别注释者的上下文理解。因此，引入了一个注释者切换器，以支持带有注释者输入的$\\texttt{Adapter} \\circ \\texttt{BERT}$，其灵感来自于Parameter Generation Network (PGN)，其关键思想是使用参数生成网络（PGN），通过输入annotators动态地产生适配器参数。通过这种方式，模型可以在不同的annotators之间灵活地切换。 具体来说，假设$V$是所有适配器参数的矢量形式，通过打包操作，也可以解包恢复所有的适配器参数，PGN模块就是根据annotators的输入动态生成$\\texttt{Adapter} \\circ \\texttt{BERT}$的$V$，如模型图中右边的橙色部分所示，切换器switcher可以被形式化为： $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1’ \\dots \\textbf{n}_1’\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ 其中$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$，$\\textbf{x} =\\textbf{r}_1’ \\dots \\textbf{n}_1’$是注释者$a$对$x=w_1 \\dots w_n$的annotator-aware的表示，$\\textbf{e}^a$是annotator的embedding。 BiLSTM编码层（BiLSTM Encoding）$\\texttt{Adapter} \\circ \\texttt{BERT}$需要一个额外的面向任务的模块来进行高级特征提取。在这里利用单一的BiLSTM层来实现：$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$，用于下一步的推理和训练。 CRF层（CRF inference and training）最后使用CRF来计算候选顺序输出$y = l_1 \\dots l_n$的全局得分。 $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ 其中$\\textbf{W}^{crf}、 \\textbf{b}^{crf}、 \\textbf{T}$是模型的参数。给定一个输入$(x，a)$，通过维特比算法进行推理,对于训练，定义了一个句子级别的交叉熵目标。 $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ 其中$y^a$是$a$对$x$的黄金标准输出，$y$属于所有可能的候选人，$p(y^a|x, a)$表示句子级的概率。 ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#词表示层word-representation"},{"categories":["documentation"],"content":"模型架构包括四个部分： (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.模型结构 词表示层（word representation）假设存在一个包含$n$个单词的句子$w_1 \\dots w_n$,我们首先通过$\\texttt{Adapter} \\circ \\texttt{BERT}$将其转换为矢量表征. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ 注意：值得注意的是，$\\texttt{Adapter} \\circ \\texttt{BERT}$方法不再需要对庞大的BERT参数进行微调，而是通过调整轻得多的适配器参数来获得相当的性能。因此通过这种方式，可以很容易地将词的表示法扩展为annotator-aware的表示法。 注释者切换器层（annotator switcher）作者目标是有效地学习不同数据标注师意识到的词汇特征，这可以被视为对个别注释者的上下文理解。因此，引入了一个注释者切换器，以支持带有注释者输入的$\\texttt{Adapter} \\circ \\texttt{BERT}$，其灵感来自于Parameter Generation Network (PGN)，其关键思想是使用参数生成网络（PGN），通过输入annotators动态地产生适配器参数。通过这种方式，模型可以在不同的annotators之间灵活地切换。 具体来说，假设$V$是所有适配器参数的矢量形式，通过打包操作，也可以解包恢复所有的适配器参数，PGN模块就是根据annotators的输入动态生成$\\texttt{Adapter} \\circ \\texttt{BERT}$的$V$，如模型图中右边的橙色部分所示，切换器switcher可以被形式化为： $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1’ \\dots \\textbf{n}_1’\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ 其中$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$，$\\textbf{x} =\\textbf{r}_1’ \\dots \\textbf{n}_1’$是注释者$a$对$x=w_1 \\dots w_n$的annotator-aware的表示，$\\textbf{e}^a$是annotator的embedding。 BiLSTM编码层（BiLSTM Encoding）$\\texttt{Adapter} \\circ \\texttt{BERT}$需要一个额外的面向任务的模块来进行高级特征提取。在这里利用单一的BiLSTM层来实现：$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$，用于下一步的推理和训练。 CRF层（CRF inference and training）最后使用CRF来计算候选顺序输出$y = l_1 \\dots l_n$的全局得分。 $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ 其中$\\textbf{W}^{crf}、 \\textbf{b}^{crf}、 \\textbf{T}$是模型的参数。给定一个输入$(x，a)$，通过维特比算法进行推理,对于训练，定义了一个句子级别的交叉熵目标。 $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ 其中$y^a$是$a$对$x$的黄金标准输出，$y$属于所有可能的候选人，$p(y^a|x, a)$表示句子级的概率。 ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#注释者切换器层annotator-switcher"},{"categories":["documentation"],"content":"模型架构包括四个部分： (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.模型结构 词表示层（word representation）假设存在一个包含$n$个单词的句子$w_1 \\dots w_n$,我们首先通过$\\texttt{Adapter} \\circ \\texttt{BERT}$将其转换为矢量表征. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ 注意：值得注意的是，$\\texttt{Adapter} \\circ \\texttt{BERT}$方法不再需要对庞大的BERT参数进行微调，而是通过调整轻得多的适配器参数来获得相当的性能。因此通过这种方式，可以很容易地将词的表示法扩展为annotator-aware的表示法。 注释者切换器层（annotator switcher）作者目标是有效地学习不同数据标注师意识到的词汇特征，这可以被视为对个别注释者的上下文理解。因此，引入了一个注释者切换器，以支持带有注释者输入的$\\texttt{Adapter} \\circ \\texttt{BERT}$，其灵感来自于Parameter Generation Network (PGN)，其关键思想是使用参数生成网络（PGN），通过输入annotators动态地产生适配器参数。通过这种方式，模型可以在不同的annotators之间灵活地切换。 具体来说，假设$V$是所有适配器参数的矢量形式，通过打包操作，也可以解包恢复所有的适配器参数，PGN模块就是根据annotators的输入动态生成$\\texttt{Adapter} \\circ \\texttt{BERT}$的$V$，如模型图中右边的橙色部分所示，切换器switcher可以被形式化为： $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1’ \\dots \\textbf{n}_1’\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ 其中$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$，$\\textbf{x} =\\textbf{r}_1’ \\dots \\textbf{n}_1’$是注释者$a$对$x=w_1 \\dots w_n$的annotator-aware的表示，$\\textbf{e}^a$是annotator的embedding。 BiLSTM编码层（BiLSTM Encoding）$\\texttt{Adapter} \\circ \\texttt{BERT}$需要一个额外的面向任务的模块来进行高级特征提取。在这里利用单一的BiLSTM层来实现：$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$，用于下一步的推理和训练。 CRF层（CRF inference and training）最后使用CRF来计算候选顺序输出$y = l_1 \\dots l_n$的全局得分。 $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ 其中$\\textbf{W}^{crf}、 \\textbf{b}^{crf}、 \\textbf{T}$是模型的参数。给定一个输入$(x，a)$，通过维特比算法进行推理,对于训练，定义了一个句子级别的交叉熵目标。 $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ 其中$y^a$是$a$对$x$的黄金标准输出，$y$属于所有可能的候选人，$p(y^a|x, a)$表示句子级的概率。 ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#bilstm编码层bilstm-encoding"},{"categories":["documentation"],"content":"模型架构包括四个部分： (1) word representation (2) annotator switcher (3) BiLSTM Encoding (4) CRF inference and training. Fig-1.模型结构 词表示层（word representation）假设存在一个包含$n$个单词的句子$w_1 \\dots w_n$,我们首先通过$\\texttt{Adapter} \\circ \\texttt{BERT}$将其转换为矢量表征. $$ e_1 \\dots e_n = \\texttt{Adapter} \\circ \\texttt{BERT}(w_1 \\dots w_n) $$ 注意：值得注意的是，$\\texttt{Adapter} \\circ \\texttt{BERT}$方法不再需要对庞大的BERT参数进行微调，而是通过调整轻得多的适配器参数来获得相当的性能。因此通过这种方式，可以很容易地将词的表示法扩展为annotator-aware的表示法。 注释者切换器层（annotator switcher）作者目标是有效地学习不同数据标注师意识到的词汇特征，这可以被视为对个别注释者的上下文理解。因此，引入了一个注释者切换器，以支持带有注释者输入的$\\texttt{Adapter} \\circ \\texttt{BERT}$，其灵感来自于Parameter Generation Network (PGN)，其关键思想是使用参数生成网络（PGN），通过输入annotators动态地产生适配器参数。通过这种方式，模型可以在不同的annotators之间灵活地切换。 具体来说，假设$V$是所有适配器参数的矢量形式，通过打包操作，也可以解包恢复所有的适配器参数，PGN模块就是根据annotators的输入动态生成$\\texttt{Adapter} \\circ \\texttt{BERT}$的$V$，如模型图中右边的橙色部分所示，切换器switcher可以被形式化为： $$ \\begin{align} \\textbf{x} \u0026=\\textbf{r}_1’ \\dots \\textbf{n}_1’\\\\ \u0026=\\textbf{PGN} \\circ \\textbf{Adapter} \\circ \\textbf{BERT}(x,a)\\\\ \u0026=\\textbf{Adapter} \\circ \\textbf{BERT}(x,\\textbf{V}=\\mathbf{\\Theta} \\times \\textbf{e}^a) \\end{align} $$ 其中$\\mathbf{\\Theta} \\in \\mathcal{R}^{\\vert \\textbf{V} \\vert \\times\\textbf{e}^a}$，$\\textbf{x} =\\textbf{r}_1’ \\dots \\textbf{n}_1’$是注释者$a$对$x=w_1 \\dots w_n$的annotator-aware的表示，$\\textbf{e}^a$是annotator的embedding。 BiLSTM编码层（BiLSTM Encoding）$\\texttt{Adapter} \\circ \\texttt{BERT}$需要一个额外的面向任务的模块来进行高级特征提取。在这里利用单一的BiLSTM层来实现：$h_1 \\dots h_n = \\texttt{BiLSTM}(x)$，用于下一步的推理和训练。 CRF层（CRF inference and training）最后使用CRF来计算候选顺序输出$y = l_1 \\dots l_n$的全局得分。 $$ \\begin{align} {\\textbf{o}_i} \u0026=\\textbf{W}^{crf} {\\textbf{h}_i}+\\textbf{b}^{crf} \\ \\end{align} $$ $$ \\begin{align} {\\sum_{i=1}^n} (\\textbf{T}[l_{i-1},l_i]+{\\textbf{o}_i}[l_i]) \\end{align} $$ 其中$\\textbf{W}^{crf}、 \\textbf{b}^{crf}、 \\textbf{T}$是模型的参数。给定一个输入$(x，a)$，通过维特比算法进行推理,对于训练，定义了一个句子级别的交叉熵目标。 $$ \\begin{align} p(y^a \\vert x,a) \u0026=\\frac{\\exp{\\texttt{score}(y^a \\vert x,a)}}{\\sum_y \\exp{\\texttt{score}(y \\vert x,a)}}\\\\ \\mathcal{L} \u0026=-\\log{(y^a \\vert x,a)} \\end{align} $$ 其中$y^a$是$a$对$x$的黄金标准输出，$y$属于所有可能的候选人，$p(y^a|x, a)$表示句子级的概率。 ","date":"2022-03-03","objectID":"/paper01/:0:4","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#crf层crf-inference-and-training"},{"categories":["documentation"],"content":"结果与讨论 Fig-1.无监督学习的实验结果 Fig-1显示了无监督情况下的测试结果。从整体上看，我们可以看到表征学习模型通过借用了Domain Adaptation，可以达到最好的性能，F1得分达到77.95，明显优于第二好的模型LC-cat。 这一结果表明作者提出的方法比其他模型更有优势。通过深入研究结果，可以发现，annotator-aware模型明显优于annotator-agnostic模型，表明注释者信息对众包学习有很大帮助，这个观察结果进一步显示了将不同注释者类比不同领域的合理性，因为领域信息对于领域适应也是有用的。此外，作者提出的表征学习方法在annotator-aware模型中的表现更好，表明模型可以更有效地捕捉注释者感知的信息。 Fig-2.有监督学习的实验结果 为了研究有监督情况，我们假设所有众包句子的专家注释是可用的。除了探索完整的专家注释，我们还研究了另外三种不同的情况，即在无监督环境下逐步增加专家注释，目的是研究我们的模型在小规模专家注释下的有效性。具体来说，我们假设专家注释的比例为1%、5%、25%和100%。 Fig-2显示了所有的结果，包括四个baselines和一个只基于专家注释的gold模型进行比较。总的来说，作者提出的表征学习模型可以为所有的场景带来最好的表现，证明它在监督学习中也是有效的。 接下来，通过比较annotator-agnostic模型和annotator-aware模型，我们可以看到annotator-aware模型效果更好，这与无监督的设置是一致的。 更有趣的是，结果显示在专家注释规模很小的情况下（1%和5%），All比gold好，而只有在有足够的专家注释时（25%和100%），这一趋势才会逆转。 该观察表明，当黄金注释(gold annotations)不足时，众包注释(crowdsourced annotations)总是有帮助的，同时，我们很容易理解MV比gold差，因为后者的训练语料质量更高。 此外，可以发现即使是增加annotator-aware机制的LC和LC-cat模型也无法获得与gold annotations类似的效果，这表明从众包注释中提炼更加优秀的数据标注可能不是最有希望的解决方案，而表征学习模型可以持续给出比gold annotations更好的结果，表明众包注释对我们的方法总是有帮助。 ","date":"2022-03-03","objectID":"/paper01/:0:5","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#结果与讨论"},{"categories":["documentation"],"content":"代码解读待复现 ","date":"2022-03-03","objectID":"/paper01/:0:6","series":null,"tags":["NER","NLP","Domain Adaptation"],"title":"ACl-2021-Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition","uri":"/paper01/#代码解读"},{"categories":["documentation"],"content":"Begio经典论文’A Neural Probabilistic Language Model‘","date":"2022-03-01","objectID":"/nnlm/","series":null,"tags":["Model","NLP","经典论文研读系列"],"title":"01-NNLM(’A Neural Probabilistic Language Model‘) ","uri":"/nnlm/"},{"categories":["documentation"],"content":"A Neural Probabilistic Language Model这篇论文是预训练语言模型的开山之作，Yoshua Bengio等于2003年提出的方法。 ","date":"2022-03-01","objectID":"/nnlm/:1:0","series":null,"tags":["Model","NLP","经典论文研读系列"],"title":"01-NNLM(’A Neural Probabilistic Language Model‘) ","uri":"/nnlm/#a-neural-probabilistic-language-model"},{"categories":["documentation"],"content":"观点 将词汇表$V$中的每个单词${w_i}$关联到一个分布式单词特征向量$\\mathcal{R}^m$。 将句子的联合概率函数表示为句子序列中单词特征向量的组合。 同时学习单词的特征向量和句子联合概率函数的参数。 ","date":"2022-03-01","objectID":"/nnlm/:1:1","series":null,"tags":["Model","NLP","经典论文研读系列"],"title":"01-NNLM(’A Neural Probabilistic Language Model‘) ","uri":"/nnlm/#观点"},{"categories":["documentation"],"content":"模型假设存在句子$w_1,\\dots，w_i,\\dots,w_n$，其中$w_n \\in V$，$V$表示词汇集合，$w_i$表示单词，目标函数是学习$f(w_t,\\dots,w_{t-n+1})=\\hat{P}(w_t \\vert w_1^{t-1})$的参数。 Bengio等人将模型分成两个部分： 一个映射函数$C$，将 $V$中的第$i$个单词$w_i$映射成为一个 特征向量 $C(w_i)\\in \\mathcal{R}^m$，它表示词汇表中与每个单词相关的分布特征向量。 一个使用映射函数$C$表示的概率函数$g$，通过上下文中单词的特征向量的乘积组成联合概率模型，$g$的输出是一个向量，它的第$i$个元素估计了概率。 $$ f(i,w_{t-1},\\dots,w_{t-n+1})=g(i,C(w_{t-1}),\\dots,C(w_{t-n+1})) $$ 函数$f$是这两个映射($C$和$g$)的组合，上下文中的所有单词都共享$C$。与这两个部分的每个部分关联一些参数。 数学符号说明： $C(i)$：单词$w$对应的词向量，其中$i$为词$w$在整个词汇表中的索引 $C$：词向量，大小为$\\vert V \\vert \\times m$的矩阵 $\\vert V \\vert$：词汇表的大小，即预料库中去重后的单词个数 $m$：词向量的维度，一般大于50 $H$：隐藏层的 weight $d$：隐藏层的 bias $U$：输出层的 weight $b$：输出层的 bias $W$：输入层到输出层的 weight $h$：隐藏层神经元个数 计算流程： 首先将输入的$n-1$个单词索引转为词向量，然后将这$n-1$个向量进行 concat，形成一个$(n-1)\\times w$ 的矩阵，用$X$表示 将$X$送入隐藏层进行计算，$\\textit{hidden}_\\text{out}=\\tanh{(d + X * H)}$ 输出层共有$\\vert V \\vert$个节点，每个节点$y_i$表示预测下一个单词$i$的概率， $y$的计算公式为$y=b+X*W+\\textit{hidden}_\\text{out} * U$ ","date":"2022-03-01","objectID":"/nnlm/:1:2","series":null,"tags":["Model","NLP","经典论文研读系列"],"title":"01-NNLM(’A Neural Probabilistic Language Model‘) ","uri":"/nnlm/#模型"},{"categories":["documentation"],"content":"代码 # code by Tae Hwan Jung @graykode, modify by wmathor import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as Data dtype = torch.FloatTensor sentences = [ \"i like dog\", \"i love coffee\", \"i hate milk\"] word_list = \" \".join(sentences).split() # ['i', 'like', 'dog', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk'] word_list = list(set(word_list)) # ['i', 'like', 'dog', 'love', 'coffee', 'hate', 'milk'] word_dict = {w: i for i, w in enumerate(word_list)} # {'i':0, 'like':1, 'dog':2, 'love':3, 'coffee':4, 'hate':5, 'milk':6} number_dict = {i: w for i, w in enumerate(word_list)} # {0:'i', 1:'like', 2:'dog', 3:'love', 4:'coffee', 5:'hate', 6:'milk'} n_class = len(word_dict) # number of Vocabulary, just like |V|, in this task n_class=7 # NNLM(Neural Network Language Model) Parameter n_step = len(sentences[0].split())-1 # n-1 in paper, look back n_step words and predict next word. In this task n_step=2 n_hidden = 2 # h in paper m = 2 # m in paper, word embedding dim def make_batch(sentences): input_batch = [] target_batch = [] for sen in sentences: word = sen.split() input = [word_dict[n] for n in word[:-1]] # [0, 1], [0, 3], [0, 5] target = word_dict[word[-1]] # 2, 4, 6 input_batch.append(input) # [[0, 1], [0, 3], [0, 5]] target_batch.append(target) # [2, 4, 6] return input_batch, target_batch input_batch, target_batch = make_batch(sentences) input_batch = torch.LongTensor(input_batch) target_batch = torch.LongTensor(target_batch) dataset = Data.TensorDataset(input_batch, target_batch) loader = Data.DataLoader(dataset=dataset, batch_size=16, shuffle=True) class NNLM(nn.Module): def __init__(self): super(NNLM, self).__init__() self.C = nn.Embedding(n_class, m) self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(dtype)) self.W = nn.Parameter(torch.randn(n_step * m, n_class).type(dtype)) self.d = nn.Parameter(torch.randn(n_hidden).type(dtype)) self.U = nn.Parameter(torch.randn(n_hidden, n_class).type(dtype)) self.b = nn.Parameter(torch.randn(n_class).type(dtype)) def forward(self, X): ''' X: [batch_size, n_step] ''' X = self.C(X) # [batch_size, n_step] =\u003e [batch_size, n_step, m] X = X.view(-1, n_step * m) # [batch_size, n_step * m] hidden_out = torch.tanh(self.d + torch.mm(X, self.H)) # [batch_size, n_hidden] output = self.b + torch.mm(X, self.W) + torch.mm(hidden_out, self.U) # [batch_size, n_class] return output model = NNLM() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=1e-3) # Training for epoch in range(5000): for batch_x, batch_y in loader: optimizer.zero_grad() output = model(batch_x) # output : [batch_size, n_class], batch_y : [batch_size] (LongTensor, not one-hot) loss = criterion(output, batch_y) if (epoch + 1)%1000 == 0: print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss)) loss.backward() optimizer.step() # Predict predict = model(input_batch).data.max(1, keepdim=True)[1] # Test print([sen.split()[:n_step] for sen in sentences], '-\u003e', [number_dict[n.item()] for n in predict.squeeze()]) ","date":"2022-03-01","objectID":"/nnlm/:2:0","series":null,"tags":["Model","NLP","经典论文研读系列"],"title":"01-NNLM(’A Neural Probabilistic Language Model‘) ","uri":"/nnlm/#代码"},{"categories":["documentation"],"content":"参考A Neural Probabilistic Language Model NNLM 的 PyTorch 实现 nlp-tutorial ","date":"2022-03-01","objectID":"/nnlm/:3:0","series":null,"tags":["Model","NLP","经典论文研读系列"],"title":"01-NNLM(’A Neural Probabilistic Language Model‘) ","uri":"/nnlm/#参考"},{"categories":null,"content":"关于我中国科学院大学计算机应用技术研究生一年级在读，主要研究方向是自然语言处理，欢迎邮件联系我进行交流👏🏻，特别欢迎carry我发论文！ ","date":"2022-02-28","objectID":"/about/:1:0","series":null,"tags":null,"title":"About","uri":"/about/#关于我"},{"categories":null,"content":"Tags👨‍💻 程序猿 💻 技术极客，热爱关于计算机的一切 🤪 强迫症与拖延症患者 🤔 数码爱好者 🍎 全家桶拥有者(bushi) ","date":"2022-02-28","objectID":"/about/:2:0","series":null,"tags":null,"title":"About","uri":"/about/#tags"},{"categories":null,"content":"关于版权本站所有的原创文章均受 创作共享 署名-非商业性 4.0 许可协议 / CC BY-NC 4.0 保护。 ","date":"2022-02-28","objectID":"/about/:0:0","series":null,"tags":null,"title":"About","uri":"/about/#关于版权"},{"categories":["documentation"],"content":"本教程希望为入门数据科学、DeepLearning的同学提供Numpy的基本操作指南。","date":"2022-02-28","objectID":"/numpyguidebook/","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/"},{"categories":["documentation"],"content":"本教程希望为入门数据科学、DeepLearning的同学提供Numpy的基本操作指南。 ","date":"2022-02-28","objectID":"/numpyguidebook/:0:0","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#"},{"categories":["documentation"],"content":"Numpy 入门指南","date":"2022-02-28","objectID":"/numpyguidebook/:1:0","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#numpy-入门指南"},{"categories":["documentation"],"content":"array基本属性Numpy的主要对象是同构多维数组。它是一个元素表，所有类型都相同，由非负整数元组构成索引。 Numpy的数组类被调用为ndarray。存在以下属性： ndarray.ndim：数组的轴（维度）的个数。 ndarray.shape：数组的维度。一个整数元组，表示每个维度中数组的大小。对于有n行和m列的矩阵，shape将是(n,m)，即shape元组长度就是rank或者维度的个数ndim。 ndarray.size：数组元素的总数。 ndarray.dtype： 一个描述数组中元素类型的对象 。 ndarray.itemsize：数组中每个元素的字节大小。例如，元素为 float64 类型的数组的 itemsize 为8（=64/8），而 complex32 类型的数组的 itemsize 为4（=32/8）。它等于 ndarray.dtype.itemsize 。 import numpy as np #如何将列表转化为矩阵 array=np.array([[1,2,3], [2,3,4]]) print(array) #查看维度ndim print('number of dim: ',array.ndim) ##output: number of dim: 2 #查看几行几列 print('shape: ',array.shape) ##output: shape: (2, 3) #查看元素个数 print('size: ',array.size) ##output: size: 6 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:1","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#array基本属性"},{"categories":["documentation"],"content":"创建数组np.array使用array函数从python元组中创建数组, 默认情况下，创建的数组的dtype是 float64 类型的。 import numpy as np #创建一维数组，ndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #创建二维数组 b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] 注意：常见错误是，调用array时候传入多个数字参数，而不提供单个数字的列表类型作为参数。 \u003e\u003e\u003e a = np.array(1,2,3,4) # WRONG \u003e\u003e\u003e a = np.array([1,2,3,4]) # RIGHT np.zeros创建一个全为0的数组 . \u003e\u003e\u003e np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.ones创建一个全为1的数组 . \u003e\u003e\u003e np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.empty创建一个数组，其初始内容是随机的，取决于内存的状态。 \u003e\u003e\u003e np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arange该函数返回指定范围内数组而不是列表 。（注意是左包含即[start,stop) ） numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) 主要参数：start–开始；step–结束；step:步长 \u003e\u003e\u003e np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) \u003e\u003e\u003e np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspace当arange与浮点参数一起使用时，由于有限的浮点精度，通常不可能预测所获得的元素的数量。出于这个原因，通常最好使用linspace函数来接收我们想要的元素数量的函数，而不是步长（step） def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): \u003e\u003e\u003e from numpy import pi \u003e\u003e\u003e np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) \u003e\u003e\u003e x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points \u003e\u003e\u003e f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#创建数组"},{"categories":["documentation"],"content":"创建数组np.array使用array函数从python元组中创建数组, 默认情况下，创建的数组的dtype是 float64 类型的。 import numpy as np #创建一维数组，ndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #创建二维数组 b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] 注意：常见错误是，调用array时候传入多个数字参数，而不提供单个数字的列表类型作为参数。 a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zeros创建一个全为0的数组 . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.ones创建一个全为1的数组 . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.empty创建一个数组，其初始内容是随机的，取决于内存的状态。 np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arange该函数返回指定范围内数组而不是列表 。（注意是左包含即[start,stop) ） numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) 主要参数：start–开始；step–结束；step:步长 np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspace当arange与浮点参数一起使用时，由于有限的浮点精度，通常不可能预测所获得的元素的数量。出于这个原因，通常最好使用linspace函数来接收我们想要的元素数量的函数，而不是步长（step） def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nparray"},{"categories":["documentation"],"content":"创建数组np.array使用array函数从python元组中创建数组, 默认情况下，创建的数组的dtype是 float64 类型的。 import numpy as np #创建一维数组，ndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #创建二维数组 b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] 注意：常见错误是，调用array时候传入多个数字参数，而不提供单个数字的列表类型作为参数。 a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zeros创建一个全为0的数组 . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.ones创建一个全为1的数组 . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.empty创建一个数组，其初始内容是随机的，取决于内存的状态。 np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arange该函数返回指定范围内数组而不是列表 。（注意是左包含即[start,stop) ） numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) 主要参数：start–开始；step–结束；step:步长 np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspace当arange与浮点参数一起使用时，由于有限的浮点精度，通常不可能预测所获得的元素的数量。出于这个原因，通常最好使用linspace函数来接收我们想要的元素数量的函数，而不是步长（step） def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npzeros"},{"categories":["documentation"],"content":"创建数组np.array使用array函数从python元组中创建数组, 默认情况下，创建的数组的dtype是 float64 类型的。 import numpy as np #创建一维数组，ndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #创建二维数组 b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] 注意：常见错误是，调用array时候传入多个数字参数，而不提供单个数字的列表类型作为参数。 a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zeros创建一个全为0的数组 . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.ones创建一个全为1的数组 . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.empty创建一个数组，其初始内容是随机的，取决于内存的状态。 np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arange该函数返回指定范围内数组而不是列表 。（注意是左包含即[start,stop) ） numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) 主要参数：start–开始；step–结束；step:步长 np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspace当arange与浮点参数一起使用时，由于有限的浮点精度，通常不可能预测所获得的元素的数量。出于这个原因，通常最好使用linspace函数来接收我们想要的元素数量的函数，而不是步长（step） def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npones"},{"categories":["documentation"],"content":"创建数组np.array使用array函数从python元组中创建数组, 默认情况下，创建的数组的dtype是 float64 类型的。 import numpy as np #创建一维数组，ndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #创建二维数组 b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] 注意：常见错误是，调用array时候传入多个数字参数，而不提供单个数字的列表类型作为参数。 a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zeros创建一个全为0的数组 . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.ones创建一个全为1的数组 . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.empty创建一个数组，其初始内容是随机的，取决于内存的状态。 np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arange该函数返回指定范围内数组而不是列表 。（注意是左包含即[start,stop) ） numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) 主要参数：start–开始；step–结束；step:步长 np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspace当arange与浮点参数一起使用时，由于有限的浮点精度，通常不可能预测所获得的元素的数量。出于这个原因，通常最好使用linspace函数来接收我们想要的元素数量的函数，而不是步长（step） def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npempty"},{"categories":["documentation"],"content":"创建数组np.array使用array函数从python元组中创建数组, 默认情况下，创建的数组的dtype是 float64 类型的。 import numpy as np #创建一维数组，ndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #创建二维数组 b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] 注意：常见错误是，调用array时候传入多个数字参数，而不提供单个数字的列表类型作为参数。 a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zeros创建一个全为0的数组 . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.ones创建一个全为1的数组 . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.empty创建一个数组，其初始内容是随机的，取决于内存的状态。 np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arange该函数返回指定范围内数组而不是列表 。（注意是左包含即[start,stop) ） numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) 主要参数：start–开始；step–结束；step:步长 np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspace当arange与浮点参数一起使用时，由于有限的浮点精度，通常不可能预测所获得的元素的数量。出于这个原因，通常最好使用linspace函数来接收我们想要的元素数量的函数，而不是步长（step） def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nparange"},{"categories":["documentation"],"content":"创建数组np.array使用array函数从python元组中创建数组, 默认情况下，创建的数组的dtype是 float64 类型的。 import numpy as np #创建一维数组，ndim=1 a=np.array([2,23,4],dtype=np.int32) print(a) ##output:[ 2 23 4] #创建二维数组 b = np.array([(1.5,2,3), (4,5,6)]) print(b) ##output: [[ 1.5 2. 3. ] ## [ 4. 5. 6. ]] 注意：常见错误是，调用array时候传入多个数字参数，而不提供单个数字的列表类型作为参数。 a = np.array(1,2,3,4) # WRONG a = np.array([1,2,3,4]) # RIGHT np.zeros创建一个全为0的数组 . np.zeros( (3,4) ) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) np.ones创建一个全为1的数组 . np.ones((2,3,4), dtype=np.int16) # dtype can also be specified array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) np.empty创建一个数组，其初始内容是随机的，取决于内存的状态。 np.empty( (2,3) ) # uninitialized, output may vary array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) np.arange该函数返回指定范围内数组而不是列表 。（注意是左包含即[start,stop) ） numpy.arange([start, ]stop, [step, ]dtype=None, *, like=None) 主要参数：start–开始；step–结束；step:步长 np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) np.arange( 0, 2, 0.3 ) # it accepts float arguments array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.linspace当arange与浮点参数一起使用时，由于有限的浮点精度，通常不可能预测所获得的元素的数量。出于这个原因，通常最好使用linspace函数来接收我们想要的元素数量的函数，而不是步长（step） def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,axis=0): from numpy import pi np.linspace( 0, 2, 9 )# 9 numbers from 0 to 2 array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) x = np.linspace( 0, 2*pi, 100 )# useful to evaluate function at lots of points f = np.sin(x) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:2","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nplinspace"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到\u003e、\u003c、==、\u003e=、 \u003c= 、!=，返回一个全为布尔值的数组 import numpy as np b=np.arange(4) ##output：[0 1 2 3] #快速查找符合要求的值,逻辑判断 print(b==3,'\\n') #output :[False False False True] print(b!=3,'\\n') #output：[ True True True False] 转秩 import numpy as np B=np.arange(14,2, -1).reshape((3,4)) # B :array([[14, 13, 12, 11], # [10, 9, 8, 7], # [ 6, 5, 4, 3]]) print(np.transpose(B)) #[[14 10 6] # [13 9 5] # [12 8 4] # [11 7 3]] print(B.T) #[[14 10 6] # [13 9 5] # [12 8 4] # [11 7 3]] np.sort对矩阵中的所有值从大到小排序。 #排序函数，sort(),针对每一行进行从小到大排序操作 B=np.arange(14,2, -1).reshape((3,4)) # B :array([[14, 13, 12, 11], # [10, 9, 8, 7], # [ 6, 5, 4, 3]]) print(np.sort(B)) # B':array([[11,12,13,14], # [ 7, 8, 9,10], # [ 3, 4, 5, 6]]) np.clipclip函数，clip(Array,Array_min,Array_max)，Array指的是将要被执行用的矩阵，而后面的最小值最大值则用于让函数判断矩阵中元素是否有比最小值小的或者比最大值大的元素，并将这些指定的元素转换为最小值或者最大值。 import numpy as np A=np.arange(2,14).reshape((3,4)) print(np.clip(A,5,9)) np.argmin查找矩阵中的最小值的索引值 np.argmax查找矩阵中的最大值的索引值 import numpy as np A=np.arange(2,14).reshape((3,4)) #[[ 2 3 4 5] # [ 6 7 8 9] # [10 11 12 13]] #numpy基本运算 print(A) #求矩阵中最小元素 print('最小值的索引值',np.argmin(A)) ##最小值的索引值 0 #求矩阵中最大元素 print('最大值的索引值',np.argmax(A)) #最大值的索引值 11 np.mean求矩阵所有值的均值,亦写成A.mean() 同np.average( ) np.average import numpy as np A=np.arange(2,14).reshape((3,4)) #求矩阵的均值 print('矩阵平均值表示之一',np.mean(A),'|',A.mean()) #矩阵平均值表示之一 7.5 | 7.5 print('矩阵平均值表示之二',np.average(A)) #矩阵平均值表示之二 7.5 np.cumsum import numpy as np A=np.arange(2,14).reshape((3,4)) #求矩阵n项累加 #eg: array([ [ 2, 3, 4, 5] # [ 6, 7, 8, 9] # [10,11,12,13] ]) # ---\u003e[2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], ---\u003e [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 \u003e\u003e\u003e b=np.array([2,4,6]) \u003e\u003e\u003e np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 \u003e\u003e\u003e c=np.array([4,9,16]) \u003e\u003e\u003e np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#数组基本运算"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#加减运算"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#点乘叉乘"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#乘方"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#逻辑运算"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#转秩"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npsort"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npclip"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npargmin"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npargmax"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npmean"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npaverage"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npcumsum"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npdiff"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npexp"},{"categories":["documentation"],"content":"数组基本运算加减运算 import numpy as np #加减运算 a=np.array([10,20,30,40]) b=np.arange(4) print(a,b) ##[10 20 30 40] [0 1 2 3] c=a+b d=a-b print(c,d) ##[10 21 32 43] [10 19 28 37] 点乘、叉乘 import numpy as np a=np.array([10,20,30,40]) b=np.arange(4) #叉乘 c=a*b print(\"\\n叉乘运算:\",c) ##output:叉乘运算: [ 0 20 60 120] #点乘 aa=np.array([[1,1],[0,1]]) bb=np.arange(4).reshape((2,2)) c_dot=np.dot(aa,bb) c_dot_2=aa.dot(bb) print(\"\\n点乘运算之一:\",c_dot) ##点乘运算之一: [[2 4] ## [2 3]] print(\"\\n点乘运算之二:\",c_dot_2) ##点乘运算之二: [[2 4] ## [2 3]] 乘方使用a**b表示a的b次方 import numpy as np b=np.arange(4) #乘方运算 f=b**2 print(\"\\n乘方运算:\",f) #output:[0 1 4 9] 逻辑运算快速查找数组中符合条件的值，涉及到、=、 [2 5 9 14 20 27 35 44 54 65 77 90] print('矩阵前n项累加',np.cumsum(A)) np.diff import numpy as np A=np.arange(2,14).reshape((3,4)) #累差运算函数diff,计算的便是每一行中后一项与前一项之差. #eg: array([ [ 2, 3, 4, 5], array([[1,1,1], # [ 6, 7, 8, 9], --- [1,1,1], # [10,11,12,13] ]) [1,1,1]]) print(np.diff(A)) np.exp求e的幂次方。 b=np.array([2,4,6]) np.exp(b) array([ 7.3890561 , 54.59815003, 403.42879349]) np.sqrt开方函数 c=np.array([4,9,16]) np.sqrt(c) array([2., 3., 4.]) ","date":"2022-02-28","objectID":"/numpyguidebook/:1:3","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npsqrt"},{"categories":["documentation"],"content":"索引、切片和迭代一维数组一维的数组可以进行索引、切片和迭代操作。 \u003e\u003e\u003e import numpy as np \u003e\u003e\u003e a=np.arange(10)**3 \u003e\u003e\u003e a array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) \u003e\u003e\u003e a[2] #获取第二个值 8 \u003e\u003e\u003e a[2:5] #获取第二到第五个值，以数组形式返回 array([ 8, 27, 64], dtype=int32) \u003e\u003e\u003e a[:6:2]=-1000 #修改第零个、第二个、第六个值为-1000 \u003e\u003e\u003e a array([ -1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729], dtype=int32) \u003e\u003e\u003e a[ : :-1] #倒序a array([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000], dtype=int32) \u003e\u003e\u003e for i in a: ... print(i**(1/3.)) ... nan 1.0 nan 3.0 nan 5.0 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 多维数组多维数组的每一个轴都有一个索引，这些索引以逗号的形式分隔的元组给出： \u003e\u003e\u003e def f(x,y): ... return 5*x+y ... \u003e\u003e\u003e b=np.fromfunction(f,(5,4),dtype=int) \u003e\u003e\u003e b array([[ 0, 1, 2, 3], [ 5, 6, 7, 8], [10, 11, 12, 13], [15, 16, 17, 18], [20, 21, 22, 23]]) \u003e\u003e\u003e b[2,3] #第二行第三列的数字 13 \u003e\u003e\u003e b[0:5,1] #第0~5行第1列的数字，以数组形式返回 array([ 1, 6, 11, 16, 21]) \u003e\u003e\u003e b[ : ,1] #第1列的数字，以数组形式返回 array([ 1, 6, 11, 16, 21]) \u003e\u003e\u003e b[1:3,:] #第1~3行的数字，以数组形式返回 array([[ 5, 6, 7, 8], [10, 11, 12, 13]]) 对多维数组进行迭代（iterating）是相对于第一个轴完成的。 \u003e\u003e\u003e for row in b: ... print(row) ... [0 1 2 3] [5 6 7 8] [10 11 12 13] [15 16 17 18] [20 21 22 23] 迭代操作如果想要对数组中的每个元素执行操作，可以使用flat属性，该属性是数组的所有元素的迭代器 : \u003e\u003e\u003e for element in b.flat: ... print(element) ... 0 1 2 3 5 6 7 8 10 11 12 13 15 16 17 18 20 21 22 23 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:4","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#索引切片和迭代"},{"categories":["documentation"],"content":"索引、切片和迭代一维数组一维的数组可以进行索引、切片和迭代操作。 import numpy as np a=np.arange(10)**3 a array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) a[2] #获取第二个值 8 a[2:5] #获取第二到第五个值，以数组形式返回 array([ 8, 27, 64], dtype=int32) a[:6:2]=-1000 #修改第零个、第二个、第六个值为-1000 a array([ -1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729], dtype=int32) a[ : :-1] #倒序a array([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000], dtype=int32) for i in a: ... print(i**(1/3.)) ... nan 1.0 nan 3.0 nan 5.0 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 多维数组多维数组的每一个轴都有一个索引，这些索引以逗号的形式分隔的元组给出： def f(x,y): ... return 5*x+y ... b=np.fromfunction(f,(5,4),dtype=int) b array([[ 0, 1, 2, 3], [ 5, 6, 7, 8], [10, 11, 12, 13], [15, 16, 17, 18], [20, 21, 22, 23]]) b[2,3] #第二行第三列的数字 13 b[0:5,1] #第0~5行第1列的数字，以数组形式返回 array([ 1, 6, 11, 16, 21]) b[ : ,1] #第1列的数字，以数组形式返回 array([ 1, 6, 11, 16, 21]) b[1:3,:] #第1~3行的数字，以数组形式返回 array([[ 5, 6, 7, 8], [10, 11, 12, 13]]) 对多维数组进行迭代（iterating）是相对于第一个轴完成的。 for row in b: ... print(row) ... [0 1 2 3] [5 6 7 8] [10 11 12 13] [15 16 17 18] [20 21 22 23] 迭代操作如果想要对数组中的每个元素执行操作，可以使用flat属性，该属性是数组的所有元素的迭代器 : for element in b.flat: ... print(element) ... 0 1 2 3 5 6 7 8 10 11 12 13 15 16 17 18 20 21 22 23 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:4","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#一维数组"},{"categories":["documentation"],"content":"索引、切片和迭代一维数组一维的数组可以进行索引、切片和迭代操作。 import numpy as np a=np.arange(10)**3 a array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) a[2] #获取第二个值 8 a[2:5] #获取第二到第五个值，以数组形式返回 array([ 8, 27, 64], dtype=int32) a[:6:2]=-1000 #修改第零个、第二个、第六个值为-1000 a array([ -1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729], dtype=int32) a[ : :-1] #倒序a array([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000], dtype=int32) for i in a: ... print(i**(1/3.)) ... nan 1.0 nan 3.0 nan 5.0 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 多维数组多维数组的每一个轴都有一个索引，这些索引以逗号的形式分隔的元组给出： def f(x,y): ... return 5*x+y ... b=np.fromfunction(f,(5,4),dtype=int) b array([[ 0, 1, 2, 3], [ 5, 6, 7, 8], [10, 11, 12, 13], [15, 16, 17, 18], [20, 21, 22, 23]]) b[2,3] #第二行第三列的数字 13 b[0:5,1] #第0~5行第1列的数字，以数组形式返回 array([ 1, 6, 11, 16, 21]) b[ : ,1] #第1列的数字，以数组形式返回 array([ 1, 6, 11, 16, 21]) b[1:3,:] #第1~3行的数字，以数组形式返回 array([[ 5, 6, 7, 8], [10, 11, 12, 13]]) 对多维数组进行迭代（iterating）是相对于第一个轴完成的。 for row in b: ... print(row) ... [0 1 2 3] [5 6 7 8] [10 11 12 13] [15 16 17 18] [20 21 22 23] 迭代操作如果想要对数组中的每个元素执行操作，可以使用flat属性，该属性是数组的所有元素的迭代器 : for element in b.flat: ... print(element) ... 0 1 2 3 5 6 7 8 10 11 12 13 15 16 17 18 20 21 22 23 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:4","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#多维数组"},{"categories":["documentation"],"content":"索引、切片和迭代一维数组一维的数组可以进行索引、切片和迭代操作。 import numpy as np a=np.arange(10)**3 a array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729], dtype=int32) a[2] #获取第二个值 8 a[2:5] #获取第二到第五个值，以数组形式返回 array([ 8, 27, 64], dtype=int32) a[:6:2]=-1000 #修改第零个、第二个、第六个值为-1000 a array([ -1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729], dtype=int32) a[ : :-1] #倒序a array([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000], dtype=int32) for i in a: ... print(i**(1/3.)) ... nan 1.0 nan 3.0 nan 5.0 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 多维数组多维数组的每一个轴都有一个索引，这些索引以逗号的形式分隔的元组给出： def f(x,y): ... return 5*x+y ... b=np.fromfunction(f,(5,4),dtype=int) b array([[ 0, 1, 2, 3], [ 5, 6, 7, 8], [10, 11, 12, 13], [15, 16, 17, 18], [20, 21, 22, 23]]) b[2,3] #第二行第三列的数字 13 b[0:5,1] #第0~5行第1列的数字，以数组形式返回 array([ 1, 6, 11, 16, 21]) b[ : ,1] #第1列的数字，以数组形式返回 array([ 1, 6, 11, 16, 21]) b[1:3,:] #第1~3行的数字，以数组形式返回 array([[ 5, 6, 7, 8], [10, 11, 12, 13]]) 对多维数组进行迭代（iterating）是相对于第一个轴完成的。 for row in b: ... print(row) ... [0 1 2 3] [5 6 7 8] [10 11 12 13] [15 16 17 18] [20 21 22 23] 迭代操作如果想要对数组中的每个元素执行操作，可以使用flat属性，该属性是数组的所有元素的迭代器 : for element in b.flat: ... print(element) ... 0 1 2 3 5 6 7 8 10 11 12 13 15 16 17 18 20 21 22 23 ","date":"2022-02-28","objectID":"/numpyguidebook/:1:4","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#迭代操作"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 \u003e\u003e\u003e a=np.floor(10*np.random.random((3,4))) \u003e\u003e\u003e a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) \u003e\u003e\u003e a.shape (3, 4) \u003e\u003e\u003e a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) \u003e\u003e\u003e a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 \u003e\u003e\u003e a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) \u003e\u003e\u003e a.T.shape (4, 3) array.reshape()改变为任意形状 。 \u003e\u003e\u003e a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 \u003e\u003e\u003e a array([[0, 1], [2, 3], [4, 5]]) \u003e\u003e\u003e np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) \u003e\u003e\u003e a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 \u003e\u003e\u003e a=np.arange(12).reshape(3,4) \u003e\u003e\u003e a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) \u003e\u003e\u003e a.resize((2,6)) \u003e\u003e\u003e a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) \u003e\u003e\u003e a = np.array([[1, 2], [3, 4]]) \u003e\u003e\u003ea \u003e\u003e\u003earray([[1, 2], [3, 4]]) \u003e\u003e\u003e b = np.array([[5, 6]]) \u003e\u003e\u003e b array([[5, 6]]) \u003e\u003e\u003e np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) \u003e\u003e\u003e np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) \u003e\u003e\u003e np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 \u003e\u003e\u003e x = np.arange(16.0).reshape(4, 4) \u003e\u003e\u003e x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) \u003e\u003e\u003e np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 \u003e\u003e\u003e x = np.arange(16.0).reshape(4, 4) \u003e\u003e\u003e x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) \u003e\u003e\u003e np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 \u003e\u003e\u003e x = np.arange(8.0) \u003e\u003e\u003e np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#array形状操作"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#改变数组的形状"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayravel"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayt"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayreshape"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#arrayresize-"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#堆叠数组"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npvstack"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nphstack"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npconcatenate"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#分割数组"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#numpysplit"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nphsplit"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#npvsplit"},{"categories":["documentation"],"content":"array形状操作改变数组的形状array.ravel()化成1*n的矩阵。 a=np.floor(10*np.random.random((3,4))) a array([[9., 8., 7., 4.], [5., 3., 5., 9.], [9., 4., 0., 0.]]) a.shape (3, 4) a.ravel() array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) a.reshape(-1) array([9., 8., 7., 4., 5., 3., 5., 9., 9., 4., 0., 0.]) Ps: array.ravel()作用等同于array.reshape(-1) array.T转置矩阵 。 a.T array([[9., 5., 9.], [8., 3., 4.], [7., 5., 0.], [4., 9., 0.]]) a.T.shape (4, 3) array.reshape()改变为任意形状 。 a = np.arange(6).reshape((3, 2))#将1*6矩阵转为3*2矩阵 a array([[0, 1], [2, 3], [4, 5]]) np.reshape(a, (2, 3)) #将3*2矩阵转为2*3矩阵 array([[0, 1, 2], [3, 4, 5]]) a.reshape(2,-1) #reshape操作中将size指定为-1，则会自动计算其他的size大小： array([[0, 1, 2], [3, 4, 5]]) array.resize( )该方法会直接修改数组本身的shape和size。 a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) a.resize((2,6)) a array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) 堆叠数组np.vstack属于一种上下合并的情况。 import numpy as np #合并Array A=np.array([1,1,1]) B=np.array([2,2,2]) #vstack:属于一种上下合并 print(np.vstack((A,B))) #Vertical stack #output: [[1 1 1] # [2 2 2]] np.hstack属于一种左右合并的情况 import numpy as np A=np.array([1,1,1]) B=np.array([2,2,2]) D=np.hstack((A,B)) print(D) #[1 1 1 2 2 2] E=np.hstack((B,A)) print(E) #[2 2 2 1 1 1] np.concatenate #针对多个矩阵或序列进行合并操作，借助 # np.concatenate((A,A,A,...),axis=0 或 1) a = np.array([[1, 2], [3, 4]]) a array([[1, 2], [3, 4]]) b = np.array([[5, 6]]) b array([[5, 6]]) np.concatenate((a, b), axis=0)#合并列 array([[1, 2], [3, 4], [5, 6]]) np.concatenate((a, b.T), axis=1) #合并行 array([[1, 2, 5], [3, 4, 6]]) np.concatenate((a, b), axis=None) array([1, 2, 3, 4, 5, 6]) 分割数组numpy.split import numpy as np A=np.arange(12).reshape((3,4)) print(A) #分割函数np.split(array,number of split row/column,axis= 0 or 1) print(np.split(A,2,axis=1))#把四列分成2块（2列一块） # [array([ [0, 1], # [4, 5], # [8, 9]]), array([[ 2, 3], # [ 6, 7], # [10, 11]])] #axis=0,表示按行分割；axis=1,表示按列分割 print(np.split(A,3,axis=0)) #把三行按行分成3块（一行一块） #[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] np.hsplit按列拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.hsplit(x, 2) [array([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), array([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])] np.vsplit按行拆开数组。 x = np.arange(16.0).reshape(4, 4) x array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) np.vsplit(x, 2) [array([[0., 1., 2., 3.], [4., 5., 6., 7.]]), array([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])] np.array_split将一个数组拆分为大小相等或近似相等的多个子数组。如果无法进行均等划分，则不会引发异常。 x = np.arange(8.0) np.array_split(x, 3) [array([0., 1., 2.]), array([3., 4., 5.]), array([6., 7.])] ","date":"2022-02-28","objectID":"/numpyguidebook/:1:5","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#nparray_split"},{"categories":["documentation"],"content":"拷贝和深拷贝当计算和操作数组时，有时会将数据复制到新数组中，有时则不会 。 存在以下3种情况： 完全不复制简单分配不会复制数组对象或其数据。 import numpy as np a=np.arange(4) # =的赋值方式会带有关联性 b=a c=a d=b #改变a的第一个值，b、c、d的第一个值也会同时改变。 浅拷贝不同的数组对象可以共享相同的数据。view方法创建一个查看相同数据的新数组对象。 \u003e\u003e\u003e import numpy as np \u003e\u003e\u003e a=np.arange(12).reshape(3,4) \u003e\u003e\u003e a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) \u003e\u003e\u003e c=a.view() \u003e\u003e\u003e c is a False \u003e\u003e\u003e c.base is a False \u003e\u003e\u003e c array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) \u003e\u003e\u003e c.shape = 2,6 \u003e\u003e\u003e c array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) \u003e\u003e\u003e a.shape (3, 4) \u003e\u003e\u003e c[0,4] = 1234 \u003e\u003e\u003e a array([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]]) \u003e\u003e\u003e c array([[ 0, 1, 2, 3, 1234, 5], [ 6, 7, 8, 9, 10, 11]]) 深拷贝copy()该copy方法生成数组及其数据的完整副本。 import numpy as np a=np.arange(4) #copy()的赋值方式没有关联性 b=a.copy() print(b) a[3]=45 print('a:',a) #a: [11 1 2 45] print('b:',b) #b: [11 1 2 3] @all right save,ZhangGehang. ","date":"2022-02-28","objectID":"/numpyguidebook/:1:6","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#拷贝和深拷贝"},{"categories":["documentation"],"content":"拷贝和深拷贝当计算和操作数组时，有时会将数据复制到新数组中，有时则不会 。 存在以下3种情况： 完全不复制简单分配不会复制数组对象或其数据。 import numpy as np a=np.arange(4) # =的赋值方式会带有关联性 b=a c=a d=b #改变a的第一个值，b、c、d的第一个值也会同时改变。 浅拷贝不同的数组对象可以共享相同的数据。view方法创建一个查看相同数据的新数组对象。 import numpy as np a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c=a.view() c is a False c.base is a False c array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c.shape = 2,6 c array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) a.shape (3, 4) c[0,4] = 1234 a array([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]]) c array([[ 0, 1, 2, 3, 1234, 5], [ 6, 7, 8, 9, 10, 11]]) 深拷贝copy()该copy方法生成数组及其数据的完整副本。 import numpy as np a=np.arange(4) #copy()的赋值方式没有关联性 b=a.copy() print(b) a[3]=45 print('a:',a) #a: [11 1 2 45] print('b:',b) #b: [11 1 2 3] @all right save,ZhangGehang. ","date":"2022-02-28","objectID":"/numpyguidebook/:1:6","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#完全不复制"},{"categories":["documentation"],"content":"拷贝和深拷贝当计算和操作数组时，有时会将数据复制到新数组中，有时则不会 。 存在以下3种情况： 完全不复制简单分配不会复制数组对象或其数据。 import numpy as np a=np.arange(4) # =的赋值方式会带有关联性 b=a c=a d=b #改变a的第一个值，b、c、d的第一个值也会同时改变。 浅拷贝不同的数组对象可以共享相同的数据。view方法创建一个查看相同数据的新数组对象。 import numpy as np a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c=a.view() c is a False c.base is a False c array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c.shape = 2,6 c array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) a.shape (3, 4) c[0,4] = 1234 a array([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]]) c array([[ 0, 1, 2, 3, 1234, 5], [ 6, 7, 8, 9, 10, 11]]) 深拷贝copy()该copy方法生成数组及其数据的完整副本。 import numpy as np a=np.arange(4) #copy()的赋值方式没有关联性 b=a.copy() print(b) a[3]=45 print('a:',a) #a: [11 1 2 45] print('b:',b) #b: [11 1 2 3] @all right save,ZhangGehang. ","date":"2022-02-28","objectID":"/numpyguidebook/:1:6","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#浅拷贝"},{"categories":["documentation"],"content":"拷贝和深拷贝当计算和操作数组时，有时会将数据复制到新数组中，有时则不会 。 存在以下3种情况： 完全不复制简单分配不会复制数组对象或其数据。 import numpy as np a=np.arange(4) # =的赋值方式会带有关联性 b=a c=a d=b #改变a的第一个值，b、c、d的第一个值也会同时改变。 浅拷贝不同的数组对象可以共享相同的数据。view方法创建一个查看相同数据的新数组对象。 import numpy as np a=np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c=a.view() c is a False c.base is a False c array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) c.shape = 2,6 c array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) a.shape (3, 4) c[0,4] = 1234 a array([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]]) c array([[ 0, 1, 2, 3, 1234, 5], [ 6, 7, 8, 9, 10, 11]]) 深拷贝copy()该copy方法生成数组及其数据的完整副本。 import numpy as np a=np.arange(4) #copy()的赋值方式没有关联性 b=a.copy() print(b) a[3]=45 print('a:',a) #a: [11 1 2 45] print('b:',b) #b: [11 1 2 3] @all right save,ZhangGehang. ","date":"2022-02-28","objectID":"/numpyguidebook/:1:6","series":null,"tags":["numpy","ML"],"title":"NumpyGuidebook","uri":"/numpyguidebook/#深拷贝copy"}]