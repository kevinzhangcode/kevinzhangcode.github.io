# Transformers Domain Adaptation




## Transformers Domain Adaptation

æœ¬æŒ‡å—è¯´æ˜äº†ç«¯åˆ°ç«¯`Domain Adaptation`å·¥ä½œæµç¨‹ï¼Œå…¶ä¸­æˆ‘ä»¬ä¸ºç”Ÿç‰©åŒ»å­¦NLPåº”ç”¨ç¨‹åºé€‚åº”é¢†åŸŸè½¬æ¢æ¨¡å‹ã€‚

å®ƒå±•ç¤ºäº†æˆ‘ä»¬åœ¨ç ”ç©¶ä¸­ç ”ç©¶çš„ä¸¤ç§é¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯:
1. æ•°æ®é€‰æ‹© (Data Selection)
2. è¯æ±‡é‡å¢åŠ  (Vocabulary Augmentation)

æ¥ä¸‹æ¥ï¼Œå°†æ¼”ç¤ºè¿™æ ·ä¸€ä¸ª`Domain Adaptation`çš„Transformeræ¨¡å‹æ˜¯å¦‚ä½•ä¸ğŸ¤—`transformer`çš„åŸ¹è®­ç•Œé¢å…¼å®¹çš„ï¼Œä»¥åŠå®ƒå¦‚ä½•ä¼˜äºå¼€ç®±å³ç”¨çš„(æ— é¢†åŸŸé€‚åº”çš„)æ¨¡å‹ã€‚è¿™äº›æŠ€æœ¯åº”ç”¨äºBERT-smallï¼Œä½†æ˜¯ä»£ç åº“è¢«ç¼–å†™æˆå¯æ¨å¹¿åˆ°[HuggingFace](https://huggingface.co/)æ”¯æŒçš„å…¶ä»–Transformerç±»ã€‚

#### è­¦å‘Š

å¯¹äºæœ¬æŒ‡å—ï¼Œç”±äºå†…å­˜å’Œæ—¶é—´çš„é™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸŸå†…è¯­æ–™åº“çš„ä¸€ä¸ªå°å¾—å¤šçš„å­é›†(<0.05%)ã€‚



### å‡†å¤‡å·¥ä½œ

#### å®‰è£…ä¾èµ–ç¨‹åº

ä½¿ç”¨`pip`å®‰è£…`transformers-domain-adaptation`

```shell
pip install transformers-domain-adaptation -i https://pypi.tuna.tsinghua.edu.cn/simple
```



#### ä¸‹è½½demo files

```shell
wget http://georgian-toolkit.s3.amazonaws.com/transformers-domain-adaptation/colab/files.zip

unzip ./files.zip
```



#### å¸¸é‡

æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ¨¡å‹å¡å’Œæ–‡æœ¬è¯­æ–™åº“çš„ç›¸å…³è·¯å¾„ã€‚

åœ¨`domain adaptation`çš„èƒŒæ™¯ä¸‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„è¯­æ–™åº“ã€‚

1. å¾®è°ƒè¯­æ–™åº“(Fine-Tuning Corpus)
>ç»™å®šä¸€ä¸ªNLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç­‰ï¼‰ï¼Œè¿™ä¸ªæ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†å°±æ˜¯å¾®è°ƒè¯­æ–™åº“ã€‚

2. åœ¨åŸŸè¯­æ–™åº“ (In-Domain Corpus)
>è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºé¢†åŸŸé¢„è®­ç»ƒã€‚æ–‡æœ¬é¢†åŸŸä¸å¾®è°ƒè¯­æ–™åº“çš„é¢†åŸŸç›¸åŒï¼Œç”šè‡³æ›´å¹¿æ³›ã€‚



```python
# é¢„è®­ç»ƒæ¨¡å‹åç§°
model_card = 'bert-base-uncased'

# Domain-pre-training corpora é¢†åŸŸé¢„è®­ç»ƒè¯­æ–™
dpt_corpus_train = './data/pubmed_subset_train.txt'
dpt_corpus_train_data_selected = './data/pubmed_subset_train_data_selected.txt'
dpt_corpus_val = './data/pubmed_subset_val.txt'

# Fine-tuning corpora
# If there are multiple downstream NLP tasks/corpora, you can concatenate those files together
ft_corpus_train = './data/BC2GM_train.txt'
```



#### åŠ è½½æ¨¡å‹å’Œtokenizer

```python
from transformers import AutoModelForMaskedLM, AutoTokenizer

model = AutoModelForMaskedLM.from_pretrained(model_card)
tokenizer = AutoTokenizer.from_pretrained(model_card)
```





#### æ•°æ®é€‰æ‹©

åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é¢†åŸŸå†…è¯­æ–™åº“çš„æ•°æ®éƒ½å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„æˆ–ç›¸å…³çš„ã€‚å¯¹äºä¸ç›¸å…³çš„æ–‡ä»¶ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸ä¼šé™ä½é¢†åŸŸé€‚åº”æ¨¡å‹çš„æ€§èƒ½ï¼›åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šå€’é€€å¹¶å¤±å»å®è´µçš„é¢„è®­ç»ƒä¿¡æ¯å³`ç¾éš¾æ€§çš„é—å¿˜`ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨[Ruder & Plank](https://aclanthology.org/D17-1038.pdf)è®¾è®¡çš„å„ç§ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»åŸŸå†…è¯­æ–™åº“ä¸­é€‰æ‹©å¯èƒ½ä¸ä¸‹æ¸¸å¾®è°ƒæ•°æ®é›†ç›¸å…³çš„æ–‡æ¡£ã€‚



```python
from pathlib import Path

from transformers_domain_adaptation import DataSelector


selector = DataSelector(
    keep=0.5,  # TODO Replace with `keep`
    tokenizer=tokenizer,
    similarity_metrics=['euclidean'],
    diversity_metrics=[
        "type_token_ratio",
        "entropy",
    ],
)
```



```python
#  å°†æ–‡æœ¬æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­
fine_tuning_texts = Path(ft_corpus_train).read_text().splitlines()
training_texts = Path(dpt_corpus_train).read_text().splitlines()


#  åœ¨å¾®è°ƒè¯­æ–™åº“è¿›è¡Œfit
selector.fit(fine_tuning_texts)

# ä»åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸­é€‰æ‹©ç›¸å…³æ–‡ä»¶
selected_corpus = selector.transform(training_texts)

# åœ¨`dpt_corpus_train_data_selected`ä¸‹å°†é€‰å®šçš„è¯­æ–™åº“ä¿å­˜åˆ°ç£ç›˜ 
Path(dpt_corpus_train_data_selected).write_text('\n'.join(selected_corpus));
```

ç”±äºæˆ‘ä»¬åœ¨`DataSelector`ä¸­æŒ‡å®šäº†`keep=0.5`ï¼Œæ‰€ä»¥é€‰æ‹©çš„`è¯­æ–™åº“åº”è¯¥æ˜¯åŸŸå†…è¯­æ–™åº“çš„ä¸€åŠå¤§å°`ï¼ŒåŒ…å«å‰50%æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚



```python
print(len(training_texts), len(selected_corpus))
# (10000, 5000)

print(selected_corpus[0])
```

```shell
Chlorophyll content,leaf mass to per area,net photosynthetic rate and bioactive ingredients of Asarum heterotropoides var. mandshuricum,a skiophyte grown in four levels of solar irradiance were measured and analyzed in order to investigate the response of photosynthetic capability to light irradiance and other environmental factors. It suggested that the leaf mass to per area of plant was greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in the decrease of leaf mass to per area at every phenological stage. At expanding leaf stage,the rate of Chla and Chlb was 3. 11 when A. heterotropoides var. mandshuricum grew in full light irradiance which is similar to the rate of heliophytes,however,the rate of Chla and Chlb was below to 3. 0 when they grew in shading environment. The content of Chla,Chlb and Chl( a+b) was the greatest value of four kinds of light irradiance and decreasing intensity of solar irradiance resulted in its decreasing remarkably( P<0. 05). The rate of Chla and Chlb decreased but the content of Chla,Chlb and Chl( a+b) increased gradually with continued shading. The maximum value of photosynthetically active radiation appeared at 10: 00-12: 00 am in a day. The maximum value of net photosynthetic rate appeared at 8: 30-9: 00 am and the minimum value appeared at 14: 00-14: 30 pm at each phenological stage if plants grew in full sunlight. However,when plants grew in shading,the maximum value of net photosynthetic rate appeared at about 10: 30 am and the minimum value appeared at 12: 20-12: 50 pm at each phenological stage. At expanding leaf stage and flowering stage,the average of net photosynthetic rate of leaves in full sunlight was remarkably higher than those in shading and it decreased greatly with decreasing of irradiance gradually( P < 0. 05). However,at fruiting stage,the average of net photosynthetic rate of leaves in full sunlight was lower than those in 50% and 28% full sunlight but higher than those in 12% full sunlight. All photosynthetic diurnal variation parameters of plants measured in four kinds of different irradiance at three stages were used in correlation analysis. The results suggested that no significant correlation was observed between net photosynthetic rate and photosynthetically active radiation,and significant negative correlation was observed between net photosynthetic rate and environmental temperature as well as vapor pressure deficit expect for 12% full sunlight. Positive correlation was observed between net photosynthestic rate and relative humidity expect for 12% full sunlight. Significant positive correlation was observed between net photosynthetic rate and stomatal conductance in the four light treatments. Only,in 12% full sunlight,the net photosynthetic rate was significantly related to photosynthetically active radiation rather than related to environmental temperature,vapor pressure deficit and relative humidity. In each light treatment,a significant positive correlation was observed between environmental temperature and vapor pressure deficit,relative humidity as well as stomatal conductance. Volatile oil content was 1. 46%,2. 16%,1. 56%,1. 30% respectively. ethanol extracts was 23. 44%,22. 45%,22. 18%,21. 12% respectively. Asarinin content was 0. 281%,0. 291%,0. 279% and 0. 252% respectively. The characteristic components of Asarum volatile oil of plant in different light treatments did not change significantly among different groups.
```

#### è¯æ±‡æ‰©å……

æˆ‘ä»¬å¯ä»¥æ‰©å±•æ¨¡å‹çš„ç°æœ‰è¯æ±‡ç”¨ä»¥åŒ…æ‹¬ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­ã€‚è¿™æ ·å°±å¯ä»¥åœ¨é¢†åŸŸé¢„è®­ç»ƒä¸­æ˜ç¡®å­¦ä¹ è¿™ç§æœ¯è¯­çš„è¡¨ç¤ºã€‚

```python
from transformers_domain_adaptation import VocabAugmentor

target_vocab_size = 31_000  # len(tokenizer) == 30_522

augmentor = VocabAugmentor(
    tokenizer=tokenizer, 
    cased=False, 
    target_vocab_size=target_vocab_size
)

# åœ¨å¾®è°ƒè¯­æ–™åº“çš„åŸºç¡€ä¸Šè·å¾—æ–°çš„ç‰¹å®šé¢†åŸŸæœ¯è¯­
new_tokens = augmentor.get_new_tokens(ft_corpus_train)

print(new_tokens[:20])
# ['cdna', 'transcriptional', 'tyrosine', 'phosphorylation', 'kda', 'homology', 'enhancer', 'assays', 'exon', 'nucleotide', 'genomic', 'encodes', 'deletion', 'polymerase', 'nf', 'cloned', 'recombinant', 'putative', 'transcripts', 'homologous']
```



#### ç”¨æ–°çš„è¯æ±‡æœ¯è¯­æ›´æ–°æ¨¡å‹å’Œtokenizer

```python
tokenizer.add_tokens(new_tokens)
model.resize_token_embeddings(len(tokenizer))
# Embedding(31000, 768)
```



### Domain Pre-Training

`Domain PreTraining`æ˜¯`Domain Adaptation`çš„ç¬¬ä¸‰æ­¥ï¼Œæˆ‘ä»¬åœ¨é¢†åŸŸå†…è¯­æ–™åº“ä¸Šç”¨åŒæ ·çš„é¢„è®­ç»ƒç¨‹åºç»§ç»­è®­ç»ƒ`Transformeræ¨¡å‹`ã€‚



#### åˆ›å»ºæ•°æ®é›†

```python
import itertools as it
from pathlib import Path
from typing import Sequence, Union, Generator

from datasets import load_dataset
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments

datasets = load_dataset(
    'text', 
    data_files={
        "train": dpt_corpus_train_data_selected, 
        "val": dpt_corpus_val
    }
)

tokenized_datasets = datasets.map(
    lambda examples: tokenizer(examples['text'], truncation=True, max_length=model.config.max_position_embeddings), 
    batched=True
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)
```

#### å®ä¾‹åŒ–TrainingArgumentså’ŒTrainer

```python
training_args = TrainingArguments(
    output_dir="./results/domain_pre_training",
    overwrite_output_dir=True,
    max_steps=100,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    evaluation_strategy="steps",
    save_steps=50,
    save_total_limit=2,
    logging_steps=50,
    seed=42,
    # fp16=True,
    dataloader_num_workers=2,
    disable_tqdm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['val'],
    data_collator=data_collator,
    tokenizer=tokenizer,  # è¿™ä¸ªæ ‡è®°å™¨æœ‰æ–°çš„tokens
)

# è¿›è¡Œè®­ç»ƒ
trainer.train()
```

`è®­ç»ƒç»“æœ`

| Step | Training Loss | Validation Loss | Runtime   | Samples Per Second |
| ---- | ------------- | --------------- | --------- | ------------------ |
| 50   | 2.813800      | 2.409768        | 75.058500 | 13.323000          |
| 100  | 2.520700      | 2.342451        | 74.257200 | 13.467000          |



### ä¸ºç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒ

æˆ‘ä»¬å¯ä»¥ä¸º`HuggingFace`æ”¯æŒçš„ä»»ä½•å¾®è°ƒä»»åŠ¡æ’å…¥æˆ‘ä»¬çš„`domain adaptation`æ¨¡å‹ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨BC2GMæ•°æ®é›†ï¼ˆä¸€ä¸ªæµè¡Œçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æ•°æ®é›†ï¼‰ä¸Šæ¯”è¾ƒä¸€ä¸ªå¼€ç®±å³ç”¨ï¼ˆOOBï¼‰æ¨¡å‹ä¸ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç”¨äºNERé¢„å¤„ç†å’Œè¯„ä¼°çš„å®ç”¨å‡½æ•°æ”¹ç¼–è‡ªHuggingFaceçš„[NERå¾®è°ƒç¤ºä¾‹ç¬”è®°](https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb)ã€‚

#### å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå½¢æˆNERæ•°æ®é›†

```python
from typing import NamedTuple
from functools import partial
from typing_extensions import Literal

import numpy as np
from datasets import Dataset, load_dataset, load_metric


class Example(NamedTuple):
    token: str
    label: str
        
def load_ner_dataset(mode: Literal['train', 'val', 'test']):
    file = f"data/BC2GM_{mode}.tsv"
    examples = []
    with open(file) as f:
        token = []
        label = []
        for line in f:
            if line.strip() == "":
                examples.append(Example(token=token, label=label))
                token = []
                label = []
                continue
            t, l = line.strip().split("\t")
            token.append(t)
            label.append(l)
            
    res = list(zip(*[(ex.token, ex.label) for ex in examples]))
    d = {'token': res[0], 'labels': res[1]}
    return Dataset.from_dict(d)


def tokenize_and_align_labels(examples, tokenizer):
    tokenized_inputs = tokenizer(examples["token"], truncation=True, is_split_into_words=True)
    label_to_id = dict(map(reversed, enumerate(label_list)))

    labels = []
    for i, label in enumerate(examples["labels"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # ç‰¹æ®Šæ ‡è®°æœ‰ä¸€ä¸ªå•è¯IDï¼Œæ˜¯Noneã€‚æˆ‘ä»¬å°†æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œå› æ­¤å®ƒä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¢«è‡ªåŠ¨å¿½ç•¥äº†ã€‚
            if word_idx is None:
                label_ids.append(-100)
            # æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è®¾ç½®æ ‡ç­¾ã€‚
            elif word_idx != previous_word_idx:
                label_ids.append(label_to_id[label[word_idx]])
            # å¯¹äºä¸€ä¸ªè¯ä¸­çš„å…¶ä»–æ ‡è®°ï¼Œæˆ‘ä»¬æ ¹æ®label_all_tokensçš„æ ‡å¿—ï¼Œå°†æ ‡ç­¾è®¾ç½®ä¸ºå½“å‰æ ‡ç­¾æˆ–-100ã€‚
            else:
                label_ids.append(label_to_id[label[word_idx]])
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs


def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # ç§»é™¤è¢«å¿½ç•¥çš„ç´¢å¼•ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰ã€‚
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }
```



##### å®‰è£… seqeval

```shell
pip install seqeval
```

[seqeval](https://pypi.org/project/seqeval/)æ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ ‡è®°è¯„ä¼°çš„Pythonæ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°åˆ†å—ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€éƒ¨åˆ†è¯­éŸ³æ ‡è®°ã€è¯­ä¹‰è§’è‰²æ ‡è®°ç­‰ã€‚



```python
label_list = ["O", "B", "I"]
metric = load_metric('seqeval')

train_dataset = load_ner_dataset('train')
val_dataset = load_ner_dataset('val')
test_dataset = load_ner_dataset('test')

print(train_dataset[0:1])
print(val_dataset[0:1])
print(test_dataset[0:1])
# {'token': [['Immunohistochemical', 'staining', 'was', 'positive', 'for', 'S', '-', '100', 'in', 'all', '9', 'cases', 'stained', ',', 'positive', 'for', 'HMB', '-', '45', 'in', '9', '(', '90', '%', ')', 'of', '10', ',', 'and', 'negative', 'for', 'cytokeratin', 'in', 'all', '9', 'cases', 'in', 'which', 'myxoid', 'melanoma', 'remained', 'in', 'the', 'block', 'after', 'previous', 'sections', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]}
# {'token': [['Joys', 'and', 'F', '.']], 'labels': [['O', 'O', 'O', 'O']]}
# {'token': [['Physical', 'mapping', '220', 'kb', 'centromeric', 'of', 'the', 'human', 'MHC', 'and', 'DNA', 'sequence', 'analysis', 'of', 'the', '43', '-', 'kb', 'segment', 'including', 'the', 'RING1', ',', 'HKE6', ',', 'and', 'HKE4', 'genes', '.']], 'labels': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'O']]}
```



#### å®ä¾‹åŒ–NERæ¨¡å‹

åœ¨æ­¤ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„NERæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ:

1. `da_model`: æˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­åˆšåˆšè®­ç»ƒçš„ä¸€ä¸ª`Domain Adaptation`çš„NERæ¨¡å‹

2. `da_full_corpus_model`: åŒæ ·çš„é¢†åŸŸé€‚åº”æ€§NERæ¨¡å‹ï¼Œåªæ˜¯å®ƒæ˜¯åœ¨å®Œæ•´çš„é¢†åŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚

3. `oob_model`: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„BERT-NERæ¨¡å‹ï¼ˆæ²¡æœ‰ç»è¿‡Domain Adaptationï¼‰ã€‚

```python
from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification

best_checkpoint = './results/domain_pre_training/checkpoint-100'
da_model = AutoModelForTokenClassification.from_pretrained(best_checkpoint, num_labels=len(label_list))

da_full_corpus_model = AutoModelForTokenClassification.from_pretrained('./domain-adapted-bert', num_labels=len(label_list))
full_corpus_tokenizer = AutoTokenizer.from_pretrained('./domain-adapted-bert')

oob_tokenizer = AutoTokenizer.from_pretrained(model_card)
oob_model = AutoModelForTokenClassification.from_pretrained(model_card, num_labels=len(label_list))
```



#### ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºæ•°æ®é›†ã€TrainingArgumentså’ŒTrainer

```python
from typing import Dict

from datasets import Dataset


def preprocess_datasets(tokenizer, **datasets) -> Dict[str, Dataset]:
    tokenize_ner = partial(tokenize_and_align_labels, tokenizer=tokenizer)
    return {k: ds.map(tokenize_ner, batched=True) for k, ds in datasets.items()}

######################
##### `da_model` #####
######################
da_datasets = preprocess_datasets(
    tokenizer, 
    train=train_dataset, 
    val=val_dataset, 
    test=test_dataset
)

print(da_datasets)

training_args = TrainingArguments(
    output_dir="./results/domain_adapted_fine_tuning",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    save_steps=200,
    save_total_limit=2,
    logging_steps=100,
    seed=42,
    fp16=True,
    dataloader_num_workers=2,
    disable_tqdm=False
)

da_trainer = Trainer(
    model=da_model,
    args=training_args,
    train_dataset=da_datasets['train'],
    eval_dataset=da_datasets['val'],
    data_collator=DataCollatorForTokenClassification(tokenizer),
    tokenizer=tokenizer,  # This tokenizer has new tokens
    compute_metrics=compute_metrics
)


##################################
##### `da_model_full_corpus` #####
##################################
da_full_corpus_datasets = preprocess_datasets(
    full_corpus_tokenizer, 
    train=train_dataset, 
    val=val_dataset, 
    test=test_dataset
)

training_args = TrainingArguments(
    output_dir="./results/domain_adapted_full_corpus_fine_tuning",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    save_steps=200,
    save_total_limit=2,
    logging_steps=100,
    seed=42,
    fp16=True,
    dataloader_num_workers=2,
    disable_tqdm=False
)

da_full_corpus_trainer = Trainer(
    model=da_full_corpus_model,
    args=training_args,
    train_dataset=da_full_corpus_datasets['train'],
    eval_dataset=da_full_corpus_datasets['val'],
    data_collator=DataCollatorForTokenClassification(full_corpus_tokenizer),
    tokenizer=full_corpus_tokenizer,  # This tokenizer has new tokens
    compute_metrics=compute_metrics
)


#######################
##### `oob_model` #####
#######################
oob_datasets = preprocess_datasets(
    oob_tokenizer, 
    train=train_dataset, 
    val=val_dataset, 
    test=test_dataset
)

training_args = TrainingArguments(
    output_dir="./results/out_of_the_box_fine_tuning",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    save_steps=200,
    save_total_limit=2,
    logging_steps=100,
    seed=42,
    fp16=True,
    dataloader_num_workers=2,
    disable_tqdm=False
)

oob_model_trainer = Trainer(
    model=oob_model,
    args=training_args,
    train_dataset=oob_datasets['train'],
    eval_dataset=oob_datasets['val'],
    data_collator=DataCollatorForTokenClassification(oob_tokenizer),
    tokenizer=oob_tokenizer,  # è¿™æ˜¯åŸå§‹çš„tokenizerï¼ˆæ²¡æœ‰ç‰¹å®šé¢†åŸŸçš„tokenï¼‰ã€‚
    compute_metrics=compute_metrics
)
```



#### è®­ç»ƒå’Œè¯„ä¼°`da_model`

```python
da_trainer.train()
da_trainer.evaluate(da_datasets['test'])
```



è®­ç»ƒç»“æœ

| Step | Training Loss | Validation Loss | Precision | Recall   | F1       | Accuracy | Runtime   | Samples Per Second |
| ---- | ------------- | --------------- | --------- | -------- | -------- | -------- | --------- | ------------------ |
| 100  | 0.258000      | 0.156976        | 0.621130  | 0.604061 | 0.612477 | 0.941589 | 64.652200 | 38.962000          |
| 200  | 0.148500      | 0.130968        | 0.689895  | 0.693156 | 0.691522 | 0.951615 | 64.609800 | 38.988000          |
| 300  | 0.131800      | 0.119317        | 0.671880  | 0.774549 | 0.719571 | 0.954099 | 64.629700 | 38.976000          |
| 400  | 0.116800      | 0.108599        | 0.738141  | 0.743567 | 0.740844 | 0.959777 | 64.621000 | 38.981000          |
| 500  | 0.078600      | 0.106925        | 0.749023  | 0.771574 | 0.760131 | 0.962172 | 64.869500 | 38.832000          |
| 600  | 0.074800      | 0.098790        | 0.749081  | 0.784351 | 0.766310 | 0.962727 | 64.517500 | 39.044000          |
| 700  | 0.073000      | 0.099364        | 0.763633  | 0.784351 | 0.773854 | 0.964268 | 64.496200 | 39.057000          |



#### è®­ç»ƒå’Œè¯„ä¼°`da_model_full_corpus`

```python
da_full_corpus_trainer.train()
da_full_corpus_trainer.evaluate(da_full_corpus_datasets['test'])
```

`ç»“æœ`

| Step | Training Loss | Validation Loss | Precision |   Recall |       F1 | Accuracy |  Runtime | Samples Per Second |
| ---: | ------------: | --------------: | --------: | -------: | -------: | -------: | -------: | -----------------: |
|  100 |      0.231900 |        0.127792 |  0.671435 | 0.829809 | 0.742268 | 0.952202 | 8.246900 |         305.450000 |
|  200 |      0.108300 |        0.086280 |  0.817341 | 0.826690 | 0.821989 | 0.968876 | 8.064200 |         312.366000 |
|  300 |      0.089600 |        0.083020 |  0.807372 | 0.838995 | 0.822879 | 0.969839 | 8.014300 |         314.313000 |
|  400 |      0.080600 |        0.078229 |  0.801577 | 0.880763 | 0.839306 | 0.971885 | 8.141400 |         309.405000 |
|  500 |      0.050800 |        0.075855 |  0.843227 | 0.864125 | 0.853548 | 0.973716 | 8.172500 |         308.230000 |
|  600 |      0.052500 |        0.075362 |  0.845051 | 0.858232 | 0.851591 | 0.973550 | 8.057900 |         312.611000 |
|  700 |      0.047400 |        0.073649 |  0.851391 | 0.864818 | 0.858052 | 0.974442 | 8.029400 |                    |



#### è®­ç»ƒå’Œè¯„ä¼°`oob_model`



```python
oob_model_trainer.train()
oob_model_trainer.evaluate(oob_datasets['test'])
```

`ç»“æœ`

| Step | Training Loss | Validation Loss | Precision |   Recall |       F1 | Accuracy |  Runtime | Samples Per Second |
| ---: | ------------: | --------------: | --------: | -------: | -------: | -------: | -------: | -----------------: |
|  100 |      0.229200 |        0.133785 |  0.678159 | 0.803118 | 0.735368 | 0.947964 | 8.654700 |         291.056000 |
|  200 |      0.135200 |        0.109798 |  0.745311 | 0.825984 | 0.783576 | 0.957941 | 8.660700 |         290.855000 |
|  300 |      0.117200 |        0.099117 |  0.782186 | 0.837120 | 0.808721 | 0.962326 | 8.699700 |         289.550000 |
|  400 |      0.101300 |        0.095984 |  0.827210 | 0.822420 | 0.824808 | 0.965538 | 8.725000 |         288.710000 |
|  500 |      0.069000 |        0.103978 |  0.788701 | 0.845731 | 0.816221 | 0.961440 | 8.690600 |         289.853000 |
|  600 |      0.064100 |        0.092247 |  0.827396 | 0.848404 | 0.837768 | 0.967232 | 8.671200 |         290.501000 |
|  700 |      0.064400 |        0.090411 |  0.829128 | 0.853749 | 0.841258 | 0.968306 | 8.821600 |         285.549000 |



### ç»“æœ

æˆ‘ä»¬çœ‹åˆ°ï¼Œåœ¨è¿™ä¸‰ä¸ªæ¨¡å‹ä¸­ï¼Œ`da_full_corpus_model`ï¼ˆåœ¨æ•´ä¸ªåŸŸå†…è®­ç»ƒè¯­æ–™åº“ä¸Šè¿›è¡Œäº†åŸŸè°ƒæ•´ï¼‰åœ¨æµ‹è¯•F1å¾—åˆ†ä¸Šæ¯”`oob_model`é«˜å‡º`2%`ä»¥ä¸Šã€‚äº‹å®ä¸Šï¼Œè¿™ä¸ª`da_full_corpus_model`æ¨¡å‹æ˜¯æˆ‘ä»¬è®­ç»ƒçš„åœ¨BC2GMä¸Šä¼˜äºSOTAçš„è®¸å¤šé¢†åŸŸé€‚åº”æ¨¡å‹ä¹‹ä¸€ã€‚

æ­¤å¤–ï¼Œ`da_model`çš„è¡¨ç°ä¹Ÿä½äº`oob_model`ã€‚è¿™æ˜¯å¯ä»¥é¢„æœŸçš„ï¼Œå› ä¸º`da_model`åœ¨æœ¬æŒ‡å—ä¸­ç»å†äº†æœ€å°çš„é¢†åŸŸé¢„è®­ç»ƒã€‚



## æ€»ç»“

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œä½ å·²ç»çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ "DataSelector "å’Œ "VocabAugmentor"ï¼Œé€šè¿‡åˆ†åˆ«æ‰§è¡Œæ•°æ®é€‰æ‹©å’Œè¯æ±‡æ‰©å±•ï¼Œå¯¹å˜å‹å™¨æ¨¡å‹è¿›è¡Œé¢†åŸŸè°ƒæ•´ã€‚

ä½ è¿˜çœ‹åˆ°å®ƒä»¬ä¸HuggingFaceçš„æ‰€æœ‰äº§å“å…¼å®¹ã€‚å˜æ¢å™¨"ã€"æ ‡è®°å™¨ "å’Œ "æ•°æ®é›†"ã€‚

æœ€åè¡¨æ˜ï¼Œåœ¨å®Œæ•´çš„é¢†åŸŸå†…è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸé€‚åº”çš„æ¨¡å‹æ¯”å¼€ç®±å³ç”¨çš„æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚



## å‚è€ƒ

[Transformers-Domain-Adaptation](https://github.com/georgian-io/Transformers-Domain-Adaptation)

[Guide to Transformers Domain Adaptation](https://colab.research.google.com/drive/1RAigUDEPpwdfgbzDII0C6-nmtoqgKABA?usp=sharing)


